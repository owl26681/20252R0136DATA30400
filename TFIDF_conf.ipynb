{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26217918-5c27-43a7-a504-84e4cd5bba75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T08:32:32.070002Z",
     "iopub.status.busy": "2025-11-13T08:32:32.069602Z",
     "iopub.status.idle": "2025-11-13T08:32:33.339873Z",
     "shell.execute_reply": "2025-11-13T08:32:33.339398Z",
     "shell.execute_reply.started": "2025-11-13T08:32:32.069986Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25eba133-3361-42f0-8305-6df95935d14f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T08:32:33.340823Z",
     "iopub.status.busy": "2025-11-13T08:32:33.340547Z",
     "iopub.status.idle": "2025-11-13T08:32:35.716847Z",
     "shell.execute_reply": "2025-11-13T08:32:35.716348Z",
     "shell.execute_reply.started": "2025-11-13T08:32:33.340808Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chocolate_bars 0.5059\n",
      "chocolate_gifts 0.4276\n",
      "chocolate 0.3754\n",
      "chocolate_covered_fruit 0.3548\n",
      "dried_fruit_raisins 0.2765\n",
      "chocolate_pretzels 0.2373\n",
      "fresh_baked_cookies 0.2332\n",
      "grocery_gourmet_food 0.2266\n",
      "snack_gifts 0.2254\n",
      "chocolate_assortments 0.2173\n",
      "candy_chocolate 0.1635\n",
      "hot_cocoa 0.1609\n",
      "food 0.1457\n",
      "gourmet_gifts 0.1342\n",
      "snack_food 0.121\n",
      "trail_mix 0.1181\n",
      "granola_trail_mix_bars 0.1044\n",
      "fruit_leather 0.1023\n",
      "toaster_pastries 0.1015\n",
      "cookies 0.0996\n",
      "fruit 0.0941\n",
      "raisins 0.0925\n",
      "meat_poultry 0.0903\n",
      "marshmallows 0.087\n",
      "changing_table_pads_covers 0.0863\n",
      "popcorn 0.085\n",
      "granola_bars 0.0846\n",
      "produce 0.0833\n",
      "solid_feeding 0.0831\n",
      "milk 0.0827\n",
      "chocolate_truffles 0.0822\n",
      "rice_cakes 0.0777\n",
      "nutrition_wellness 0.0746\n",
      "party_mix 0.0734\n",
      "p_t_s 0.0716\n",
      "fruit_gifts 0.0684\n",
      "sensual_delights 0.0649\n",
      "foie_gras_p_t_s 0.062\n",
      "sugars_sweeteners 0.0604\n",
      "salsas 0.0594\n",
      "eggs 0.059\n",
      "cakes 0.0569\n",
      "nutrition_bars_drinks 0.0569\n",
      "chocolate_covered_nuts 0.0565\n",
      "dessert_gifts 0.0551\n",
      "spices_gifts 0.0545\n",
      "meat_gifts 0.0541\n",
      "crackers 0.0533\n",
      "juices 0.0517\n",
      "puffed_snacks 0.0512\n",
      "bars 0.0506\n",
      "cloth_diapers 0.0498\n",
      "sausages 0.0495\n",
      "baby_food 0.0489\n",
      "assortments 0.0484\n",
      "jams 0.0457\n",
      "pudding 0.0455\n",
      "jams_preserves_gifts 0.0446\n",
      "fruits 0.0429\n",
      "pretzels 0.0393\n",
      "sauces_gifts 0.0365\n",
      "seafood_gifts 0.0356\n",
      "nut_clusters 0.0322\n",
      "jerky_dried_meats 0.0322\n",
      "suckers_lollipops 0.0318\n",
      "treats 0.0312\n",
      "cheese_gifts 0.0306\n",
      "candy_gifts 0.0302\n",
      "baby_formula 0.0299\n",
      "gummy_candy 0.0229\n",
      "jerky 0.0\n",
      "toys_games 0.0\n",
      "games 0.0\n",
      "puzzles 0.0\n",
      "jigsaw_puzzles 0.0\n",
      "board_games 0.0\n",
      "beverages 0.0\n",
      "beauty 0.0\n",
      "makeup 0.0\n",
      "nails 0.0\n",
      "arts_crafts 0.0\n",
      "drawing_painting_supplies 0.0\n",
      "action_toy_figures 0.0\n",
      "figures 0.0\n",
      "dolls_accessories 0.0\n",
      "dolls 0.0\n",
      "card_games 0.0\n",
      "drawing_sketching_tablets 0.0\n",
      "baby_toddler_toys 0.0\n",
      "shape_sorters 0.0\n",
      "health_personal_care 0.0\n",
      "personal_care 0.0\n",
      "deodorants_antiperspirants 0.0\n",
      "learning_education 0.0\n",
      "habitats 0.0\n",
      "electronics_for_kids 0.0\n",
      "household_supplies 0.0\n",
      "household_batteries 0.0\n",
      "push_pull_toys 0.0\n",
      "stuffed_animals_plush 0.0\n",
      "tricycles 0.0\n",
      "scooters_wagons 0.0\n",
      "clay_dough 0.0\n",
      "health_care 0.0\n",
      "allergy 0.0\n",
      "baby_products 0.0\n",
      "gear 0.0\n",
      "baby_gyms_playmats 0.0\n",
      "shaving_hair_removal 0.0\n",
      "skin_care 0.0\n",
      "face 0.0\n",
      "animals_figures 0.0\n",
      "feminine_care 0.0\n",
      "music_sound 0.0\n",
      "oral_hygiene 0.0\n",
      "grown_up_toys 0.0\n",
      "dress_up_pretend_play 0.0\n",
      "pretend_play 0.0\n",
      "novelty_gag_toys 0.0\n",
      "bath_body 0.0\n",
      "cleansers 0.0\n",
      "playsets 0.0\n",
      "d_puzzles 0.0\n",
      "dollhouses 0.0\n",
      "lip_care_products 0.0\n",
      "tools_accessories 0.0\n",
      "nail_tools 0.0\n",
      "eye_care 0.0\n",
      "pill_cases_splitters 0.0\n",
      "hair_care 0.0\n",
      "styling_products 0.0\n",
      "electronic_toys 0.0\n",
      "body 0.0\n",
      "toy_balls 0.0\n",
      "eyes 0.0\n",
      "trading_card_games 0.0\n",
      "foot_care 0.0\n",
      "hands_nails 0.0\n",
      "sun 0.0\n",
      "medical_supplies_equipment 0.0\n",
      "daily_living_aids 0.0\n",
      "baby_child_care 0.0\n",
      "paper_plastic 0.0\n",
      "incontinence 0.0\n",
      "shampoos 0.0\n",
      "conditioners 0.0\n",
      "music_players_karaoke 0.0\n",
      "cough_cold 0.0\n",
      "bath 0.0\n",
      "tests 0.0\n",
      "building_toys 0.0\n",
      "building_sets 0.0\n",
      "stress_reduction 0.0\n",
      "family_planning_contraceptives 0.0\n",
      "vitamins_supplements 0.0\n",
      "hair_color 0.0\n",
      "pain_relievers 0.0\n",
      "cotton_swabs 0.0\n",
      "styling_tools 0.0\n",
      "first_aid 0.0\n",
      "scrubs_body_treatments 0.0\n",
      "cleaning_tools 0.0\n",
      "pegged_puzzles 0.0\n",
      "diabetes 0.0\n",
      "magic_kits_accessories 0.0\n",
      "gifts 0.0\n",
      "albums 0.0\n",
      "crib_toys_attachments 0.0\n",
      "digestion_nausea 0.0\n",
      "electronic_pets 0.0\n",
      "sexual_wellness 0.0\n",
      "safer_sex 0.0\n",
      "thermometers 0.0\n",
      "stacking_nesting_toys 0.0\n",
      "makeup_remover 0.0\n",
      "temporary_tattoos 0.0\n",
      "sports_outdoor_play 0.0\n",
      "play_tents_tunnels 0.0\n",
      "science 0.0\n",
      "sports 0.0\n",
      "bath_toys 0.0\n",
      "puppets 0.0\n",
      "systems_accessories 0.0\n",
      "health_monitors 0.0\n",
      "inflatable_bouncers 0.0\n",
      "hobbies 0.0\n",
      "model_building_kits_tools 0.0\n",
      "blackboards_whiteboards 0.0\n",
      "pools_water_fun 0.0\n",
      "rattles 0.0\n",
      "sandboxes_accessories 0.0\n",
      "activity_play_centers 0.0\n",
      "car_seat_stroller_toys 0.0\n",
      "feeding 0.0\n",
      "bottle_feeding 0.0\n",
      "breastfeeding 0.0\n",
      "diapering 0.0\n",
      "diaper_changing_kits 0.0\n",
      "puzzle_accessories 0.0\n",
      "diaper_pails_refills 0.0\n",
      "safety 0.0\n",
      "bathroom_safety 0.0\n",
      "massage_relaxation 0.0\n",
      "gates_doorways 0.0\n",
      "nursery 0.0\n",
      "furniture 0.0\n",
      "monitors 0.0\n",
      "plush_backpacks_purses 0.0\n",
      "statues 0.0\n",
      "bathing_skin_care 0.0\n",
      "bathing_tubs_seats 0.0\n",
      "vehicles_remote_control 0.0\n",
      "play_vehicles 0.0\n",
      "backpacks_carriers 0.0\n",
      "craft_kits 0.0\n",
      "car_seats_accessories 0.0\n",
      "car_seats 0.0\n",
      "nursery_d_cor 0.0\n",
      "hammering_pounding_toys 0.0\n",
      "bedding 0.0\n",
      "play_trains_railway_sets 0.0\n",
      "rockets 0.0\n",
      "stacking_blocks 0.0\n",
      "diaper_bags 0.0\n",
      "strollers 0.0\n",
      "gym_sets_swings 0.0\n",
      "pregnancy_maternity 0.0\n",
      "maternity_pillows 0.0\n",
      "rocking_spring_ride_ons 0.0\n",
      "braces 0.0\n",
      "accessories 0.0\n",
      "vehicle_playsets 0.0\n",
      "doll_accessories 0.0\n",
      "pet_supplies 0.0\n",
      "cats 0.0\n",
      "litter_housebreaking 0.0\n",
      "spinning_tops 0.0\n",
      "sets 0.0\n",
      "travel_games 0.0\n",
      "pillows_stools 0.0\n",
      "battling_tops 0.0\n",
      "cameras_camcorders 0.0\n",
      "dance_mats 0.0\n",
      "radio_control 0.0\n",
      "grooming_healthcare_kits 0.0\n",
      "balls 0.0\n",
      "tile_games 0.0\n",
      "potty_training 0.0\n",
      "potties_seats 0.0\n",
      "highchairs_booster_seats 0.0\n",
      "stuffed_animals_toys 0.0\n",
      "dvd_games 0.0\n",
      "edge_corner_guards 0.0\n",
      "basic_life_skills_toys 0.0\n",
      "activity_centers_entertainers 0.0\n",
      "thermometer_accessories 0.0\n",
      "wipes_holders 0.0\n",
      "gift_sets 0.0\n",
      "joggers 0.0\n",
      "facial_steamers 0.0\n",
      "kites_wind_spinners 0.0\n",
      "dogs 0.0\n",
      "toys 0.0\n",
      "walkers 0.0\n",
      "slumber_bags 0.0\n",
      "die_cast_vehicles 0.0\n",
      "easels 0.0\n",
      "lips 0.0\n",
      "tea 0.0\n",
      "reading_writing 0.0\n",
      "stacking_games 0.0\n",
      "sauces_dips 0.0\n",
      "sauces 0.0\n",
      "breakfast_foods 0.0\n",
      "cereals 0.0\n",
      "shopping_cart_covers 0.0\n",
      "pantry_staples 0.0\n",
      "scaled_model_vehicles 0.0\n",
      "cooking_baking_supplies 0.0\n",
      "personal_video_players_accessories 0.0\n",
      "fragrance 0.0\n",
      "women_s 0.0\n",
      "keepsakes 0.0\n",
      "swings 0.0\n",
      "trains_accessories 0.0\n",
      "disposable_diapers 0.0\n",
      "plug_play_video_games 0.0\n",
      "floor_puzzles 0.0\n",
      "fresh_flowers_live_indoor_plants 0.0\n",
      "live_indoor_plants 0.0\n",
      "weight_loss_products 0.0\n",
      "smoking_cessation 0.0\n",
      "beauty_fashion 0.0\n",
      "mirrors 0.0\n",
      "coffee 0.0\n",
      "cabinet_locks_straps 0.0\n",
      "plush_pillows 0.0\n",
      "floor_games 0.0\n",
      "makeup_brushes_tools 0.0\n",
      "alternative_medicine 0.0\n",
      "men_s 0.0\n",
      "step_stools 0.0\n",
      "rails_rail_guards 0.0\n",
      "laundry 0.0\n",
      "women_s_health 0.0\n",
      "standard 0.0\n",
      "beds_furniture 0.0\n",
      "herbs 0.0\n",
      "sleep_positioners 0.0\n",
      "health_supplies 0.0\n",
      "breakfast_cereal_bars 0.0\n",
      "body_art 0.0\n",
      "condiments 0.0\n",
      "breads_bakery 0.0\n",
      "dried_beans 0.0\n",
      "household_cleaning 0.0\n",
      "collars 0.0\n",
      "educational_repellents 0.0\n",
      "adult_toys_games 0.0\n",
      "teddy_bears 0.0\n",
      "therapeutic_skin_care 0.0\n",
      "sand_water_tables 0.0\n",
      "slot_cars 0.0\n",
      "soft_drinks 0.0\n",
      "chips_crisps 0.0\n",
      "licorice 0.0\n",
      "feeding_watering_supplies 0.0\n",
      "blasters_foam_play 0.0\n",
      "meat_seafood 0.0\n",
      "wild_game_fowl 0.0\n",
      "spices_seasonings 0.0\n",
      "training_behavior_aids 0.0\n",
      "gardening_tools 0.0\n",
      "bathroom_aids_safety 0.0\n",
      "pogo_sticks_hoppers 0.0\n",
      "powdered_drink_mixes 0.0\n",
      "playards 0.0\n",
      "gag_toys_practical_jokes 0.0\n",
      "lighters 0.0\n",
      "money_banks 0.0\n",
      "marble_runs 0.0\n",
      "game_collections 0.0\n",
      "kitchen_safety 0.0\n",
      "fish_aquatic_pets 0.0\n",
      "gum 0.0\n",
      "outdoor_safety 0.0\n",
      "hair_nails 0.0\n",
      "aquarium_lights 0.0\n",
      "blocks 0.0\n",
      "tandem 0.0\n",
      "occupational_physical_therapy_aids 0.0\n",
      "packaged_meals_side_dishes 0.0\n",
      "indoor_climbers_play_structures 0.0\n",
      "pumps_filters 0.0\n",
      "beds_accessories 0.0\n",
      "energy_drinks 0.0\n",
      "sleep_snoring 0.0\n",
      "geography 0.0\n",
      "small_animals 0.0\n",
      "houses_habitats 0.0\n",
      "dairy_eggs 0.0\n",
      "cheese 0.0\n",
      "travel_systems 0.0\n",
      "walkie_talkies 0.0\n",
      "mobility_aids_equipment 0.0\n",
      "sexual_enhancers 0.0\n",
      "dips 0.0\n",
      "dollhouse_accessories 0.0\n",
      "bathing_accessories 0.0\n",
      "grooming 0.0\n",
      "baby_seats 0.0\n",
      "wind_up_toys 0.0\n",
      "dishwashing 0.0\n",
      "carriers_strollers 0.0\n",
      "flash_cards 0.0\n",
      "brain_teasers 0.0\n",
      "nesting_dolls 0.0\n",
      "test_kits 0.0\n",
      "lightweight 0.0\n",
      "hair_loss_products 0.0\n",
      "water_treatments 0.0\n",
      "birds 0.0\n",
      "hair_scalp_treatments 0.0\n",
      "cages_accessories 0.0\n",
      "gummy_candies 0.0\n",
      "houses 0.0\n",
      "ear_care 0.0\n",
      "pizza_crusts 0.0\n",
      "hard_candies 0.0\n",
      "sports_supplements 0.0\n",
      "baking_mixes 0.0\n",
      "pork_rinds 0.0\n",
      "pasta_noodles 0.0\n",
      "carriers_travel_products 0.0\n",
      "fresh_fruits 0.0\n",
      "chips 0.0\n",
      "mathematics_counting 0.0\n",
      "toy_banks 0.0\n",
      "training_pants 0.0\n",
      "tea_gifts 0.0\n",
      "oils 0.0\n",
      "aquarium_hoods 0.0\n",
      "tortillas 0.0\n",
      "doors 0.0\n",
      "standard_playing_card_decks 0.0\n",
      "fudge 0.0\n",
      "syrups 0.0\n",
      "printing_stamping 0.0\n",
      "toy_gift_sets 0.0\n",
      "canned_jarred_food 0.0\n",
      "fresh_vegetables 0.0\n",
      "apparel_accessories 0.0\n",
      "chewing_gum 0.0\n",
      "puzzle_play_mats 0.0\n",
      "electrical_safety 0.0\n",
      "marble_games 0.0\n",
      "miniatures 0.0\n",
      "finger_boards_finger_bikes 0.0\n",
      "coconut_water 0.0\n",
      "handheld_games 0.0\n",
      "slime_putty_toys 0.0\n",
      "pastries 0.0\n",
      "health_baby_care 0.0\n",
      "teethers 0.0\n",
      "butter 0.0\n",
      "breakfast_bakery 0.0\n",
      "stickers 0.0\n",
      "soaps_cleansers 0.0\n",
      "fitness_equipment 0.0\n",
      "water 0.0\n",
      "portable_changing_pads 0.0\n",
      "dice_gaming_dice 0.0\n",
      "pacifiers_accessories 0.0\n",
      "cocktail_mixers 0.0\n",
      "aquariums 0.0\n",
      "ball_pits_accessories 0.0\n",
      "seafood 0.0\n",
      "bags_cases 0.0\n",
      "jelly_beans 0.0\n",
      "novelty_spinning_tops 0.0\n",
      "automatic_feeders 0.0\n",
      "mints 0.0\n",
      "makeup_sets 0.0\n",
      "cleaners 0.0\n",
      "fresh_cut_flowers 0.0\n",
      "prams 0.0\n",
      "nuts_seeds 0.0\n",
      "taffy 0.0\n",
      "bunny_rabbit_central 0.0\n",
      "rabbit_hutches 0.0\n",
      "aquarium_d_cor 0.0\n",
      "viewfinders 0.0\n",
      "harnesses_leashes 0.0\n",
      "game_accessories 0.0\n",
      "game_room_games 0.0\n",
      "cages 0.0\n",
      "non_slip_bath_mats 0.0\n",
      "halva 0.0\n",
      "stimulants 0.0\n",
      "beanbags_foot_bags 0.0\n",
      "shampoo_conditioner_sets 0.0\n",
      "breadcrumbs 0.0\n",
      "extracts_flavoring 0.0\n",
      "plush_puppets 0.0\n",
      "shampoo_plus_conditioner 0.0\n",
      "memorials 0.0\n",
      "die_cast_toy_vehicles 0.0\n",
      "aquarium_starter_kits 0.0\n",
      "coffee_gifts 0.0\n",
      "air_fresheners 0.0\n",
      "sugar_substitutes 0.0\n",
      "bacon 0.0\n",
      "cat_flaps 0.0\n",
      "aquarium_heaters 0.0\n",
      "hair_relaxers 0.0\n",
      "breads 0.0\n",
      "packaged_breads 0.0\n",
      "dessert_toppings 0.0\n",
      "diaper_stackers_caddies 0.0\n",
      "prisms_kaleidoscopes 0.0\n",
      "maternity 0.0\n",
      "crackers_biscuits 0.0\n",
      "coin_collecting 0.0\n",
      "kickball_playground_balls 0.0\n",
      "hair_perms_texturizers 0.0\n",
      "yo_yos 0.0\n",
      "flours_meals 0.0\n",
      "beef 0.0\n",
      "molding_sculpting_sticks 0.0\n",
      "washcloths_towels 0.0\n",
      "stuffing 0.0\n",
      "baking_powder 0.0\n",
      "cereal 0.0\n",
      "exotic_meats 0.0\n",
      "breadsticks 0.0\n",
      "cloth_diaper_accessories 0.0\n",
      "carriers 0.0\n",
      "toffee 0.0\n",
      "hair_coloring_tools 0.0\n",
      "caramels 0.0\n",
      "aromatherapy 0.0\n",
      "seat_covers 0.0\n",
      "bondage_gear_accessories 0.0\n",
      "sun_protection 0.0\n",
      "dinners 0.0\n",
      "aquarium_stands 0.0\n",
      "teaching_clocks 0.0\n",
      "milk_substitutes 0.0\n",
      "bubble_bath 0.0\n",
      "novelties 0.0\n",
      "beads 0.0\n",
      "fish_bowls 0.0\n",
      "odor_stain_removers 0.0\n",
      "food_coloring 0.0\n",
      "children_s 0.0\n",
      "ice_cream_frozen_desserts 0.0\n",
      "pastry_decorations 0.0\n",
      "chicken 0.0\n",
      "sports_drinks 0.0\n",
      "aprons_smocks 0.0\n",
      "electronics 0.0\n",
      "sex_furniture 0.0\n",
      "pork 0.0\n",
      "dried_fruit 0.0\n",
      "flying_toys 0.0\n",
      "shampoo 0.0\n",
      "coatings_batters 0.0\n",
      "hydrometers 0.0\n",
      "lamb 0.0\n",
      "exercise_wheels 0.0\n",
      "breeding_tanks 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, ConcatDataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "def load_label_file(path: str) -> str:\n",
    "    \"\"\"key: value1,value2,... í˜•ì‹ìœ¼ë¡œ ëœ .txt íŒŒì¼ì„ í†µì§¸ë¡œ ì½ì–´ì„œ ë¬¸ìì—´ë¡œ ë°˜í™˜\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def parse_key_value_lines(text: str):\n",
    "    \"\"\"'key:val1,val2,...' ì—¬ëŸ¬ ì¤„ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜\"\"\"\n",
    "    id2label = {}\n",
    "    for line in text.splitlines():\n",
    "        line = line.strip()\n",
    "        if not line or \":\" not in line:\n",
    "            continue\n",
    "        key, vals = line.split(\":\", 1)\n",
    "        id2label[key.strip()] = vals.strip()\n",
    "    return id2label\n",
    "\n",
    "def preprocess_label_text(label_path_str: str):\n",
    "    cleaned = label_path_str.lower()\n",
    "    cleaned = re.sub(r\"[:,]\", \" \", cleaned)\n",
    "    cleaned = re.sub(r\"_\", \" \", cleaned)\n",
    "    cleaned = re.sub(r\"[^a-z0-9 ]\", \" \", cleaned)\n",
    "    cleaned = re.sub(r\"\\s+\", \" \", cleaned).strip()\n",
    "    return cleaned\n",
    "\n",
    "def build_tfidf_vectorizer(label_texts):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    label_tfidf = vectorizer.fit_transform(label_texts)\n",
    "    return vectorizer, label_tfidf\n",
    "\n",
    "def compute_lexical_similarity(doc_text, vectorizer, label_tfidf):\n",
    "    doc_vec = vectorizer.transform([doc_text])\n",
    "    sims = cosine_similarity(doc_vec, label_tfidf)[0]\n",
    "    return sims\n",
    "\n",
    "    \n",
    "label_raw_text = load_label_file(\"Amazon_products/class_related_keywords.txt\")  # ë„¤ íŒŒì¼ ì´ë¦„ì— ë§ì¶° ë°”ê¿”\n",
    "id2label = parse_key_value_lines(label_raw_text)\n",
    "\n",
    "# 2) ë¼ë²¨ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬í•´ì„œ TF-IDF í•™ìŠµ\n",
    "label_keys = list(id2label.keys())\n",
    "label_texts = [\n",
    "    preprocess_label_text(f\"{k} {id2label[k]}\")\n",
    "    for k in label_keys\n",
    "]\n",
    "vectorizer, label_tfidf = build_tfidf_vectorizer(label_texts)\n",
    "\n",
    "# 3) í…ŒìŠ¤íŠ¸ìš© ë¬¸ì„œ í•˜ë‚˜ ë„£ì–´ë³´ê¸°\n",
    "doc = \"gourmet organic chocolate snack\"\n",
    "doc_clean = preprocess_label_text(doc)\n",
    "sims = compute_lexical_similarity(doc_clean, vectorizer, label_tfidf)\n",
    "\n",
    "# 4) ê²°ê³¼ ë³´ê¸°\n",
    "label_sims = list(zip(label_keys, sims))\n",
    "label_sims.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for lbl, score in label_sims:\n",
    "    print(lbl, round(score, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19ba131e-72d7-4e8c-b1fe-2f3ef78b1466",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T08:32:35.717505Z",
     "iopub.status.busy": "2025-11-13T08:32:35.717294Z",
     "iopub.status.idle": "2025-11-13T08:32:35.728944Z",
     "shell.execute_reply": "2025-11-13T08:32:35.728553Z",
     "shell.execute_reply.started": "2025-11-13T08:32:35.717490Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3466,)\n"
     ]
    }
   ],
   "source": [
    "def build_label_embeddings(label_keys, label_tfidf, dense: bool = True):\n",
    "    \"\"\"\n",
    "    label_keys: ë¼ë²¨ ì´ë¦„ ë¦¬ìŠ¤íŠ¸ (vectorizeí•  ë•Œ ì¼ë˜ ìˆœì„œì™€ ê°™ì•„ì•¼ í•¨)\n",
    "    label_tfidf: shape = (n_labels, vocab_size) ì¸ sparse matrix\n",
    "    dense: Trueë©´ numpy arrayë¡œ ë°”ê¿”ì„œ ëŒë ¤ì¤Œ\n",
    "\n",
    "    return:\n",
    "        dict: {label_name: embedding_vector}\n",
    "    \"\"\"\n",
    "    embeddings = {}\n",
    "    if dense:\n",
    "        label_tfidf_dense = label_tfidf.toarray()\n",
    "        for i, label in enumerate(label_keys):\n",
    "            embeddings[label] = label_tfidf_dense[i]\n",
    "    else:\n",
    "        # sparse ê·¸ëŒ€ë¡œ\n",
    "        for i, label in enumerate(label_keys):\n",
    "            embeddings[label] = label_tfidf[i]\n",
    "    return embeddings\n",
    "\n",
    "label_embeddings = build_label_embeddings(label_keys, label_tfidf, dense=True)\n",
    "print(label_embeddings[\"grocery_gourmet_food\"].shape)  # (vocab_size,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e1bcacb-8765-49a0-b6d6-d18d6f5e5531",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T01:28:59.947460Z",
     "iopub.status.busy": "2025-11-11T01:28:59.947322Z",
     "iopub.status.idle": "2025-11-11T01:28:59.988985Z",
     "shell.execute_reply": "2025-11-11T01:28:59.988493Z",
     "shell.execute_reply.started": "2025-11-11T01:28:59.947446Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cde32d7f-3c5e-4810-ab28-07a7abd05c59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T04:51:03.300617Z",
     "iopub.status.busy": "2025-11-13T04:51:03.300194Z",
     "iopub.status.idle": "2025-11-13T04:51:03.307868Z",
     "shell.execute_reply": "2025-11-13T04:51:03.307464Z",
     "shell.execute_reply.started": "2025-11-13T04:51:03.300601Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5efad2e-4006-4b90-920b-1e70cc6b7ca7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T08:32:35.730530Z",
     "iopub.status.busy": "2025-11-13T08:32:35.730399Z",
     "iopub.status.idle": "2025-11-13T08:32:35.735416Z",
     "shell.execute_reply": "2025-11-13T08:32:35.735039Z",
     "shell.execute_reply.started": "2025-11-13T08:32:35.730517Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roots: [0, 3, 10, 23, 40, 169]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_edges(path):\n",
    "    edges = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            raw = line.strip()\n",
    "            if not raw or raw.startswith(\"#\"):\n",
    "                continue\n",
    "            parts = raw.split()\n",
    "            if len(parts) < 2:\n",
    "                continue\n",
    "            try:\n",
    "                u, v = int(parts[0]), int(parts[1])\n",
    "            except ValueError:\n",
    "                continue\n",
    "            edges.append((u, v))\n",
    "    return edges\n",
    "\n",
    "def find_roots(edges):\n",
    "    parents = set()\n",
    "    children = set()\n",
    "    for u, v in edges:\n",
    "        parents.add(u)\n",
    "        children.add(v)\n",
    "    # ë¶€ëª¨ë¡œë§Œ ë‚˜ì˜¨ ì• ë“¤ = ë£¨íŠ¸ë“¤\n",
    "    roots = parents - children\n",
    "    return sorted(roots)\n",
    "\n",
    "# --- ì‚¬ìš© ---\n",
    "E = load_edges(\"Amazon_products/class_hierarchy.txt\")\n",
    "\n",
    "N = 531\n",
    "A = np.zeros((N, N), dtype=np.uint8)\n",
    "for u, v in E:\n",
    "    A[u, v] = 1\n",
    "    A[v, u] = 1   # íƒìƒ‰ìš©ìœ¼ë¡œëŠ” ë¬´ë°©í–¥ ì¸ì ‘í–‰ë ¬ ì¨ë„ ë¨\n",
    "\n",
    "B = np.zeros((N, N), dtype=np.uint8)\n",
    "for u, v in E:\n",
    "    B[u, v] = 1\n",
    "\n",
    "roots = find_roots(E)\n",
    "print(\"roots:\", roots)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b080c703-91ce-4a83-b745-e87986fcddc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T08:32:36.051241Z",
     "iopub.status.busy": "2025-11-13T08:32:36.051014Z",
     "iopub.status.idle": "2025-11-13T08:32:36.053390Z",
     "shell.execute_reply": "2025-11-13T08:32:36.052943Z",
     "shell.execute_reply.started": "2025-11-13T08:32:36.051227Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3db08a7-6154-49f7-b54f-07b725579112",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T08:32:36.380458Z",
     "iopub.status.busy": "2025-11-13T08:32:36.380009Z",
     "iopub.status.idle": "2025-11-13T08:32:38.427674Z",
     "shell.execute_reply": "2025-11-13T08:32:38.426596Z",
     "shell.execute_reply.started": "2025-11-13T08:32:36.380419Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_docs_txt(path):\n",
    "    \"\"\"\n",
    "    'idx<TAB>text' í˜•íƒœì˜ íŒŒì¼ì„ ì½ì–´ì„œ\n",
    "    ids: [int, ...]\n",
    "    texts: [str, ...]\n",
    "    ì„ ë¦¬í„´\n",
    "    \"\"\"\n",
    "    ids = []\n",
    "    texts = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            # íƒ­ ê¸°ì¤€\n",
    "            idx_str, txt = line.split(\"\\t\", 1)\n",
    "            ids.append(int(idx_str))\n",
    "            texts.append(txt)\n",
    "    return ids, texts\n",
    "\n",
    "\n",
    "\n",
    "def build_doc_embeddings_from_existing_vectorizer(doc_texts, vectorizer):\n",
    "    \"\"\"\n",
    "    doc_texts: ì „ì²˜ë¦¬ ì „ì˜ ì›ë¬¸ ë¦¬ìŠ¤íŠ¸\n",
    "    vectorizer: ë¼ë²¨ì— ëŒ€í•´ fitë˜ì–´ ìˆëŠ” TfidfVectorizer\n",
    "    return: dense numpy array [N_docs, vocab]\n",
    "    \"\"\"\n",
    "    # ë¼ë²¨ì´ë‘ ë™ì¼ ê·œì¹™ìœ¼ë¡œ ì „ì²˜ë¦¬\n",
    "    cleaned_docs = [preprocess_label_text(t) for t in doc_texts]\n",
    "    doc_tfidf = vectorizer.transform(cleaned_docs)   # sparse\n",
    "    doc_emb = doc_tfidf.toarray().astype(np.float32)\n",
    "    return doc_emb\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ\n",
    "# 1) ë¬¸ì„œ ì½ê¸°\n",
    "doc_ids, doc_texts = load_docs_txt(\"Amazon_products/train/train_corpus.txt\")\n",
    "\n",
    "# 2) ë¼ë²¨ ë•Œ ë§Œë“  vectorizer ì¬ì‚¬ìš©í•´ì„œ ì„ë² ë”© ë§Œë“¤ê¸°\n",
    "doc_embeddings = build_doc_embeddings_from_existing_vectorizer(doc_texts, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bd3ba19-4969-452b-9bbe-e2cdbfce97e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T08:32:38.428675Z",
     "iopub.status.busy": "2025-11-13T08:32:38.428517Z",
     "iopub.status.idle": "2025-11-13T08:32:38.431346Z",
     "shell.execute_reply": "2025-11-13T08:32:38.430880Z",
     "shell.execute_reply.started": "2025-11-13T08:32:38.428661Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29487, 3466)\n"
     ]
    }
   ],
   "source": [
    "print(doc_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cd05771-6906-4acf-8372-ba0bfd7d4a19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T08:35:00.292721Z",
     "iopub.status.busy": "2025-11-13T08:35:00.292469Z",
     "iopub.status.idle": "2025-11-13T08:35:00.302544Z",
     "shell.execute_reply": "2025-11-13T08:35:00.302090Z",
     "shell.execute_reply.started": "2025-11-13T08:35:00.292705Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "def build_parents_children(adj: np.ndarray):\n",
    "    C = adj.shape[0]\n",
    "    parents = [np.flatnonzero(adj[:, j]).astype(np.int64) for j in range(C)]\n",
    "    children = [np.flatnonzero(adj[j]).astype(np.int64) for j in range(C)]\n",
    "    return parents, children\n",
    "\n",
    "def build_siblings(parents, children):\n",
    "    C = len(parents)\n",
    "    sibs = [set() for _ in range(C)]\n",
    "    for c in range(C):\n",
    "        for p in parents[c]:\n",
    "            for ch in children[p]:\n",
    "                if ch != c:\n",
    "                    sibs[c].add(int(ch))\n",
    "    sibs = [np.array(sorted(s), dtype=np.int64) for s in sibs]\n",
    "    return sibs\n",
    "\n",
    "# --------------------------------\n",
    "# í•œ ë¬¸ì„œì—ì„œ conf(D, c) ê³„ì‚° (candidateë§Œ)\n",
    "# --------------------------------\n",
    "def compute_conf_doc(\n",
    "    sims: np.ndarray,\n",
    "    cand: np.ndarray,\n",
    "    parents,\n",
    "    siblings,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    sims : [C]  (sim(D, c))\n",
    "    cand : [M]  candidate label indexë“¤\n",
    "    return: conf_cand [M]\n",
    "    \"\"\"\n",
    "    conf = np.empty_like(cand, dtype=np.float32)\n",
    "    for k, c in enumerate(cand):\n",
    "        par = parents[c]\n",
    "        sib = siblings[c]\n",
    "        neigh_vals = []\n",
    "\n",
    "        if len(par) > 0:\n",
    "            neigh_vals.append(sims[par].max())\n",
    "        if len(sib) > 0:\n",
    "            neigh_vals.append(sims[sib].max())\n",
    "\n",
    "        neigh_max = max(neigh_vals) if neigh_vals else 0.0\n",
    "        conf[k] = float(sims[c] - neigh_max)\n",
    "    return conf\n",
    "\n",
    "def hierarchical_beam_similarity_avg(\n",
    "    doc_vec: np.ndarray,\n",
    "    label_emb: np.ndarray,\n",
    "    adj_upper: np.ndarray,\n",
    "    roots: list[int] = [0],       # ì—¬ëŸ¬ ë£¨íŠ¸\n",
    "    beam: int = 5,\n",
    "    per_parent: str | int = \"l+2\",\n",
    "    tau: float = 0.35,\n",
    "    eps: float = 1e-9,\n",
    "    max_depth: int | None = None,\n",
    "    normalize: bool = False,      # í•„ìš”í•˜ë©´ Trueë¡œ\n",
    "):\n",
    "    doc = np.asarray(doc_vec, dtype=np.float32)\n",
    "    L = np.asarray(label_emb, dtype=np.float32)\n",
    "    A = np.asarray(adj_upper).astype(bool)\n",
    "    N, d = L.shape\n",
    "\n",
    "    if normalize:\n",
    "        doc = doc / (np.linalg.norm(doc) + eps)\n",
    "        L = L / (np.linalg.norm(L, axis=1, keepdims=True) + eps)\n",
    "\n",
    "    # ë¡œì»¬ ì ìˆ˜\n",
    "    sims = L @ doc\n",
    "    p = 1.0 / (1.0 + np.exp(-sims / max(tau, 1e-6)))\n",
    "\n",
    "    children = [np.flatnonzero(A[i]) for i in range(N)]\n",
    "\n",
    "    S = np.full(N, -np.inf, dtype=np.float32)\n",
    "    K = np.full(N, -np.inf, dtype=np.float32)\n",
    "    Llen = np.zeros(N, dtype=np.int32)\n",
    "\n",
    "    roots = list(roots)\n",
    "    for r in roots:\n",
    "        S[r] = 0.0\n",
    "        Llen[r] = 0\n",
    "        K[r] = -np.inf\n",
    "\n",
    "    levels = [roots[:]]\n",
    "    cur = roots[:]\n",
    "    level_id = 0\n",
    "\n",
    "    while True:\n",
    "        cand_best = {}\n",
    "        if isinstance(per_parent, str) and per_parent.startswith(\"l+\"):\n",
    "            # \"l+2\", \"l+4\" ë“± ì¼ë°˜ ì²˜ë¦¬\n",
    "            offset = int(per_parent[2:])\n",
    "            k_parent = level_id + offset\n",
    "        else:\n",
    "            k_parent = int(per_parent)\n",
    "\n",
    "        for par in cur:\n",
    "            ch = children[par]\n",
    "            if ch.size == 0:\n",
    "                continue\n",
    "            if ch.size > k_parent:\n",
    "                idx = np.argpartition(-sims[ch], k_parent - 1)[:k_parent]\n",
    "                ch = ch[idx]\n",
    "            for c in ch:\n",
    "                S_c = S[par] + float(p[c])\n",
    "                L_c = Llen[par] + 1\n",
    "                K_c = S_c / (L_c + eps)\n",
    "                if (c not in cand_best) or (K_c > cand_best[c][2]):\n",
    "                    cand_best[c] = (S_c, L_c, K_c)\n",
    "\n",
    "        if not cand_best:\n",
    "            break\n",
    "\n",
    "        kept = sorted(cand_best.items(), key=lambda x: x[1][2], reverse=True)[:min(beam, len(cand_best))]\n",
    "        next_level = [i for i, _ in kept]\n",
    "        for i, (Si, Li, Ki) in kept:\n",
    "            S[i], Llen[i], K[i] = Si, Li, Ki\n",
    "\n",
    "        levels.append(next_level)\n",
    "        cur = next_level\n",
    "        level_id += 1\n",
    "        if max_depth is not None and level_id >= max_depth:\n",
    "            break\n",
    "\n",
    "    return K, levels, sims, p\n",
    "\n",
    "\n",
    "\n",
    "def topk_labels_by_avg(\n",
    "    doc_vec, label_emb, adj_upper, rootã„´=(0,), beam=5, per_parent=\"l+2\", k=5, **kw\n",
    "):\n",
    "    \"\"\"í‰ê·  ì ìˆ˜ ê¸°ë°˜ ìµœì¢… ìƒìœ„ k ë¼ë²¨(ë£¨íŠ¸ ì œì™¸).\"\"\"\n",
    "    K, levels, sims, p = hierarchical_beam_similarity_avg(\n",
    "        doc_vec, label_emb, adj_upper, root=list(roots), beam=beam, per_parent=per_parent, **kw\n",
    "    )\n",
    "    root_set = set(roots)\n",
    "    order = np.argsort(-K)\n",
    "    order = [i for i in order if i not in root_set and np.isfinite(K[i])]\n",
    "    top = order[:k]\n",
    "    return top, K[top]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb5f7b9f-1627-4a14-aca0-769891ef07db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T04:06:01.589232Z",
     "iopub.status.busy": "2025-11-11T04:06:01.588974Z",
     "iopub.status.idle": "2025-11-11T04:06:01.597957Z",
     "shell.execute_reply": "2025-11-11T04:06:01.597468Z",
     "shell.execute_reply.started": "2025-11-11T04:06:01.589213Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8e4244ac-1ed8-470c-bc63-1470f460ea33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T09:05:26.407760Z",
     "iopub.status.busy": "2025-11-13T09:05:26.407539Z",
     "iopub.status.idle": "2025-11-13T09:05:26.428028Z",
     "shell.execute_reply": "2025-11-13T09:05:26.427614Z",
     "shell.execute_reply.started": "2025-11-13T09:05:26.407745Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Self-training pipeline with hierarchical silver labeling and dynamic dataloaders.\n",
    "\n",
    "- Reads document/label embeddings CSVs (first column \"id\", rest feat000..feat127)\n",
    "- Reads upper-triangular adjacency (A[i,j]=1 means i->j)\n",
    "- Makes initial silver labels via hierarchical beam search (average score)\n",
    "- Splits into train/val on silver set; keeps the rest as unlabeled pool\n",
    "- Trains a multi-label classifier (Linear/MLP) with BCEWithLogitsLoss\n",
    "- Each epoch, pseudo-labels unlabeled docs whose predicted probs exceed a threshold\n",
    "- Adds them to the training set (up to top_k per doc), with patience-based early stopping\n",
    "\n",
    "Run example\n",
    "-----------\n",
    "python self_training_pipeline.py \\\n",
    "  --doc_csv docs.csv \\\n",
    "  --label_csv labels.csv \\\n",
    "  --adj adj.npy \\\n",
    "  --val_ratio 0.2 --epochs 50 --patience 5 \\\n",
    "  --silver_threshold 0.60 --silver_topk 3 --beam 5 --tau 0.35 --root_id 0 \\\n",
    "  --pseudo_threshold 0.70 --pseudo_topk 3 --batch_size 256 --lr 1e-3\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import argparse\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "def load_embeddings_csv(path: str | Path, id_col: str = \"id\") -> Tuple[List[int], np.ndarray]:\n",
    "    \"\"\"Load embeddings from CSV where the first column is an id and the rest are feature columns.\n",
    "    Returns (ids, float32 matrix).\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    cols = list(df.columns)\n",
    "    if id_col in df.columns:\n",
    "        id_series = df[id_col]\n",
    "        X = df.drop(columns=[id_col])\n",
    "    else:\n",
    "        # Fallback: use the first column as id\n",
    "        id_series = df.iloc[:, 0]\n",
    "        X = df.iloc[:, 1:]\n",
    "    ids = id_series.astype(int).tolist()\n",
    "    X = X.to_numpy(dtype=np.float32)\n",
    "    return ids, X\n",
    "\n",
    "\n",
    "# ----------------------------- Datasets -----------------------------\n",
    "\n",
    "class MultiLabelDataset(Dataset):\n",
    "    def __init__(self, X: np.ndarray, Y: np.ndarray, indices: List[int] | None = None):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.indices = np.array(indices if indices is not None else np.arange(X.shape[0]), dtype=np.int64)\n",
    "    def __len__(self):\n",
    "        return self.indices.shape[0]\n",
    "    def __getitem__(self, idx: int):\n",
    "        i = int(self.indices[idx])\n",
    "        x = torch.from_numpy(self.X[i])\n",
    "        y = torch.from_numpy(self.Y[i])\n",
    "        return x, y\n",
    "\n",
    "class UnlabeledDataset(Dataset):\n",
    "    def __init__(self, X: np.ndarray, indices: List[int]):\n",
    "        self.X = X\n",
    "        self.indices = np.array(indices, dtype=np.int64)\n",
    "    def __len__(self):\n",
    "        return self.indices.shape[0]\n",
    "    def __getitem__(self, idx: int):\n",
    "        i = int(self.indices[idx])\n",
    "        x = torch.from_numpy(self.X[i])\n",
    "        return x, i\n",
    "\n",
    "# ----------------------------- Model -----------------------------\n",
    "\n",
    "class MLPHead(nn.Module):\n",
    "    def __init__(self, in_dim: int, out_dim: int, hidden: int | None = 256, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        if hidden is None or hidden <= 0:\n",
    "            self.net = nn.Sequential(\n",
    "                nn.LayerNorm(in_dim),\n",
    "                nn.Linear(in_dim, out_dim),\n",
    "            )\n",
    "        else:\n",
    "            self.net = nn.Sequential(\n",
    "                nn.LayerNorm(in_dim),\n",
    "                nn.Linear(in_dim, hidden),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(hidden, out_dim),\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# ----------------------------- Utils -----------------------------\n",
    "\n",
    "def to_device(batch, device):\n",
    "    if isinstance(batch, (tuple, list)):\n",
    "        return [b.to(device) if torch.is_tensor(b) else b for b in batch]\n",
    "    return batch.to(device)\n",
    "\n",
    "\n",
    "def micro_f1(y_true: np.ndarray, y_prob: np.ndarray, thr: float = 0.5, eps: float = 1e-9) -> float:\n",
    "    y_pred = (y_prob >= thr).astype(np.float32)\n",
    "    tp = (y_true * y_pred).sum()\n",
    "    fp = ((1 - y_true) * y_pred).sum()\n",
    "    fn = (y_true * (1 - y_pred)).sum()\n",
    "    prec = tp / (tp + fp + eps)\n",
    "    rec = tp / (tp + fn + eps)\n",
    "    f1 = 2 * prec * rec / (prec + rec + eps)\n",
    "    return float(f1)\n",
    "\n",
    "# -------- Initial silver labeling (no CSV save; in-memory) --------\n",
    "\n",
    "# --------------------------------\n",
    "# K-beam + confidence ê¸°ë°˜ silver ìƒì„±\n",
    "# --------------------------------\n",
    "def make_initial_silver(\n",
    "    docs: np.ndarray,         # [N_docs, d_doc]\n",
    "    label_emb: np.ndarray,    # [C, d_lab]\n",
    "    adj_upper: np.ndarray,    # [C, C]  parent -> child (upper adj)\n",
    "    roots: List[int],\n",
    "    silver_topk: int = 3,\n",
    "    beam: int = 15,\n",
    "    per_parent = \"l+4\",\n",
    "    tau: float = 0.35,\n",
    "    min_conf: float = 0.0,    # confê°€ ì´ ê°’ë³´ë‹¤ ì‘ìœ¼ë©´ ë²„ë¦¼\n",
    "    eps: float = 1e-9,\n",
    ") -> List[List[int]]:\n",
    "    \"\"\"\n",
    "    - hierarchical_beam_similarity_avg + conf(D,c) + median í•„í„° ê¸°ë°˜ silver ìƒì„±\n",
    "    - conf(D,c) = sim(D,c) - max_{par/sib} sim(D, c')\n",
    "    - conf(D,c) >= median_c && conf(D,c) >= min_conf ì¸ ë¼ë²¨ë§Œ core í›„ë³´ë¡œ ë‚¨ê¹€\n",
    "    \"\"\"\n",
    "    N_docs = docs.shape[0]\n",
    "    C = label_emb.shape[0]\n",
    "\n",
    "    parents, children = build_parents_children(adj_upper)\n",
    "    siblings = build_siblings(parents, children)\n",
    "\n",
    "    # ë¼ë²¨ë³„ conf ê°’ ëª¨ìœ¼ê¸°\n",
    "    conf_per_label: list[list[float]] = [[] for _ in range(C)]\n",
    "\n",
    "    # ë¬¸ì„œë³„ candidate / conf ì €ì¥ (ë‘ ë²ˆì§¸ passì— ì‚¬ìš©)\n",
    "    cand_per_doc: list[np.ndarray] = []\n",
    "    conf_per_doc: list[np.ndarray] = []\n",
    "\n",
    "    # ---------- 1st pass: K-beam + conf(D,c) ìˆ˜ì§‘ ----------\n",
    "    for d_idx in range(N_docs):\n",
    "        doc_vec = docs[d_idx]\n",
    "\n",
    "        # K-beamìœ¼ë¡œ í›„ë³´ ë¼ë²¨ íƒìƒ‰\n",
    "        K, levels, sims, p = hierarchical_beam_similarity_avg(\n",
    "            doc_vec,\n",
    "            label_emb,\n",
    "            adj_upper,\n",
    "            roots=roots,\n",
    "            beam=beam,\n",
    "            per_parent=per_parent,\n",
    "            tau=tau,\n",
    "            eps=eps,\n",
    "            max_depth=None,\n",
    "            normalize=False,\n",
    "        )\n",
    "\n",
    "        # beamì—ì„œ ë°©ë¬¸ëœ ë¼ë²¨ë“¤ì„ candidateë¡œ\n",
    "        cand = sorted({int(c) for lvl in levels for c in lvl})\n",
    "        cand = np.array(cand, dtype=np.int64)\n",
    "\n",
    "        if cand.size == 0:\n",
    "            cand_per_doc.append(cand)\n",
    "            conf_per_doc.append(np.zeros(0, dtype=np.float32))\n",
    "            continue\n",
    "\n",
    "        sims_doc = sims  # [C]\n",
    "        conf_cand = compute_conf_doc(sims_doc, cand, parents, siblings)\n",
    "\n",
    "        # ë¼ë²¨ë³„ conf ëª©ë¡ì— ì¶”ê°€\n",
    "        for c, cf in zip(cand, conf_cand):\n",
    "            conf_per_label[c].append(float(cf))\n",
    "\n",
    "        cand_per_doc.append(cand)\n",
    "        conf_per_doc.append(conf_cand)\n",
    "\n",
    "    # ---------- ë¼ë²¨ë³„ median(conf) ê³„ì‚° ----------\n",
    "    median_conf = np.full(C, np.inf, dtype=np.float32)\n",
    "    for c in range(C):\n",
    "        vals = conf_per_label[c]\n",
    "        if len(vals) == 0:\n",
    "            continue\n",
    "        median_conf[c] = float(np.median(vals))\n",
    "\n",
    "    # ---------- 2nd pass: median + min_conf ê¸°ì¤€ìœ¼ë¡œ silver í™•ì • ----------\n",
    "    silver: List[List[int]] = []\n",
    "    for cand, conf_cand in zip(cand_per_doc, conf_per_doc):\n",
    "        if cand.size == 0:\n",
    "            silver.append([])\n",
    "            continue\n",
    "\n",
    "        kept = []\n",
    "        for c, cf in zip(cand, conf_cand):\n",
    "            if not np.isfinite(cf):\n",
    "                continue\n",
    "            if cf < median_conf[c]:\n",
    "                continue\n",
    "            if cf < min_conf:\n",
    "                continue\n",
    "            kept.append((int(c), float(cf)))\n",
    "\n",
    "        kept.sort(key=lambda x: x[1], reverse=True)\n",
    "        selected = [c for c, _ in kept[:silver_topk]]\n",
    "        silver.append(selected)\n",
    "\n",
    "    return silver\n",
    "\n",
    "def make_initial_silver_hier_conf(\n",
    "    docs: np.ndarray,         # [N_docs, d_doc]\n",
    "    label_emb: np.ndarray,    # [C, d_lab]\n",
    "    adj_upper: np.ndarray,    # [C, C]  parent -> child\n",
    "    roots: List[int],\n",
    "    silver_topk: int = 3,\n",
    "    beam: int = 15,\n",
    "    per_parent = \"l+4\",\n",
    "    tau: float = 0.35,\n",
    "    silver_threshold: float = 0.65,  # ğŸ”´ p-threshold\n",
    "    min_conf: float = 0.0,           # ğŸ”´ conf-threshold\n",
    "    eps: float = 1e-9,\n",
    ") -> List[List[int]]:\n",
    "    \"\"\"\n",
    "    - hierarchical_beam_similarity_avg + conf(D,c) + median + p-threshold ê¸°ë°˜ silver ìƒì„±\n",
    "    - candidate ì¡°ê±´:\n",
    "        1) beam search í†µí•´ ë°©ë¬¸ëœ ë…¸ë“œ\n",
    "        2) p(D,c) >= silver_threshold\n",
    "    - ìµœì¢… core ì¡°ê±´:\n",
    "        1) conf(D,c) >= median_c\n",
    "        2) conf(D,c) >= min_conf\n",
    "    \"\"\"\n",
    "    N_docs = docs.shape[0]\n",
    "    C = label_emb.shape[0]\n",
    "\n",
    "    parents, children = build_parents_children(adj_upper)\n",
    "    siblings = build_siblings(parents, children)\n",
    "\n",
    "    # ë¼ë²¨ë³„ conf ê°’ ëª¨ìœ¼ê¸°\n",
    "    conf_per_label: list[list[float]] = [[] for _ in range(C)]\n",
    "\n",
    "    # ë¬¸ì„œë³„ candidate / conf ì €ì¥\n",
    "    cand_per_doc: list[np.ndarray] = []\n",
    "    conf_per_doc: list[np.ndarray] = []\n",
    "\n",
    "    # ---------- 1st pass: K-beam + (p, conf) ìˆ˜ì§‘ ----------\n",
    "    for d_idx in range(N_docs):\n",
    "        doc_vec = docs[d_idx]\n",
    "\n",
    "        K, levels, sims, p = hierarchical_beam_similarity_avg(\n",
    "            doc_vec,\n",
    "            label_emb,\n",
    "            adj_upper,\n",
    "            roots=roots,\n",
    "            beam=beam,\n",
    "            per_parent=per_parent,\n",
    "            tau=tau,\n",
    "            eps=eps,\n",
    "            max_depth=None,\n",
    "            normalize=False,\n",
    "        )\n",
    "\n",
    "        # beamì—ì„œ ë°©ë¬¸ëœ ë¼ë²¨ë“¤ì„ candidateë¡œ\n",
    "        cand = sorted({int(c) for lvl in levels for c in lvl})\n",
    "        cand = np.array(cand, dtype=np.int64)\n",
    "\n",
    "        if cand.size == 0:\n",
    "            cand_per_doc.append(cand)\n",
    "            conf_per_doc.append(np.zeros(0, dtype=np.float32))\n",
    "            continue\n",
    "\n",
    "        sims_doc = sims        # [C]\n",
    "        p_doc    = p           # [C]\n",
    "\n",
    "        # ğŸ”´ p-threshold ì ìš©: p >= silver_threshold ì¸ ê²ƒë§Œ ë‚¨ê¹€\n",
    "        mask_thr = p_doc[cand] >= silver_threshold\n",
    "        cand = cand[mask_thr]\n",
    "\n",
    "        if cand.size == 0:\n",
    "            cand_per_doc.append(cand)\n",
    "            conf_per_doc.append(np.zeros(0, dtype=np.float32))\n",
    "            continue\n",
    "\n",
    "        conf_cand = compute_conf_doc(sims_doc, cand, parents, siblings)\n",
    "\n",
    "        # ë¼ë²¨ë³„ conf ëª©ë¡ì— ì¶”ê°€\n",
    "        for c, cf in zip(cand, conf_cand):\n",
    "            conf_per_label[c].append(float(cf))\n",
    "\n",
    "        cand_per_doc.append(cand)\n",
    "        conf_per_doc.append(conf_cand)\n",
    "\n",
    "    # ---------- ë¼ë²¨ë³„ median(conf) ê³„ì‚° ----------\n",
    "    median_conf = np.full(C, np.inf, dtype=np.float32)\n",
    "    for c in range(C):\n",
    "        vals = conf_per_label[c]\n",
    "        if len(vals) == 0:\n",
    "            continue\n",
    "        median_conf[c] = float(np.median(vals))\n",
    "\n",
    "    # ---------- 2nd pass: median + min_conf ê¸°ì¤€ìœ¼ë¡œ silver í™•ì • ----------\n",
    "    silver: List[List[int]] = []\n",
    "    for cand, conf_cand in zip(cand_per_doc, conf_per_doc):\n",
    "        if cand.size == 0:\n",
    "            silver.append([])\n",
    "            continue\n",
    "\n",
    "        kept = []\n",
    "        for c, cf in zip(cand, conf_cand):\n",
    "            if not np.isfinite(cf):\n",
    "                continue\n",
    "            if cf < median_conf[c]:  # median ì¡°ê±´\n",
    "                continue\n",
    "            if cf < min_conf:        # ì ˆëŒ€ threshold ì¡°ê±´\n",
    "                continue\n",
    "            kept.append((int(c), float(cf)))\n",
    "\n",
    "        kept.sort(key=lambda x: x[1], reverse=True)\n",
    "        selected = [c for c, _ in kept[:silver_topk]]\n",
    "        silver.append(selected)\n",
    "\n",
    "    return silver\n",
    "# ------------------------ Train / Self-Training ------------------------\n",
    "\n",
    "def train_epoch(model, loader, optim, device, criterion):\n",
    "    model.train()\n",
    "    total = 0.0\n",
    "    for x, y in loader:\n",
    "        x, y = to_device(x, device), to_device(y, device)\n",
    "        optim.zero_grad(set_to_none=True)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        total += float(loss.detach().cpu().item()) * x.size(0)\n",
    "    return total / max(1, len(loader.dataset))\n",
    "\n",
    "\n",
    "def eval_epoch(model, loader, device, criterion, thr=0.5):\n",
    "    model.eval()\n",
    "    total = 0.0\n",
    "    ys = []\n",
    "    ps = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = to_device(x, device), to_device(y, device)\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            total += float(loss.detach().cpu().item()) * x.size(0)\n",
    "            prob = torch.sigmoid(logits).detach().cpu().numpy()\n",
    "            ys.append(y.detach().cpu().numpy())\n",
    "            ps.append(prob)\n",
    "    y_true = np.concatenate(ys, axis=0)\n",
    "    y_prob = np.concatenate(ps, axis=0)\n",
    "    f1 = micro_f1(y_true, y_prob, thr=thr)\n",
    "    return total / max(1, len(loader.dataset)), f1, y_prob\n",
    "\n",
    "def _compute_conf_for_doc(prob_row: np.ndarray,\n",
    "                          cand: np.ndarray,\n",
    "                          parents,\n",
    "                          siblings) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    prob_row: [C]  (p(D, :))\n",
    "    cand: [M] candidate label index\n",
    "    return: conf_cand [M]\n",
    "    \"\"\"\n",
    "    conf = np.empty(len(cand), dtype=np.float32)\n",
    "    for k, c in enumerate(cand):\n",
    "        par = parents[c]\n",
    "        sib = siblings[c]\n",
    "\n",
    "        neigh_vals = []\n",
    "        if len(par) > 0:\n",
    "            neigh_vals.append(prob_row[par].max())\n",
    "        if len(sib) > 0:\n",
    "            neigh_vals.append(prob_row[sib].max())\n",
    "\n",
    "        neigh_max = max(neigh_vals) if neigh_vals else 0.0\n",
    "        conf[k] = float(prob_row[c] - neigh_max)\n",
    "    return conf\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cccc039-f93f-49de-8843-19c7b5e4338d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d15867a-f278-41ab-88a0-e751c62e5873",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0b4cfc-c593-4841-b847-2759ad663984",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c63ecfd-663d-4ed8-88b2-e8ecbc14de5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T09:05:28.126672Z",
     "iopub.status.busy": "2025-11-13T09:05:28.126439Z",
     "iopub.status.idle": "2025-11-13T09:08:25.322587Z",
     "shell.execute_reply": "2025-11-13T09:08:25.322086Z",
     "shell.execute_reply.started": "2025-11-13T09:05:28.126658Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 29487\n",
      "with silver: 12449\n",
      "unlabeled : 17038\n",
      "9960 2489 17038\n",
      "Epoch 001 | train_loss=0.433  val_loss=0.103  val_f1=0.008\n",
      "  + (skip pseudo-labeling on warmup epoch)\n",
      "Epoch 002 | train_loss=0.052  val_loss=0.037  val_f1=0.080\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 003 | train_loss=0.034  val_loss=0.032  val_f1=0.127\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 004 | train_loss=0.031  val_loss=0.030  val_f1=0.135\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 005 | train_loss=0.030  val_loss=0.029  val_f1=0.135\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 006 | train_loss=0.029  val_loss=0.028  val_f1=0.135\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 007 | train_loss=0.028  val_loss=0.028  val_f1=0.135\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 008 | train_loss=0.028  val_loss=0.028  val_f1=0.135\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 009 | train_loss=0.028  val_loss=0.027  val_f1=0.135\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 010 | train_loss=0.027  val_loss=0.027  val_f1=0.135\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 011 | train_loss=0.027  val_loss=0.027  val_f1=0.135\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 012 | train_loss=0.027  val_loss=0.027  val_f1=0.135\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 013 | train_loss=0.027  val_loss=0.027  val_f1=0.135\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 014 | train_loss=0.027  val_loss=0.027  val_f1=0.136\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 015 | train_loss=0.027  val_loss=0.026  val_f1=0.168\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 016 | train_loss=0.026  val_loss=0.026  val_f1=0.162\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 017 | train_loss=0.026  val_loss=0.026  val_f1=0.194\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 018 | train_loss=0.026  val_loss=0.026  val_f1=0.213\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 019 | train_loss=0.025  val_loss=0.025  val_f1=0.227\n",
      "  + Added 2 pseudo-labeled docs (unl pool â†’ 17036 left)\n",
      "Epoch 020 | train_loss=0.025  val_loss=0.025  val_f1=0.241\n",
      "  + Added 10 pseudo-labeled docs (unl pool â†’ 17026 left)\n",
      "Epoch 021 | train_loss=0.024  val_loss=0.024  val_f1=0.267\n",
      "  + Added 24 pseudo-labeled docs (unl pool â†’ 17002 left)\n",
      "Epoch 022 | train_loss=0.024  val_loss=0.023  val_f1=0.273\n",
      "  + Added 64 pseudo-labeled docs (unl pool â†’ 16938 left)\n",
      "Epoch 023 | train_loss=0.023  val_loss=0.023  val_f1=0.289\n",
      "  + Added 91 pseudo-labeled docs (unl pool â†’ 16847 left)\n",
      "Epoch 024 | train_loss=0.022  val_loss=0.022  val_f1=0.304\n",
      "  + Added 94 pseudo-labeled docs (unl pool â†’ 16753 left)\n",
      "Epoch 025 | train_loss=0.022  val_loss=0.022  val_f1=0.322\n",
      "  + Added 91 pseudo-labeled docs (unl pool â†’ 16662 left)\n",
      "Epoch 026 | train_loss=0.021  val_loss=0.021  val_f1=0.338\n",
      "  + Added 115 pseudo-labeled docs (unl pool â†’ 16547 left)\n",
      "Epoch 027 | train_loss=0.021  val_loss=0.021  val_f1=0.357\n",
      "  + Added 145 pseudo-labeled docs (unl pool â†’ 16402 left)\n",
      "Epoch 028 | train_loss=0.020  val_loss=0.020  val_f1=0.367\n",
      "  + Added 104 pseudo-labeled docs (unl pool â†’ 16298 left)\n",
      "Epoch 029 | train_loss=0.020  val_loss=0.020  val_f1=0.380\n",
      "  + Added 133 pseudo-labeled docs (unl pool â†’ 16165 left)\n",
      "Epoch 030 | train_loss=0.019  val_loss=0.019  val_f1=0.388\n",
      "  + Added 114 pseudo-labeled docs (unl pool â†’ 16051 left)\n",
      "Epoch 031 | train_loss=0.019  val_loss=0.019  val_f1=0.402\n",
      "  + Added 99 pseudo-labeled docs (unl pool â†’ 15952 left)\n",
      "Epoch 032 | train_loss=0.018  val_loss=0.018  val_f1=0.411\n",
      "  + Added 109 pseudo-labeled docs (unl pool â†’ 15843 left)\n",
      "Epoch 033 | train_loss=0.018  val_loss=0.018  val_f1=0.422\n",
      "  + Added 95 pseudo-labeled docs (unl pool â†’ 15748 left)\n",
      "Epoch 034 | train_loss=0.017  val_loss=0.018  val_f1=0.433\n",
      "  + Added 75 pseudo-labeled docs (unl pool â†’ 15673 left)\n",
      "Epoch 035 | train_loss=0.017  val_loss=0.017  val_f1=0.442\n",
      "  + Added 81 pseudo-labeled docs (unl pool â†’ 15592 left)\n",
      "Epoch 036 | train_loss=0.016  val_loss=0.017  val_f1=0.449\n",
      "  + Added 97 pseudo-labeled docs (unl pool â†’ 15495 left)\n",
      "Epoch 037 | train_loss=0.016  val_loss=0.016  val_f1=0.461\n",
      "  + Added 83 pseudo-labeled docs (unl pool â†’ 15412 left)\n",
      "Epoch 038 | train_loss=0.015  val_loss=0.016  val_f1=0.472\n",
      "  + Added 108 pseudo-labeled docs (unl pool â†’ 15304 left)\n",
      "Epoch 039 | train_loss=0.015  val_loss=0.016  val_f1=0.481\n",
      "  + Added 93 pseudo-labeled docs (unl pool â†’ 15211 left)\n",
      "Epoch 040 | train_loss=0.015  val_loss=0.015  val_f1=0.488\n",
      "  + Added 72 pseudo-labeled docs (unl pool â†’ 15139 left)\n",
      "Epoch 041 | train_loss=0.014  val_loss=0.015  val_f1=0.495\n",
      "  + Added 79 pseudo-labeled docs (unl pool â†’ 15060 left)\n",
      "Epoch 042 | train_loss=0.014  val_loss=0.015  val_f1=0.500\n",
      "  + Added 119 pseudo-labeled docs (unl pool â†’ 14941 left)\n",
      "Epoch 043 | train_loss=0.013  val_loss=0.015  val_f1=0.506\n",
      "  + Added 107 pseudo-labeled docs (unl pool â†’ 14834 left)\n",
      "Epoch 044 | train_loss=0.013  val_loss=0.014  val_f1=0.514\n",
      "  + Added 75 pseudo-labeled docs (unl pool â†’ 14759 left)\n",
      "Epoch 045 | train_loss=0.013  val_loss=0.014  val_f1=0.518\n",
      "  + Added 120 pseudo-labeled docs (unl pool â†’ 14639 left)\n",
      "Epoch 046 | train_loss=0.012  val_loss=0.014  val_f1=0.523\n",
      "  + Added 123 pseudo-labeled docs (unl pool â†’ 14516 left)\n",
      "Epoch 047 | train_loss=0.012  val_loss=0.014  val_f1=0.531\n",
      "  + Added 107 pseudo-labeled docs (unl pool â†’ 14409 left)\n",
      "Epoch 048 | train_loss=0.012  val_loss=0.013  val_f1=0.535\n",
      "  + Added 113 pseudo-labeled docs (unl pool â†’ 14296 left)\n",
      "Epoch 049 | train_loss=0.011  val_loss=0.013  val_f1=0.538\n",
      "  + Added 122 pseudo-labeled docs (unl pool â†’ 14174 left)\n",
      "Epoch 050 | train_loss=0.011  val_loss=0.013  val_f1=0.546\n",
      "  + Added 120 pseudo-labeled docs (unl pool â†’ 14054 left)\n",
      "Epoch 051 | train_loss=0.011  val_loss=0.013  val_f1=0.552\n",
      "  + Added 91 pseudo-labeled docs (unl pool â†’ 13963 left)\n",
      "Epoch 052 | train_loss=0.010  val_loss=0.012  val_f1=0.556\n",
      "  + Added 157 pseudo-labeled docs (unl pool â†’ 13806 left)\n",
      "Epoch 053 | train_loss=0.010  val_loss=0.012  val_f1=0.564\n",
      "  + Added 95 pseudo-labeled docs (unl pool â†’ 13711 left)\n",
      "Epoch 054 | train_loss=0.010  val_loss=0.012  val_f1=0.565\n",
      "  + Added 135 pseudo-labeled docs (unl pool â†’ 13576 left)\n",
      "Epoch 055 | train_loss=0.010  val_loss=0.012  val_f1=0.573\n",
      "  + Added 124 pseudo-labeled docs (unl pool â†’ 13452 left)\n",
      "Epoch 056 | train_loss=0.009  val_loss=0.012  val_f1=0.578\n",
      "  + Added 129 pseudo-labeled docs (unl pool â†’ 13323 left)\n",
      "Epoch 057 | train_loss=0.009  val_loss=0.012  val_f1=0.581\n",
      "  + Added 150 pseudo-labeled docs (unl pool â†’ 13173 left)\n",
      "Epoch 058 | train_loss=0.009  val_loss=0.011  val_f1=0.586\n",
      "  + Added 122 pseudo-labeled docs (unl pool â†’ 13051 left)\n",
      "Epoch 059 | train_loss=0.009  val_loss=0.011  val_f1=0.589\n",
      "  + Added 120 pseudo-labeled docs (unl pool â†’ 12931 left)\n",
      "Epoch 060 | train_loss=0.008  val_loss=0.011  val_f1=0.593\n",
      "  + Added 161 pseudo-labeled docs (unl pool â†’ 12770 left)\n",
      "Epoch 061 | train_loss=0.008  val_loss=0.011  val_f1=0.597\n",
      "  + Added 109 pseudo-labeled docs (unl pool â†’ 12661 left)\n",
      "Epoch 062 | train_loss=0.008  val_loss=0.011  val_f1=0.598\n",
      "  + Added 131 pseudo-labeled docs (unl pool â†’ 12530 left)\n",
      "Epoch 063 | train_loss=0.008  val_loss=0.011  val_f1=0.604\n",
      "  + Added 117 pseudo-labeled docs (unl pool â†’ 12413 left)\n",
      "Epoch 064 | train_loss=0.007  val_loss=0.011  val_f1=0.605\n",
      "  + Added 155 pseudo-labeled docs (unl pool â†’ 12258 left)\n",
      "Epoch 065 | train_loss=0.007  val_loss=0.011  val_f1=0.611\n",
      "  + Added 103 pseudo-labeled docs (unl pool â†’ 12155 left)\n",
      "Epoch 066 | train_loss=0.007  val_loss=0.010  val_f1=0.613\n",
      "  + Added 90 pseudo-labeled docs (unl pool â†’ 12065 left)\n",
      "Epoch 067 | train_loss=0.007  val_loss=0.010  val_f1=0.613\n",
      "  + Added 105 pseudo-labeled docs (unl pool â†’ 11960 left)\n",
      "Epoch 068 | train_loss=0.006  val_loss=0.010  val_f1=0.615\n",
      "  + Added 101 pseudo-labeled docs (unl pool â†’ 11859 left)\n",
      "Epoch 069 | train_loss=0.006  val_loss=0.010  val_f1=0.616\n",
      "  + Added 105 pseudo-labeled docs (unl pool â†’ 11754 left)\n",
      "Epoch 070 | train_loss=0.006  val_loss=0.010  val_f1=0.618\n",
      "  + Added 79 pseudo-labeled docs (unl pool â†’ 11675 left)\n",
      "Epoch 071 | train_loss=0.006  val_loss=0.010  val_f1=0.621\n",
      "  + Added 98 pseudo-labeled docs (unl pool â†’ 11577 left)\n",
      "Epoch 072 | train_loss=0.006  val_loss=0.010  val_f1=0.621\n",
      "  + Added 105 pseudo-labeled docs (unl pool â†’ 11472 left)\n",
      "Epoch 073 | train_loss=0.006  val_loss=0.010  val_f1=0.624\n",
      "  + Added 134 pseudo-labeled docs (unl pool â†’ 11338 left)\n",
      "Epoch 074 | train_loss=0.005  val_loss=0.010  val_f1=0.624\n",
      "  + Added 105 pseudo-labeled docs (unl pool â†’ 11233 left)\n",
      "Epoch 075 | train_loss=0.005  val_loss=0.010  val_f1=0.626\n",
      "  + Added 88 pseudo-labeled docs (unl pool â†’ 11145 left)\n",
      "Epoch 076 | train_loss=0.005  val_loss=0.010  val_f1=0.627\n",
      "  + Added 86 pseudo-labeled docs (unl pool â†’ 11059 left)\n",
      "Epoch 077 | train_loss=0.005  val_loss=0.010  val_f1=0.630\n",
      "  + Added 83 pseudo-labeled docs (unl pool â†’ 10976 left)\n",
      "Epoch 078 | train_loss=0.005  val_loss=0.010  val_f1=0.630\n",
      "  + Added 85 pseudo-labeled docs (unl pool â†’ 10891 left)\n",
      "Epoch 079 | train_loss=0.005  val_loss=0.010  val_f1=0.631\n",
      "  + Added 79 pseudo-labeled docs (unl pool â†’ 10812 left)\n",
      "Epoch 080 | train_loss=0.004  val_loss=0.010  val_f1=0.632\n",
      "  + Added 79 pseudo-labeled docs (unl pool â†’ 10733 left)\n",
      "Epoch 081 | train_loss=0.004  val_loss=0.010  val_f1=0.633\n",
      "  + Added 95 pseudo-labeled docs (unl pool â†’ 10638 left)\n",
      "Epoch 082 | train_loss=0.004  val_loss=0.010  val_f1=0.634\n",
      "  + Added 103 pseudo-labeled docs (unl pool â†’ 10535 left)\n",
      "Epoch 083 | train_loss=0.004  val_loss=0.010  val_f1=0.633\n",
      "  + Added 73 pseudo-labeled docs (unl pool â†’ 10462 left)\n",
      "Epoch 084 | train_loss=0.004  val_loss=0.010  val_f1=0.635\n",
      "  + Added 78 pseudo-labeled docs (unl pool â†’ 10384 left)\n",
      "Epoch 085 | train_loss=0.004  val_loss=0.010  val_f1=0.636\n",
      "  + Added 80 pseudo-labeled docs (unl pool â†’ 10304 left)\n",
      "Epoch 086 | train_loss=0.004  val_loss=0.010  val_f1=0.637\n",
      "  + Added 74 pseudo-labeled docs (unl pool â†’ 10230 left)\n",
      "Epoch 087 | train_loss=0.004  val_loss=0.010  val_f1=0.637\n",
      "  + Added 113 pseudo-labeled docs (unl pool â†’ 10117 left)\n",
      "Epoch 088 | train_loss=0.003  val_loss=0.010  val_f1=0.640\n",
      "  + Added 85 pseudo-labeled docs (unl pool â†’ 10032 left)\n",
      "Epoch 089 | train_loss=0.003  val_loss=0.010  val_f1=0.639\n",
      "  + Added 64 pseudo-labeled docs (unl pool â†’ 9968 left)\n",
      "Epoch 090 | train_loss=0.003  val_loss=0.010  val_f1=0.640\n",
      "  + Added 66 pseudo-labeled docs (unl pool â†’ 9902 left)\n",
      "Epoch 091 | train_loss=0.003  val_loss=0.010  val_f1=0.640\n",
      "  + Added 83 pseudo-labeled docs (unl pool â†’ 9819 left)\n",
      "Epoch 092 | train_loss=0.003  val_loss=0.010  val_f1=0.640\n",
      "  + Added 67 pseudo-labeled docs (unl pool â†’ 9752 left)\n",
      "Epoch 093 | train_loss=0.003  val_loss=0.010  val_f1=0.642\n",
      "  + Added 38 pseudo-labeled docs (unl pool â†’ 9714 left)\n",
      "Epoch 094 | train_loss=0.003  val_loss=0.010  val_f1=0.642\n",
      "  + Added 40 pseudo-labeled docs (unl pool â†’ 9674 left)\n",
      "Epoch 095 | train_loss=0.003  val_loss=0.010  val_f1=0.641\n",
      "  + Added 35 pseudo-labeled docs (unl pool â†’ 9639 left)\n",
      "Epoch 096 | train_loss=0.003  val_loss=0.010  val_f1=0.640\n",
      "  + Added 59 pseudo-labeled docs (unl pool â†’ 9580 left)\n",
      "Epoch 097 | train_loss=0.003  val_loss=0.010  val_f1=0.641\n",
      "  + Added 74 pseudo-labeled docs (unl pool â†’ 9506 left)\n",
      "Epoch 098 | train_loss=0.003  val_loss=0.010  val_f1=0.642\n",
      "  + Added 70 pseudo-labeled docs (unl pool â†’ 9436 left)\n",
      "Epoch 099 | train_loss=0.002  val_loss=0.010  val_f1=0.640\n",
      "  + Added 67 pseudo-labeled docs (unl pool â†’ 9369 left)\n",
      "Epoch 100 | train_loss=0.002  val_loss=0.010  val_f1=0.640\n",
      "  + Added 48 pseudo-labeled docs (unl pool â†’ 9321 left)\n",
      "Epoch 101 | train_loss=0.002  val_loss=0.010  val_f1=0.639\n",
      "  + Added 49 pseudo-labeled docs (unl pool â†’ 9272 left)\n",
      "Epoch 102 | train_loss=0.002  val_loss=0.010  val_f1=0.641\n",
      "  + Added 40 pseudo-labeled docs (unl pool â†’ 9232 left)\n",
      "Epoch 103 | train_loss=0.002  val_loss=0.011  val_f1=0.642\n",
      "  + Added 43 pseudo-labeled docs (unl pool â†’ 9189 left)\n",
      "Epoch 104 | train_loss=0.002  val_loss=0.011  val_f1=0.642\n",
      "  + Added 54 pseudo-labeled docs (unl pool â†’ 9135 left)\n",
      "Epoch 105 | train_loss=0.002  val_loss=0.011  val_f1=0.642\n",
      "  + Added 43 pseudo-labeled docs (unl pool â†’ 9092 left)\n",
      "Epoch 106 | train_loss=0.002  val_loss=0.011  val_f1=0.641\n",
      "  + Added 47 pseudo-labeled docs (unl pool â†’ 9045 left)\n",
      "Epoch 107 | train_loss=0.002  val_loss=0.011  val_f1=0.641\n",
      "  + Added 52 pseudo-labeled docs (unl pool â†’ 8993 left)\n",
      "Epoch 108 | train_loss=0.002  val_loss=0.011  val_f1=0.641\n",
      "  + Added 52 pseudo-labeled docs (unl pool â†’ 8941 left)\n",
      "Epoch 109 | train_loss=0.002  val_loss=0.011  val_f1=0.640\n",
      "  + Added 48 pseudo-labeled docs (unl pool â†’ 8893 left)\n",
      "Epoch 110 | train_loss=0.002  val_loss=0.011  val_f1=0.640\n",
      "  + Added 47 pseudo-labeled docs (unl pool â†’ 8846 left)\n",
      "Epoch 111 | train_loss=0.002  val_loss=0.011  val_f1=0.640\n",
      "  + Added 62 pseudo-labeled docs (unl pool â†’ 8784 left)\n",
      "Epoch 112 | train_loss=0.002  val_loss=0.011  val_f1=0.640\n",
      "  + Added 46 pseudo-labeled docs (unl pool â†’ 8738 left)\n",
      "Epoch 113 | train_loss=0.002  val_loss=0.011  val_f1=0.641\n",
      "  + Added 35 pseudo-labeled docs (unl pool â†’ 8703 left)\n",
      "Epoch 114 | train_loss=0.002  val_loss=0.011  val_f1=0.641\n",
      "Early stopping at epoch 114 (best f1=0.6424)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "\n",
    "doc_ids = np.arange(len(doc_embeddings), dtype=np.int64)   # 0..num_docs-1\n",
    "X = doc_embeddings.astype(np.float32)                      # [num_docs, d_doc]\n",
    "\n",
    "# ë¼ë²¨ ì„ë² ë”© ì„¸íŒ…\n",
    "label_ids = np.arange(len(label_keys), dtype=np.int64)     # 0..530\n",
    "L = np.vstack([label_embeddings[k] for k in label_keys]).astype(np.float32)   # [531, d_label]\n",
    "\n",
    "\n",
    "# 1) ë¼ë²¨ ìˆœì„œì™€ B(ë¶€ëª¨->ìì‹) ë§ì¶”ê¸°\n",
    "order = np.argsort(label_ids)\n",
    "label_ids = [label_ids[i] for i in order]\n",
    "L = L[order]\n",
    "assert B.shape == (L.shape[0], L.shape[0]), \"Adjacency/label size mismatch\"\n",
    "\n",
    "# 2) ê³„ì¸µ silver ë§Œë“¤ê¸°\n",
    "silver = make_initial_silver_hier_conf(\n",
    "    X,               # ë¬¸ì„œ ì„ë² ë”©\n",
    "    L,               # ë¼ë²¨ ì„ë² ë”© (TF-IDFë“  GATë“ )\n",
    "    B,               # parent->child adj\n",
    "    roots=roots,\n",
    "    silver_topk=3,\n",
    "    beam=15,\n",
    "    per_parent=\"l+4\",\n",
    "    tau=0.35,\n",
    "    silver_threshold=0.6,  # p(D,c) threshold\n",
    "    min_conf=0.0,           # conf threshold (í•„ìš”í•˜ë©´ 0.05 ê°™ì€ ê°’ìœ¼ë¡œ ì˜¬ë ¤ë„ ë¨)\n",
    ")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 1) ê³„ì¸µ ì •ë³´ì—ì„œ parents / children ë½‘ê¸°\n",
    "#    B[parent, child] = 1 ì´ë¼ê³  í–ˆìœ¼ë‹ˆê¹Œ ê·¸ëŒ€ë¡œ ì”€\n",
    "# -------------------------------------------------\n",
    "# B: [C, C] (parent -> child)\n",
    "def build_parents_children(adj):\n",
    "    C = adj.shape[0]\n",
    "    parents = [np.flatnonzero(adj[:, j]).astype(np.int64) for j in range(C)]\n",
    "    children = [np.flatnonzero(adj[j]).astype(np.int64) for j in range(C)]\n",
    "    return parents, children\n",
    "\n",
    "parents, children = build_parents_children(B)\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2) silver â†’ ê³„ì¸µ pos/neg ë§ˆìŠ¤í¬ë¡œ ë³€í™˜\n",
    "# -------------------------------------------------\n",
    "def build_pos_neg_masks(silver, parents, children, num_labels):\n",
    "    \"\"\"\n",
    "    silver: list[list[int]]  # ë¬¸ì„œë§ˆë‹¤ core label indexë“¤\n",
    "    parents / children: list[np.ndarray]\n",
    "    return:\n",
    "      pos_masks: np.array [N_docs, C]\n",
    "      neg_masks: np.array [N_docs, C]\n",
    "    \"\"\"\n",
    "    N = len(silver)\n",
    "    C = num_labels\n",
    "    pos_masks = np.zeros((N, C), dtype=np.float32)\n",
    "    neg_masks = np.zeros((N, C), dtype=np.float32)\n",
    "\n",
    "    all_idx = np.arange(C)\n",
    "\n",
    "    for i, core in enumerate(silver):\n",
    "        core = list(core)\n",
    "        # 1) coreì˜ ë¶€ëª¨ê¹Œì§€ positive\n",
    "        pos_set = set(core)\n",
    "        for c in core:\n",
    "            for p in parents[c]:\n",
    "                pos_set.add(int(p))\n",
    "\n",
    "        # 2) childrenì€ ë‚˜ì¤‘ì— negativeì—ì„œ ì œì™¸\n",
    "        child_set = set()\n",
    "        for c in core:\n",
    "            for ch in children[c]:\n",
    "                child_set.add(int(ch))\n",
    "\n",
    "        # pos ë§ˆìŠ¤í¬\n",
    "        for p in pos_set:\n",
    "            pos_masks[i, p] = 1.0\n",
    "\n",
    "        # neg = ì „ì²´ - pos - children\n",
    "        for j in all_idx:\n",
    "            if j in pos_set:\n",
    "                continue\n",
    "            if j in child_set:\n",
    "                continue\n",
    "            neg_masks[i, j] = 1.0\n",
    "\n",
    "    return pos_masks, neg_masks\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3) Dataset: ë¬¸ì„œ ì„ë² ë”© + pos/neg ë§ˆìŠ¤í¬\n",
    "# -------------------------------------------------\n",
    "class HierMultiLabelDataset(Dataset):\n",
    "    def __init__(self, X, pos_masks, neg_masks, indices=None):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.pos = pos_masks.astype(np.float32)\n",
    "        self.neg = neg_masks.astype(np.float32)\n",
    "        if indices is None:\n",
    "            self.indices = np.arange(self.X.shape[0], dtype=np.int64)\n",
    "        else:\n",
    "            self.indices = np.array(indices, dtype=np.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.indices.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i = int(self.indices[idx])\n",
    "        x = torch.from_numpy(self.X[i])\n",
    "        pos = torch.from_numpy(self.pos[i])\n",
    "        neg = torch.from_numpy(self.neg[i])\n",
    "        return x, pos, neg\n",
    "\n",
    "class UnlabeledDataset(Dataset):\n",
    "    def __init__(self, X, indices):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.indices = np.array(indices, dtype=np.int64)\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    def __getitem__(self, idx):\n",
    "        i = int(self.indices[idx])\n",
    "        return torch.from_numpy(self.X[i]), i\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 4) Bilinear classifier\n",
    "#    doc_emb: [B, d_doc]\n",
    "#    label_emb: [C, d_lab]  (ë¯¸ë¦¬ GATë¡œ ë§Œë“  ê±°)\n",
    "#    ì ìˆ˜: doc @ W @ label_emb^T\n",
    "# -------------------------------------------------\n",
    "class BilinearHierClassifier(nn.Module):\n",
    "    def __init__(self, doc_dim, label_emb, hidden_dim=None):\n",
    "        super().__init__()\n",
    "        # label_embëŠ” íŒŒë¼ë¯¸í„°ë¡œ ë“¤ê³ ìˆë˜, ì—…ë°ì´íŠ¸ ì•ˆ í•œë‹¤ê³  ê°€ì •(ì›í•˜ë©´ nn.Parameterë¡œ)\n",
    "        self.register_buffer(\"label_emb\", torch.tensor(label_emb, dtype=torch.float32))\n",
    "        C, d_lab = self.label_emb.shape\n",
    "        self.doc_dim = doc_dim\n",
    "        self.label_dim = d_lab\n",
    "\n",
    "        if hidden_dim is None:\n",
    "            # ë°”ë¡œ doc_dim -> label_dim\n",
    "            self.interaction = nn.Linear(doc_dim, d_lab, bias=False)\n",
    "            self.proj = None\n",
    "        else:\n",
    "            # doc_dim -> hidden -> label_dim ê°™ì€ ê²ƒë„ ê°€ëŠ¥\n",
    "            self.interaction = nn.Sequential(\n",
    "                nn.Linear(doc_dim, hidden_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(hidden_dim, d_lab, bias=False),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [B, d_doc]\n",
    "        return: logits [B, C]\n",
    "        \"\"\"\n",
    "        # x -> same dim as label\n",
    "        h = self.interaction(x)                             # [B, d_lab]\n",
    "        # [B, d_lab] @ [d_lab, C] -> [B, C]\n",
    "        logits = torch.matmul(h, self.label_emb.t())\n",
    "        return logits\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 5) loss: ê³„ì¸µ ë§ˆìŠ¤í¬ë¥¼ ì”Œìš´ BCE\n",
    "# -------------------------------------------------\n",
    "def hierarchical_bce_loss(logits, pos_mask, neg_mask):\n",
    "    # logits: [B, C]\n",
    "    # pos_mask, neg_mask: [B, C]\n",
    "    loss_pos = -(pos_mask * F.logsigmoid(logits)).sum()\n",
    "    loss_neg = -(neg_mask * F.logsigmoid(-logits)).sum()\n",
    "    denom = (pos_mask.sum() + neg_mask.sum()).clamp(min=1.0)\n",
    "    return (loss_pos + loss_neg) / denom\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 6) í•™ìŠµ ë£¨í”„ ì˜ˆì‹œ\n",
    "# -------------------------------------------------\n",
    "# ì´ë¯¸ ìˆëŠ” ê²ƒë“¤: X (ë¬¸ì„œ BERT ì„ë² ë”©) : [N_docs, d_doc]\n",
    "#                  L (ë¼ë²¨ GAT ì„ë² ë”©)  : [C, d_lab]\n",
    "#                  B_adj (ë¶€ëª¨->ìì‹)   : [C, C]\n",
    "#                  silver (list[list[int]]) : ë¬¸ì„œë³„ core label index\n",
    "def train_epoch_hier(model, loader, opt, device):\n",
    "    model.train()\n",
    "    total = 0.0\n",
    "    for xb, posb, negb in loader:\n",
    "        xb = xb.to(device)\n",
    "        posb = posb.to(device)\n",
    "        negb = negb.to(device)\n",
    "        logits = model(xb)\n",
    "        loss = hierarchical_bce_loss(logits, posb, negb)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        total += loss.item() * xb.size(0)\n",
    "    return total / len(loader.dataset)\n",
    "\n",
    "# 1) micro F1 ê³„ì‚°\n",
    "def micro_f1_from_logits(logits, pos_mask, thr=0.5, eps=1e-9):\n",
    "    \"\"\"\n",
    "    logits: [B, C]\n",
    "    pos_mask: [B, C]  (1: positive, 0: else)\n",
    "    \"\"\"\n",
    "    probs = torch.sigmoid(logits)\n",
    "    preds = (probs >= thr).float()\n",
    "\n",
    "    y_true = pos_mask\n",
    "    y_pred = preds\n",
    "\n",
    "    tp = (y_true * y_pred).sum()\n",
    "    fp = ((1 - y_true) * y_pred).sum()\n",
    "    fn = (y_true * (1 - y_pred)).sum()\n",
    "\n",
    "    precision = tp / (tp + fp + eps)\n",
    "    recall    = tp / (tp + fn + eps)\n",
    "    f1 = 2 * precision * recall / (precision + recall + eps)\n",
    "    return f1.item()\n",
    "\n",
    "# 2) eval í•¨ìˆ˜ ìˆ˜ì •: loss + f1 ë‘˜ ë‹¤\n",
    "def eval_epoch_hier(model, loader, device, k=3, thr=None):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    f1_list = []\n",
    "    with torch.no_grad():\n",
    "        for xb, posb, negb in loader:\n",
    "            xb = xb.to(device)\n",
    "            posb = posb.to(device)\n",
    "            negb = negb.to(device)\n",
    "\n",
    "            logits = model(xb)\n",
    "            loss = hierarchical_bce_loss(logits, posb, negb)  # ìœ„ì— ë°”ê¾¼ ë²„ì „\n",
    "            total_loss += loss.item() * xb.size(0)\n",
    "\n",
    "            probs = torch.sigmoid(logits)\n",
    "\n",
    "            if thr is not None:\n",
    "                pred = (probs >= thr).float()\n",
    "            else:\n",
    "                # top-k ë°©ì‹\n",
    "                B, C = probs.shape\n",
    "                pred = torch.zeros_like(probs)\n",
    "                topk = probs.topk(k, dim=1).indices\n",
    "                pred.scatter_(1, topk, 1.0)\n",
    "\n",
    "            # micro-f1\n",
    "            y_true = posb\n",
    "            y_pred = pred\n",
    "            tp = (y_true * y_pred).sum().item()\n",
    "            fp = ((1 - y_true) * y_pred).sum().item()\n",
    "            fn = (y_true * (1 - y_pred)).sum().item()\n",
    "            prec = tp / (tp + fp + 1e-9)\n",
    "            rec  = tp / (tp + fn + 1e-9)\n",
    "            f1   = 2 * prec * rec / (prec + rec + 1e-9)\n",
    "            f1_list.append(f1)\n",
    "\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    avg_f1 = float(np.mean(f1_list)) if f1_list else 0.0\n",
    "    return avg_loss, avg_f1\n",
    "def pseudo_label_and_grow_hier_conf(\n",
    "    model,\n",
    "    unl_ds,             # UnlabeledDataset\n",
    "    parents,\n",
    "    children,\n",
    "    num_labels,\n",
    "    device,\n",
    "    pseudo_threshold: float = 0.45,   # p(D,c) threshold\n",
    "    pseudo_topk: int = 3,\n",
    "    batch_size: int = 512,\n",
    "    max_candidates: int = 32,         # í•œ ë¬¸ì„œë‹¹ ìµœëŒ€ í›„ë³´ ë¼ë²¨ ìˆ˜\n",
    "):\n",
    "    \"\"\"\n",
    "    - unl_dsì—ì„œ pseudo-labelë¡œ ì“¸ ë¬¸ì„œë¥¼ ë½‘ê³ \n",
    "    - confidence + label-wise median ê¸°ë°˜ìœ¼ë¡œ core ë¼ë²¨ ê²°ì •\n",
    "    - core + ë¶€ëª¨ = pos, ìì‹ = unknown, ë‚˜ë¨¸ì§€ = neg\n",
    "    \"\"\"\n",
    "    if len(unl_ds) == 0:\n",
    "        return [], None, None\n",
    "\n",
    "    loader = DataLoader(unl_ds, batch_size=batch_size, shuffle=False)\n",
    "    model.eval()\n",
    "\n",
    "    C = num_labels\n",
    "    siblings = build_siblings(parents, children)\n",
    "\n",
    "    # 1-pass: í›„ë³´ ë¼ë²¨, conf, prob ìˆ˜ì§‘ + ë¼ë²¨ë³„ conf ë¶„í¬ ì €ì¥\n",
    "    conf_per_label = [[] for _ in range(C)]\n",
    "\n",
    "    doc_indices = []         # ê° entry: i_doc (ì›ë³¸ ì¸ë±ìŠ¤)\n",
    "    cand_list = []           # ê° entry: np.array [M_doc]\n",
    "    conf_cand_list = []      # ê° entry: np.array [M_doc]\n",
    "    prob_cand_list = []      # ê° entry: np.array [M_doc]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, idxs in loader:\n",
    "            xb = xb.to(device)\n",
    "            logits = model(xb)\n",
    "            prob = torch.sigmoid(logits).cpu().numpy()   # [B, C]\n",
    "            idxs_np = idxs.cpu().numpy().tolist()\n",
    "\n",
    "            for p_row, i_doc in zip(prob, idxs_np):\n",
    "                # í™•ë¥  ë‚´ë¦¼ì°¨ìˆœ\n",
    "                order = np.argsort(-p_row)\n",
    "\n",
    "                # p >= pseudo_thresholdì¸ ì• ë“¤ ì¤‘ ìƒìœ„ max_candidatesë§Œ í›„ë³´\n",
    "                cand = [j for j in order[:max_candidates] if p_row[j] >= pseudo_threshold]\n",
    "                if len(cand) == 0:\n",
    "                    continue\n",
    "\n",
    "                cand = np.array(cand, dtype=np.int64)\n",
    "                conf_cand = _compute_conf_for_doc(p_row, cand, parents, siblings)\n",
    "                prob_cand = p_row[cand].astype(np.float32)\n",
    "\n",
    "                # ë¼ë²¨ë³„ conf ë¶„í¬ì— ì¶”ê°€\n",
    "                for c, cf in zip(cand, conf_cand):\n",
    "                    conf_per_label[c].append(float(cf))\n",
    "\n",
    "                doc_indices.append(int(i_doc))\n",
    "                cand_list.append(cand)\n",
    "                conf_cand_list.append(conf_cand)\n",
    "                prob_cand_list.append(prob_cand)\n",
    "\n",
    "    if not doc_indices:\n",
    "        # ì´ë²ˆ epochì—ëŠ” ì•„ë¬´ê²ƒë„ pseudo-labelë¡œ ì•ˆ ë„£ìŒ\n",
    "        return [], None, None\n",
    "\n",
    "    # ë¼ë²¨ë³„ median(conf) ê³„ì‚°\n",
    "    median_conf = np.full(C, -np.inf, dtype=np.float32)\n",
    "    for c in range(C):\n",
    "        vals = conf_per_label[c]\n",
    "        if len(vals) == 0:\n",
    "            continue\n",
    "        median_conf[c] = float(np.median(vals))\n",
    "\n",
    "    # 2-pass: median ê¸°ë°˜ìœ¼ë¡œ ì‹¤ì œ core ë¼ë²¨ ì„ íƒ + pos/neg ë§ˆìŠ¤í¬ ìƒì„±\n",
    "    new_idx = []\n",
    "    new_pos_list = []\n",
    "    new_neg_list = []\n",
    "\n",
    "    for i_doc, cand, conf_cand, prob_cand in zip(\n",
    "        doc_indices, cand_list, conf_cand_list, prob_cand_list\n",
    "    ):\n",
    "        keep = []\n",
    "        for c, cf, pc in zip(cand, conf_cand, prob_cand):\n",
    "            # confê°€ ë¼ë²¨ë³„ median ì´ìƒì¼ ë•Œë§Œ ì±„íƒ\n",
    "            if cf < median_conf[c]:\n",
    "                continue\n",
    "            keep.append((int(c), float(pc), float(cf)))\n",
    "\n",
    "        if not keep:\n",
    "            continue\n",
    "\n",
    "        # í™•ë¥  ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ í›„ top-k\n",
    "        keep.sort(key=lambda x: x[1], reverse=True)  # x[1] = prob\n",
    "        core = [c for c, _, _ in keep[:pseudo_topk]]\n",
    "\n",
    "        if len(core) == 0:\n",
    "            continue\n",
    "\n",
    "        # ê³„ì¸µ pos/neg êµ¬ì„± (ê¸°ì¡´ í•¨ìˆ˜ì™€ ë™ì¼ ë¡œì§)\n",
    "        pos = set(core)\n",
    "        for c in core:\n",
    "            for pa in parents[c]:\n",
    "                pos.add(int(pa))\n",
    "\n",
    "        child = set()\n",
    "        for c in core:\n",
    "            for ch in children[c]:\n",
    "                child.add(int(ch))\n",
    "\n",
    "        pos_mask = np.zeros(C, dtype=np.float32)\n",
    "        neg_mask = np.zeros(C, dtype=np.float32)\n",
    "\n",
    "        for j in pos:\n",
    "            pos_mask[j] = 1.0\n",
    "        for j in range(C):\n",
    "            if j in pos:\n",
    "                continue\n",
    "            if j in child:\n",
    "                continue\n",
    "            neg_mask[j] = 1.0\n",
    "\n",
    "        new_idx.append(int(i_doc))\n",
    "        new_pos_list.append(pos_mask)\n",
    "        new_neg_list.append(neg_mask)\n",
    "\n",
    "    if len(new_idx) == 0:\n",
    "        return [], None, None\n",
    "\n",
    "    new_pos = np.stack(new_pos_list, axis=0)\n",
    "    new_neg = np.stack(new_neg_list, axis=0)\n",
    "    return new_idx, new_pos, new_neg\n",
    "\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "has_silver = np.array([len(lbls) > 0 for lbls in silver], dtype=bool)\n",
    "N_docs = X.shape[0]\n",
    "C = L.shape[0]\n",
    "\n",
    "# silver ìˆëŠ” ë¬¸ì„œ / ì—†ëŠ” ë¬¸ì„œ\n",
    "has_silver = np.array([len(lbls) > 0 for lbls in silver], dtype=bool)\n",
    "idx_silver = np.flatnonzero(has_silver)      # ì—¬ê¸°ê°€ train/val í›„ë³´\n",
    "idx_unl    = np.flatnonzero(~has_silver)     # ì§„ì§œ unl\n",
    "\n",
    "print(\"total:\", N_docs)\n",
    "print(\"with silver:\", len(idx_silver))\n",
    "print(\"unlabeled :\", len(idx_unl))\n",
    "\n",
    "# ì´ì œ train/valì€ silver ìˆëŠ” ì• ë“¤ë§Œ ì„ì–´ì„œ ë‚˜ëˆˆë‹¤\n",
    "rng = np.random.default_rng(42)\n",
    "rng.shuffle(idx_silver)\n",
    "n_val = int(len(idx_silver) * 0.2)\n",
    "idx_val   = idx_silver[:n_val]\n",
    "idx_train = idx_silver[n_val:]\n",
    "\n",
    "# parents, children ë§Œë“¤ê¸°\n",
    "def build_parents_children(adj):\n",
    "    C = adj.shape[0]\n",
    "    parents = [np.flatnonzero(adj[:, j]).astype(np.int64) for j in range(C)]\n",
    "    children = [np.flatnonzero(adj[j]).astype(np.int64) for j in range(C)]\n",
    "    return parents, children\n",
    "\n",
    "parents, children = build_parents_children(B)\n",
    "\n",
    "pos_masks = np.zeros((N_docs, C), dtype=np.float32)\n",
    "neg_masks = np.zeros((N_docs, C), dtype=np.float32)\n",
    "\n",
    "for i in idx_silver:  # silver ìˆëŠ” ì• ë§Œ ëˆë‹¤\n",
    "    core = silver[i]\n",
    "\n",
    "    # 1) core + parents\n",
    "    pos = set(core)\n",
    "    for c in core:\n",
    "        for p in parents[c]:\n",
    "            pos.add(int(p))\n",
    "\n",
    "    # 2) childrenì€ ëª¨ë¦„\n",
    "    child = set()\n",
    "    for c in core:\n",
    "        for ch in children[c]:\n",
    "            child.add(int(ch))\n",
    "\n",
    "    for p in pos:\n",
    "        pos_masks[i, p] = 1.0\n",
    "\n",
    "    for j in range(C):\n",
    "        if j in pos:      # ì´ë¯¸ ì–‘ì„±\n",
    "            continue\n",
    "        if j in child:    # ëª¨ë¦„\n",
    "            continue\n",
    "        neg_masks[i, j] = 1.0\n",
    "\n",
    "\n",
    "\n",
    "train_ds = HierMultiLabelDataset(X, pos_masks, neg_masks, indices=idx_train)\n",
    "val_ds   = HierMultiLabelDataset(X, pos_masks, neg_masks, indices=idx_val) if len(idx_val) > 0 else None\n",
    "unl_ds   = UnlabeledDataset(X, idx_unl.tolist())\n",
    "print(len(train_ds),len(val_ds),len(unl_ds))\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=256, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=256, shuffle=False)\n",
    "\n",
    "model = BilinearHierClassifier(doc_dim=X.shape[1], label_emb=L, hidden_dim=256).to(device)\n",
    "\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "epochs = 500\n",
    "\n",
    "N_labels = L.shape[0]\n",
    "best_f1 = -1.0\n",
    "patience = 20\n",
    "no_improve = 0\n",
    "warmup_self = 1   # 1 epochì€ self-training ì•ˆ í•˜ê²Œ í•´ì„œ í•œ ë²ˆ ì•ˆì •í™”\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    # train\n",
    "    tr_loss = train_epoch_hier(model, train_loader, opt, device)\n",
    "\n",
    "    # val: f1 ê¸°ì¤€\n",
    "    if val_loader is not None and len(val_ds) > 0:\n",
    "        va_loss, va_f1 = eval_epoch_hier(model, val_loader, device, k=3)\n",
    "        print(f\"Epoch {epoch:03d} | train_loss={tr_loss:.3f}  val_loss={va_loss:.3f}  val_f1={va_f1:.3f}\")\n",
    "\n",
    "        # early stoppingì„ f1ë¡œ\n",
    "        if va_f1 > best_f1 + 1e-6:\n",
    "            best_f1 = va_f1\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch} (best f1={best_f1:.4f})\")\n",
    "                break\n",
    "    else:\n",
    "        print(f\"Epoch {epoch:03d} | train_loss={tr_loss:.3f}\")\n",
    "\n",
    "    # self-training: 1ì—í­ì— ì „ë¶€ ë“¤ì–´ê°€ëŠ” ê±° ë°©ì§€ìš©ìœ¼ë¡œ warmup ë„£ìŒ\n",
    "    if epoch <= warmup_self:\n",
    "        print(\"  + (skip pseudo-labeling on warmup epoch)\")\n",
    "        continue\n",
    "\n",
    "    new_idx, new_pos, new_neg = pseudo_label_and_grow_hier_conf(\n",
    "        model,\n",
    "        unl_ds,\n",
    "        parents,\n",
    "        children,\n",
    "        C,\n",
    "        device=device,\n",
    "        pseudo_threshold=0.45,   # p(D,c) ê¸°ë³¸ threshold\n",
    "        pseudo_topk=3,\n",
    "        batch_size=512,\n",
    "        max_candidates=32,\n",
    "    )\n",
    "\n",
    "    if len(new_idx) > 0:\n",
    "        # ì „ì—­ ë§ˆìŠ¤í¬ ê°±ì‹ \n",
    "        pos_masks[new_idx] = new_pos\n",
    "        neg_masks[new_idx] = new_neg\n",
    "\n",
    "        # unlì—ì„œ ì œê±°\n",
    "        keep_mask = ~np.isin(unl_ds.indices, np.array(new_idx, dtype=np.int64))\n",
    "        unl_ds.indices = unl_ds.indices[keep_mask]\n",
    "\n",
    "        # trainì— ì¶”ê°€\n",
    "        train_ds.indices = np.concatenate([train_ds.indices, np.array(new_idx, dtype=np.int64)])\n",
    "        train_loader = DataLoader(train_ds, batch_size=256, shuffle=True, drop_last=False)\n",
    "\n",
    "        print(f\"  + Added {len(new_idx)} pseudo-labeled docs (unl pool â†’ {len(unl_ds)} left)\")\n",
    "    else:\n",
    "        print(\"  + No pseudo-labeled docs added this epoch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec157776-e172-4c16-93a1-aacdf0a962d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd03ade-045a-4c94-89e4-de3d1e523595",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b88df0-bac0-49b4-9f58-d0c5e77583e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574d819b-4455-4b10-8541-4a9a9f2ddec4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7411b961-6f29-4ac7-ba7a-97098db48b1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T09:08:41.721333Z",
     "iopub.status.busy": "2025-11-13T09:08:41.721204Z",
     "iopub.status.idle": "2025-11-13T09:08:43.187267Z",
     "shell.execute_reply": "2025-11-13T09:08:43.186793Z",
     "shell.execute_reply.started": "2025-11-13T09:08:41.721320Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv, os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ------------ Paths (edit if needed) ------------\n",
    "TEST_CORPUS = \"Amazon_products/test/test_corpus.txt\"   # lines: pid \\t text\n",
    "OUT_PATH    = \"submission_bda.csv\"\n",
    "# ------------ Hyperparams ------------\n",
    "MIN_LABS  = 2\n",
    "MAX_LABS  = 3\n",
    "BATCH = 1024\n",
    "doc_ids, doc_texts = load_docs_txt(TEST_CORPUS)\n",
    "\n",
    "# 2) ë¼ë²¨ ë•Œ ë§Œë“  vectorizer ì¬ì‚¬ìš©í•´ì„œ ì„ë² ë”© ë§Œë“¤ê¸°\n",
    "test_embeddings = build_doc_embeddings_from_existing_vectorizer(doc_texts, vectorizer)\n",
    "test_embeddings = test_embeddings.astype(np.float32)   # [num_test, d]\n",
    "# load test pids\n",
    "pids = doc_ids   # ì´ë¯¸ ë¬¸ìì—´ id\n",
    "if \"L\" in globals():\n",
    "    if not isinstance(L, np.ndarray):\n",
    "        # ì˜ˆ: Lì´ torch.Tensorì¸ ê²½ìš°\n",
    "        L = L.detach().cpu().numpy().astype(np.float32)\n",
    "else:\n",
    "    raise ValueError(\"ë¼ë²¨ ì„ë² ë”© Lì´ ë©”ëª¨ë¦¬ì— ì—†ì–´! GAT ëë‚œ ë’¤ì˜ ì„ë² ë”©ì„ Lë¡œ ë‘¬ì•¼ í•´.\")\n",
    "\n",
    "# 5) ë¼ë²¨ idëŠ” 0..N-1ë¡œ ìƒì„± (ë„¤ê°€ ë§í•œ ëŒ€ë¡œ adjacencyë‘ ìˆœì„œê°€ ì´ë¯¸ ë§ë‹¤ê³  í–ˆìœ¼ë‹ˆê¹Œ)\n",
    "lab_ids = np.arange(L.shape[0], dtype=np.int64)\n",
    "\n",
    "# 6) adjacencyë„ ë©”ëª¨ë¦¬ì— ìˆëŠ” ê±¸ ê·¸ëŒ€ë¡œ ì“´ë‹¤\n",
    "#    ì—¬ê¸°ì„œ AëŠ” 531x531 ê°™ì€ numpy arrayë¼ê³  ê°€ì •\n",
    "assert B.shape == (L.shape[0], L.shape[0]), \"Adjacency/label size mismatch\"\n",
    "\n",
    "# 7) children ë¦¬ìŠ¤íŠ¸ ë¯¸ë¦¬ ë§Œë“¤ì–´ë‘ê¸°\n",
    "children = [np.flatnonzero(B[i]) for i in range(B.shape[0])]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d1d6c266-ff52-4c84-a586-847f0e586a7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T09:13:24.586482Z",
     "iopub.status.busy": "2025-11-13T09:13:24.586235Z",
     "iopub.status.idle": "2025-11-13T09:13:24.591118Z",
     "shell.execute_reply": "2025-11-13T09:13:24.590634Z",
     "shell.execute_reply.started": "2025-11-13T09:13:24.586463Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ef344a9-c996-4932-b7ab-9f7c5e7578b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T09:08:43.188032Z",
     "iopub.status.busy": "2025-11-13T09:08:43.187869Z",
     "iopub.status.idle": "2025-11-13T09:08:58.374419Z",
     "shell.execute_reply": "2025-11-13T09:08:58.373967Z",
     "shell.execute_reply.started": "2025-11-13T09:08:43.188018Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: submission_bda.csv | samples=19658 | min-max labels per sample=2-3 | missing_pids=0\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "def ancestors_of(node, adj):\n",
    "    # adj[parent, child] = 1 ê°€ì •\n",
    "    parents = np.flatnonzero(adj[:, node])  # (N,)\n",
    "    return parents.tolist()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "IN_DIM = test_embeddings.shape[1]\n",
    "missing = 0  # ì§€ê¸ˆì€ ì“¸ ì¼ ì—†ì§€ë§Œ ì›ë˜ ì½”ë“œë‘ í˜•íƒœ ë§ì¶°ë‘ \n",
    "\n",
    "with open(OUT_PATH, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow([\"id\", \"label\"])\n",
    "\n",
    "    buf_x, buf_pid = [], []\n",
    "\n",
    "    def flush():\n",
    "        if not buf_x:\n",
    "            return\n",
    "        xb = torch.from_numpy(np.stack(buf_x, axis=0).astype(np.float32)).to(device)\n",
    "        with torch.inference_mode():\n",
    "            prob = torch.sigmoid(model(xb)).detach().cpu().numpy()\n",
    "        prob = np.nan_to_num(prob, nan=-1.0, posinf=1.0, neginf=0.0)\n",
    "\n",
    "        for pid, p in zip(buf_pid, prob):\n",
    "            order = np.argsort(-p)\n",
    "\n",
    "            # 1) ê¸°ë³¸ í›„ë³´ ë½‘ê¸°\n",
    "            thr_keep = [i for i in order if p[i] >= 0.5][:MAX_LABS]\n",
    "            if len(thr_keep) >= MIN_LABS:\n",
    "                keep = thr_keep[:MAX_LABS]\n",
    "            else:\n",
    "                keep = order[:max(MIN_LABS, len(thr_keep))]\n",
    "                if len(keep) < MIN_LABS:\n",
    "                    keep = order[:MIN_LABS]\n",
    "\n",
    "            # 2) ë¶€ëª¨ í›„ë³´\n",
    "            parent_cands = []\n",
    "            for c in keep:\n",
    "                pars = ancestors_of(c, B)\n",
    "                for pa in pars:\n",
    "                    if pa not in keep and pa not in parent_cands:\n",
    "                        parent_cands.append(pa)\n",
    "\n",
    "            parent_cands.sort(key=lambda idx: p[idx], reverse=True)\n",
    "\n",
    "            # 3) ë‚¨ëŠ” ìŠ¬ë¡¯ ë¶€ëª¨ë¡œ ì±„ìš°ê¸°\n",
    "            final_idxs = list(keep)\n",
    "            for pa in parent_cands:\n",
    "                if len(final_idxs) >= MAX_LABS:\n",
    "                    break\n",
    "                final_idxs.append(pa)\n",
    "\n",
    "            # 4) ê·¸ë˜ë„ ëª¨ìë¼ë©´ í™•ë¥ ìˆœ\n",
    "            if len(final_idxs) < MIN_LABS:\n",
    "                for idx in order:\n",
    "                    if idx not in final_idxs:\n",
    "                        final_idxs.append(idx)\n",
    "                    if len(final_idxs) >= MIN_LABS:\n",
    "                        break\n",
    "\n",
    "            labels = sorted(int(lab_ids[i]) for i in final_idxs)\n",
    "            w.writerow([pid, \",\".join(map(str, labels))])\n",
    "\n",
    "        buf_x.clear()\n",
    "        buf_pid.clear()\n",
    "\n",
    "    # ì—¬ê¸°ì„œ ë°”ë¡œ pidsì™€ test_embeddingsë¥¼ ê°™ì´ ìˆœíšŒ\n",
    "    for pid, emb in zip(pids, test_embeddings):\n",
    "        x = emb\n",
    "        if x.dtype != np.float32:\n",
    "            x = x.astype(np.float32, copy=False)\n",
    "        buf_x.append(x)\n",
    "        buf_pid.append(pid)\n",
    "        if len(buf_x) >= BATCH:\n",
    "            flush()\n",
    "    flush()\n",
    "\n",
    "print(f\"Saved: {OUT_PATH} | samples={len(pids)} | min-max labels per sample={MIN_LABS}-{MAX_LABS} | missing_pids={missing}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e293bbf-8597-4c2c-91e1-b03ce7477217",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b72c5b-1d7a-4d59-a975-e7fa22dfbb32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a677853-591b-488e-835f-cf4b4c164c1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343dfe98-d91d-4c90-9325-ec5a6ff5c560",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49cebb9-e667-46aa-8c96-64a63c801ab3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d535af4-4d65-4127-b625-caac88456b68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T09:13:40.581845Z",
     "iopub.status.busy": "2025-11-13T09:13:40.581637Z",
     "iopub.status.idle": "2025-11-13T09:13:40.586255Z",
     "shell.execute_reply": "2025-11-13T09:13:40.585834Z",
     "shell.execute_reply.started": "2025-11-13T09:13:40.581830Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_parents_children(adj: np.ndarray):\n",
    "    C = adj.shape[0]\n",
    "    parents = [np.flatnonzero(adj[:, j]).astype(np.int64) for j in range(C)]\n",
    "    children = [np.flatnonzero(adj[j]).astype(np.int64) for j in range(C)]\n",
    "    return parents, children\n",
    "\n",
    "def build_siblings(parents, children):\n",
    "    C = len(parents)\n",
    "    sibs = [set() for _ in range(C)]\n",
    "    for c in range(C):\n",
    "        for p in parents[c]:\n",
    "            for ch in children[p]:\n",
    "                if ch != c:\n",
    "                    sibs[c].add(int(ch))\n",
    "    sibs = [np.array(sorted(s), dtype=np.int64) for s in sibs]\n",
    "    return sibs\n",
    "\n",
    "def compute_conf_all(p: np.ndarray, parents, siblings) -> np.ndarray:\n",
    "    C = p.shape[0]\n",
    "    conf = np.zeros(C, dtype=np.float32)\n",
    "    for c in range(C):\n",
    "        neigh = []\n",
    "        if len(parents[c]) > 0:\n",
    "            neigh.append(p[parents[c]].max())\n",
    "        if len(siblings[c]) > 0:\n",
    "            neigh.append(p[siblings[c]].max())\n",
    "        neigh_max = max(neigh) if neigh else 0.0\n",
    "        conf[c] = float(p[c] - neigh_max)\n",
    "    return conf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "42b2e45b-844d-437a-9fc5-1d47ad4a9832",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T09:14:05.852435Z",
     "iopub.status.busy": "2025-11-13T09:14:05.851975Z",
     "iopub.status.idle": "2025-11-13T09:14:05.857110Z",
     "shell.execute_reply": "2025-11-13T09:14:05.856664Z",
     "shell.execute_reply.started": "2025-11-13T09:14:05.852403Z"
    }
   },
   "outputs": [],
   "source": [
    "def select_labels_hierarchical(\n",
    "    p: np.ndarray,          # [C] í™•ë¥ \n",
    "    B: np.ndarray,          # [C, C] parent -> child\n",
    "    MIN_LABS: int,\n",
    "    MAX_LABS: int,\n",
    "    p_threshold: float = 0.5,\n",
    "    conf_margin: float = 0.0,   # conf >= ì´ ê°’ì¼ ë•Œë§Œ core\n",
    "):\n",
    "    parents, children = build_parents_children(B)\n",
    "    siblings = build_siblings(parents, children)\n",
    "\n",
    "    C = p.shape[0]\n",
    "    conf = compute_conf_all(p, parents, siblings)\n",
    "\n",
    "    order = np.argsort(-p)  # í™•ë¥  ë‚´ë¦¼ì°¨ìˆœ ì¸ë±ìŠ¤\n",
    "\n",
    "    core = []\n",
    "    for c in order:\n",
    "        # í™•ë¥ ì´ ë„ˆë¬´ ë‚®ê³  ì´ë¯¸ MIN_LABS ì´ìƒì´ë©´ ì¤‘ë‹¨\n",
    "        if p[c] < p_threshold and len(core) >= MIN_LABS:\n",
    "            break\n",
    "\n",
    "        # ê¸°ë³¸ í™•ë¥  ì¡°ê±´\n",
    "        if p[c] < p_threshold:\n",
    "            continue\n",
    "\n",
    "        # confidence margin ì¡°ê±´\n",
    "        if conf[c] < conf_margin:\n",
    "            continue\n",
    "\n",
    "        core.append(int(c))\n",
    "        if len(core) >= MAX_LABS:\n",
    "            break\n",
    "\n",
    "    # ì•„ë¬´ê²ƒë„ ëª» ë½‘ì•˜ìœ¼ë©´ fallback: flat top-N\n",
    "    if len(core) == 0:\n",
    "        core = [int(i) for i in order[:MIN_LABS]]\n",
    "\n",
    "    # coreì— ëŒ€í•´ ì¡°ìƒ(ë¶€ëª¨) í™•ì¥\n",
    "    final = set(core)\n",
    "    for c in core:\n",
    "        for pa in ancestors_of(c, B):\n",
    "            final.add(int(pa))\n",
    "            if len(final) >= MAX_LABS:\n",
    "                break\n",
    "\n",
    "    # ê·¸ë˜ë„ MIN_LABSë³´ë‹¤ ì ìœ¼ë©´ flat í™•ë¥  ë†’ì€ ìˆœìœ¼ë¡œ ì±„ìš°ê¸°\n",
    "    if len(final) < MIN_LABS:\n",
    "        for idx in order:\n",
    "            if idx not in final:\n",
    "                final.add(int(idx))\n",
    "            if len(final) >= MIN_LABS:\n",
    "                break\n",
    "\n",
    "    # ìµœì¢… ê²°ê³¼ë¥¼ í™•ë¥  ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬í•´ì„œ MAX_LABSë§Œ ìœ ì§€\n",
    "    final_list = list(final)\n",
    "    final_list.sort(key=lambda idx: -p[idx])\n",
    "    return final_list[:MAX_LABS]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9a133dad-b306-4349-b9d3-073022f3eb43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T09:15:35.752153Z",
     "iopub.status.busy": "2025-11-13T09:15:35.751904Z",
     "iopub.status.idle": "2025-11-13T09:18:15.985127Z",
     "shell.execute_reply": "2025-11-13T09:18:15.984644Z",
     "shell.execute_reply.started": "2025-11-13T09:15:35.752135Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: submission_bda.csv | samples=19658 | min-max labels per sample=2-3 | missing_pids=0\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "def ancestors_of(node, adj):\n",
    "    # adj[parent, child] = 1 ê°€ì •\n",
    "    parents = np.flatnonzero(adj[:, node])  # (N,)\n",
    "    return parents.tolist()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "IN_DIM = test_embeddings.shape[1]\n",
    "missing = 0  # ì§€ê¸ˆì€ ì“¸ ì¼ ì—†ì§€ë§Œ ì›ë˜ ì½”ë“œë‘ í˜•íƒœ ë§ì¶°ë‘ \n",
    "\n",
    "with open(OUT_PATH, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow([\"id\", \"label\"])\n",
    "\n",
    "    buf_x, buf_pid = [], []\n",
    "\n",
    "    def flush():\n",
    "        if not buf_x:\n",
    "            return\n",
    "        xb = torch.from_numpy(np.stack(buf_x, axis=0).astype(np.float32)).to(device)\n",
    "        with torch.inference_mode():\n",
    "            prob = torch.sigmoid(model(xb)).detach().cpu().numpy()\n",
    "        prob = np.nan_to_num(prob, nan=-1.0, posinf=1.0, neginf=0.0)\n",
    "\n",
    "\n",
    "        for pid, p in zip(buf_pid, prob):\n",
    "            final_idxs = select_labels_hierarchical(\n",
    "                p,\n",
    "                B,\n",
    "                MIN_LABS=MIN_LABS,\n",
    "                MAX_LABS=MAX_LABS,\n",
    "                p_threshold=0.5,     # ê¸°ì¡´ thresholdì™€ ë§ì¶°ë„ ë˜ê³  ì¡°ê¸ˆ ë‚®ì¶°ë„ ë¨\n",
    "                conf_margin=0.0,     # ì²˜ìŒì—” 0.0ë¡œ ì‹œì‘, ë‚˜ì¤‘ì— 0.05 ë“± íŠœë‹\n",
    "            )\n",
    "        \n",
    "            labels = sorted(int(lab_ids[i]) for i in final_idxs)\n",
    "            w.writerow([pid, \",\".join(map(str, labels))])\n",
    "        buf_x.clear()\n",
    "        buf_pid.clear()\n",
    "\n",
    "    # ì—¬ê¸°ì„œ ë°”ë¡œ pidsì™€ test_embeddingsë¥¼ ê°™ì´ ìˆœíšŒ\n",
    "    for pid, emb in zip(pids, test_embeddings):\n",
    "        x = emb\n",
    "        if x.dtype != np.float32:\n",
    "            x = x.astype(np.float32, copy=False)\n",
    "        buf_x.append(x)\n",
    "        buf_pid.append(pid)\n",
    "        if len(buf_x) >= BATCH:\n",
    "            flush()\n",
    "    flush()\n",
    "\n",
    "print(f\"Saved: {OUT_PATH} | samples={len(pids)} | min-max labels per sample={MIN_LABS}-{MAX_LABS} | missing_pids={missing}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba53595b-f997-468a-a4f9-6deee2dc1a08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb8152f-64ca-44fc-a37b-4ab46e4291d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5d29a7-c5b4-4a26-954b-b65a737f632c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
