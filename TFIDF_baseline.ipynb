{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26217918-5c27-43a7-a504-84e4cd5bba75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T07:49:37.916100Z",
     "iopub.status.busy": "2025-11-13T07:49:37.915910Z",
     "iopub.status.idle": "2025-11-13T07:49:39.190816Z",
     "shell.execute_reply": "2025-11-13T07:49:39.190143Z",
     "shell.execute_reply.started": "2025-11-13T07:49:37.916080Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25eba133-3361-42f0-8305-6df95935d14f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T07:49:39.191701Z",
     "iopub.status.busy": "2025-11-13T07:49:39.191425Z",
     "iopub.status.idle": "2025-11-13T07:49:41.592002Z",
     "shell.execute_reply": "2025-11-13T07:49:41.591552Z",
     "shell.execute_reply.started": "2025-11-13T07:49:39.191684Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chocolate_bars 0.5059\n",
      "chocolate_gifts 0.4276\n",
      "chocolate 0.3754\n",
      "chocolate_covered_fruit 0.3548\n",
      "dried_fruit_raisins 0.2765\n",
      "chocolate_pretzels 0.2373\n",
      "fresh_baked_cookies 0.2332\n",
      "grocery_gourmet_food 0.2266\n",
      "snack_gifts 0.2254\n",
      "chocolate_assortments 0.2173\n",
      "candy_chocolate 0.1635\n",
      "hot_cocoa 0.1609\n",
      "food 0.1457\n",
      "gourmet_gifts 0.1342\n",
      "snack_food 0.121\n",
      "trail_mix 0.1181\n",
      "granola_trail_mix_bars 0.1044\n",
      "fruit_leather 0.1023\n",
      "toaster_pastries 0.1015\n",
      "cookies 0.0996\n",
      "fruit 0.0941\n",
      "raisins 0.0925\n",
      "meat_poultry 0.0903\n",
      "marshmallows 0.087\n",
      "changing_table_pads_covers 0.0863\n",
      "popcorn 0.085\n",
      "granola_bars 0.0846\n",
      "produce 0.0833\n",
      "solid_feeding 0.0831\n",
      "milk 0.0827\n",
      "chocolate_truffles 0.0822\n",
      "rice_cakes 0.0777\n",
      "nutrition_wellness 0.0746\n",
      "party_mix 0.0734\n",
      "p_t_s 0.0716\n",
      "fruit_gifts 0.0684\n",
      "sensual_delights 0.0649\n",
      "foie_gras_p_t_s 0.062\n",
      "sugars_sweeteners 0.0604\n",
      "salsas 0.0594\n",
      "eggs 0.059\n",
      "cakes 0.0569\n",
      "nutrition_bars_drinks 0.0569\n",
      "chocolate_covered_nuts 0.0565\n",
      "dessert_gifts 0.0551\n",
      "spices_gifts 0.0545\n",
      "meat_gifts 0.0541\n",
      "crackers 0.0533\n",
      "juices 0.0517\n",
      "puffed_snacks 0.0512\n",
      "bars 0.0506\n",
      "cloth_diapers 0.0498\n",
      "sausages 0.0495\n",
      "baby_food 0.0489\n",
      "assortments 0.0484\n",
      "jams 0.0457\n",
      "pudding 0.0455\n",
      "jams_preserves_gifts 0.0446\n",
      "fruits 0.0429\n",
      "pretzels 0.0393\n",
      "sauces_gifts 0.0365\n",
      "seafood_gifts 0.0356\n",
      "nut_clusters 0.0322\n",
      "jerky_dried_meats 0.0322\n",
      "suckers_lollipops 0.0318\n",
      "treats 0.0312\n",
      "cheese_gifts 0.0306\n",
      "candy_gifts 0.0302\n",
      "baby_formula 0.0299\n",
      "gummy_candy 0.0229\n",
      "jerky 0.0\n",
      "toys_games 0.0\n",
      "games 0.0\n",
      "puzzles 0.0\n",
      "jigsaw_puzzles 0.0\n",
      "board_games 0.0\n",
      "beverages 0.0\n",
      "beauty 0.0\n",
      "makeup 0.0\n",
      "nails 0.0\n",
      "arts_crafts 0.0\n",
      "drawing_painting_supplies 0.0\n",
      "action_toy_figures 0.0\n",
      "figures 0.0\n",
      "dolls_accessories 0.0\n",
      "dolls 0.0\n",
      "card_games 0.0\n",
      "drawing_sketching_tablets 0.0\n",
      "baby_toddler_toys 0.0\n",
      "shape_sorters 0.0\n",
      "health_personal_care 0.0\n",
      "personal_care 0.0\n",
      "deodorants_antiperspirants 0.0\n",
      "learning_education 0.0\n",
      "habitats 0.0\n",
      "electronics_for_kids 0.0\n",
      "household_supplies 0.0\n",
      "household_batteries 0.0\n",
      "push_pull_toys 0.0\n",
      "stuffed_animals_plush 0.0\n",
      "tricycles 0.0\n",
      "scooters_wagons 0.0\n",
      "clay_dough 0.0\n",
      "health_care 0.0\n",
      "allergy 0.0\n",
      "baby_products 0.0\n",
      "gear 0.0\n",
      "baby_gyms_playmats 0.0\n",
      "shaving_hair_removal 0.0\n",
      "skin_care 0.0\n",
      "face 0.0\n",
      "animals_figures 0.0\n",
      "feminine_care 0.0\n",
      "music_sound 0.0\n",
      "oral_hygiene 0.0\n",
      "grown_up_toys 0.0\n",
      "dress_up_pretend_play 0.0\n",
      "pretend_play 0.0\n",
      "novelty_gag_toys 0.0\n",
      "bath_body 0.0\n",
      "cleansers 0.0\n",
      "playsets 0.0\n",
      "d_puzzles 0.0\n",
      "dollhouses 0.0\n",
      "lip_care_products 0.0\n",
      "tools_accessories 0.0\n",
      "nail_tools 0.0\n",
      "eye_care 0.0\n",
      "pill_cases_splitters 0.0\n",
      "hair_care 0.0\n",
      "styling_products 0.0\n",
      "electronic_toys 0.0\n",
      "body 0.0\n",
      "toy_balls 0.0\n",
      "eyes 0.0\n",
      "trading_card_games 0.0\n",
      "foot_care 0.0\n",
      "hands_nails 0.0\n",
      "sun 0.0\n",
      "medical_supplies_equipment 0.0\n",
      "daily_living_aids 0.0\n",
      "baby_child_care 0.0\n",
      "paper_plastic 0.0\n",
      "incontinence 0.0\n",
      "shampoos 0.0\n",
      "conditioners 0.0\n",
      "music_players_karaoke 0.0\n",
      "cough_cold 0.0\n",
      "bath 0.0\n",
      "tests 0.0\n",
      "building_toys 0.0\n",
      "building_sets 0.0\n",
      "stress_reduction 0.0\n",
      "family_planning_contraceptives 0.0\n",
      "vitamins_supplements 0.0\n",
      "hair_color 0.0\n",
      "pain_relievers 0.0\n",
      "cotton_swabs 0.0\n",
      "styling_tools 0.0\n",
      "first_aid 0.0\n",
      "scrubs_body_treatments 0.0\n",
      "cleaning_tools 0.0\n",
      "pegged_puzzles 0.0\n",
      "diabetes 0.0\n",
      "magic_kits_accessories 0.0\n",
      "gifts 0.0\n",
      "albums 0.0\n",
      "crib_toys_attachments 0.0\n",
      "digestion_nausea 0.0\n",
      "electronic_pets 0.0\n",
      "sexual_wellness 0.0\n",
      "safer_sex 0.0\n",
      "thermometers 0.0\n",
      "stacking_nesting_toys 0.0\n",
      "makeup_remover 0.0\n",
      "temporary_tattoos 0.0\n",
      "sports_outdoor_play 0.0\n",
      "play_tents_tunnels 0.0\n",
      "science 0.0\n",
      "sports 0.0\n",
      "bath_toys 0.0\n",
      "puppets 0.0\n",
      "systems_accessories 0.0\n",
      "health_monitors 0.0\n",
      "inflatable_bouncers 0.0\n",
      "hobbies 0.0\n",
      "model_building_kits_tools 0.0\n",
      "blackboards_whiteboards 0.0\n",
      "pools_water_fun 0.0\n",
      "rattles 0.0\n",
      "sandboxes_accessories 0.0\n",
      "activity_play_centers 0.0\n",
      "car_seat_stroller_toys 0.0\n",
      "feeding 0.0\n",
      "bottle_feeding 0.0\n",
      "breastfeeding 0.0\n",
      "diapering 0.0\n",
      "diaper_changing_kits 0.0\n",
      "puzzle_accessories 0.0\n",
      "diaper_pails_refills 0.0\n",
      "safety 0.0\n",
      "bathroom_safety 0.0\n",
      "massage_relaxation 0.0\n",
      "gates_doorways 0.0\n",
      "nursery 0.0\n",
      "furniture 0.0\n",
      "monitors 0.0\n",
      "plush_backpacks_purses 0.0\n",
      "statues 0.0\n",
      "bathing_skin_care 0.0\n",
      "bathing_tubs_seats 0.0\n",
      "vehicles_remote_control 0.0\n",
      "play_vehicles 0.0\n",
      "backpacks_carriers 0.0\n",
      "craft_kits 0.0\n",
      "car_seats_accessories 0.0\n",
      "car_seats 0.0\n",
      "nursery_d_cor 0.0\n",
      "hammering_pounding_toys 0.0\n",
      "bedding 0.0\n",
      "play_trains_railway_sets 0.0\n",
      "rockets 0.0\n",
      "stacking_blocks 0.0\n",
      "diaper_bags 0.0\n",
      "strollers 0.0\n",
      "gym_sets_swings 0.0\n",
      "pregnancy_maternity 0.0\n",
      "maternity_pillows 0.0\n",
      "rocking_spring_ride_ons 0.0\n",
      "braces 0.0\n",
      "accessories 0.0\n",
      "vehicle_playsets 0.0\n",
      "doll_accessories 0.0\n",
      "pet_supplies 0.0\n",
      "cats 0.0\n",
      "litter_housebreaking 0.0\n",
      "spinning_tops 0.0\n",
      "sets 0.0\n",
      "travel_games 0.0\n",
      "pillows_stools 0.0\n",
      "battling_tops 0.0\n",
      "cameras_camcorders 0.0\n",
      "dance_mats 0.0\n",
      "radio_control 0.0\n",
      "grooming_healthcare_kits 0.0\n",
      "balls 0.0\n",
      "tile_games 0.0\n",
      "potty_training 0.0\n",
      "potties_seats 0.0\n",
      "highchairs_booster_seats 0.0\n",
      "stuffed_animals_toys 0.0\n",
      "dvd_games 0.0\n",
      "edge_corner_guards 0.0\n",
      "basic_life_skills_toys 0.0\n",
      "activity_centers_entertainers 0.0\n",
      "thermometer_accessories 0.0\n",
      "wipes_holders 0.0\n",
      "gift_sets 0.0\n",
      "joggers 0.0\n",
      "facial_steamers 0.0\n",
      "kites_wind_spinners 0.0\n",
      "dogs 0.0\n",
      "toys 0.0\n",
      "walkers 0.0\n",
      "slumber_bags 0.0\n",
      "die_cast_vehicles 0.0\n",
      "easels 0.0\n",
      "lips 0.0\n",
      "tea 0.0\n",
      "reading_writing 0.0\n",
      "stacking_games 0.0\n",
      "sauces_dips 0.0\n",
      "sauces 0.0\n",
      "breakfast_foods 0.0\n",
      "cereals 0.0\n",
      "shopping_cart_covers 0.0\n",
      "pantry_staples 0.0\n",
      "scaled_model_vehicles 0.0\n",
      "cooking_baking_supplies 0.0\n",
      "personal_video_players_accessories 0.0\n",
      "fragrance 0.0\n",
      "women_s 0.0\n",
      "keepsakes 0.0\n",
      "swings 0.0\n",
      "trains_accessories 0.0\n",
      "disposable_diapers 0.0\n",
      "plug_play_video_games 0.0\n",
      "floor_puzzles 0.0\n",
      "fresh_flowers_live_indoor_plants 0.0\n",
      "live_indoor_plants 0.0\n",
      "weight_loss_products 0.0\n",
      "smoking_cessation 0.0\n",
      "beauty_fashion 0.0\n",
      "mirrors 0.0\n",
      "coffee 0.0\n",
      "cabinet_locks_straps 0.0\n",
      "plush_pillows 0.0\n",
      "floor_games 0.0\n",
      "makeup_brushes_tools 0.0\n",
      "alternative_medicine 0.0\n",
      "men_s 0.0\n",
      "step_stools 0.0\n",
      "rails_rail_guards 0.0\n",
      "laundry 0.0\n",
      "women_s_health 0.0\n",
      "standard 0.0\n",
      "beds_furniture 0.0\n",
      "herbs 0.0\n",
      "sleep_positioners 0.0\n",
      "health_supplies 0.0\n",
      "breakfast_cereal_bars 0.0\n",
      "body_art 0.0\n",
      "condiments 0.0\n",
      "breads_bakery 0.0\n",
      "dried_beans 0.0\n",
      "household_cleaning 0.0\n",
      "collars 0.0\n",
      "educational_repellents 0.0\n",
      "adult_toys_games 0.0\n",
      "teddy_bears 0.0\n",
      "therapeutic_skin_care 0.0\n",
      "sand_water_tables 0.0\n",
      "slot_cars 0.0\n",
      "soft_drinks 0.0\n",
      "chips_crisps 0.0\n",
      "licorice 0.0\n",
      "feeding_watering_supplies 0.0\n",
      "blasters_foam_play 0.0\n",
      "meat_seafood 0.0\n",
      "wild_game_fowl 0.0\n",
      "spices_seasonings 0.0\n",
      "training_behavior_aids 0.0\n",
      "gardening_tools 0.0\n",
      "bathroom_aids_safety 0.0\n",
      "pogo_sticks_hoppers 0.0\n",
      "powdered_drink_mixes 0.0\n",
      "playards 0.0\n",
      "gag_toys_practical_jokes 0.0\n",
      "lighters 0.0\n",
      "money_banks 0.0\n",
      "marble_runs 0.0\n",
      "game_collections 0.0\n",
      "kitchen_safety 0.0\n",
      "fish_aquatic_pets 0.0\n",
      "gum 0.0\n",
      "outdoor_safety 0.0\n",
      "hair_nails 0.0\n",
      "aquarium_lights 0.0\n",
      "blocks 0.0\n",
      "tandem 0.0\n",
      "occupational_physical_therapy_aids 0.0\n",
      "packaged_meals_side_dishes 0.0\n",
      "indoor_climbers_play_structures 0.0\n",
      "pumps_filters 0.0\n",
      "beds_accessories 0.0\n",
      "energy_drinks 0.0\n",
      "sleep_snoring 0.0\n",
      "geography 0.0\n",
      "small_animals 0.0\n",
      "houses_habitats 0.0\n",
      "dairy_eggs 0.0\n",
      "cheese 0.0\n",
      "travel_systems 0.0\n",
      "walkie_talkies 0.0\n",
      "mobility_aids_equipment 0.0\n",
      "sexual_enhancers 0.0\n",
      "dips 0.0\n",
      "dollhouse_accessories 0.0\n",
      "bathing_accessories 0.0\n",
      "grooming 0.0\n",
      "baby_seats 0.0\n",
      "wind_up_toys 0.0\n",
      "dishwashing 0.0\n",
      "carriers_strollers 0.0\n",
      "flash_cards 0.0\n",
      "brain_teasers 0.0\n",
      "nesting_dolls 0.0\n",
      "test_kits 0.0\n",
      "lightweight 0.0\n",
      "hair_loss_products 0.0\n",
      "water_treatments 0.0\n",
      "birds 0.0\n",
      "hair_scalp_treatments 0.0\n",
      "cages_accessories 0.0\n",
      "gummy_candies 0.0\n",
      "houses 0.0\n",
      "ear_care 0.0\n",
      "pizza_crusts 0.0\n",
      "hard_candies 0.0\n",
      "sports_supplements 0.0\n",
      "baking_mixes 0.0\n",
      "pork_rinds 0.0\n",
      "pasta_noodles 0.0\n",
      "carriers_travel_products 0.0\n",
      "fresh_fruits 0.0\n",
      "chips 0.0\n",
      "mathematics_counting 0.0\n",
      "toy_banks 0.0\n",
      "training_pants 0.0\n",
      "tea_gifts 0.0\n",
      "oils 0.0\n",
      "aquarium_hoods 0.0\n",
      "tortillas 0.0\n",
      "doors 0.0\n",
      "standard_playing_card_decks 0.0\n",
      "fudge 0.0\n",
      "syrups 0.0\n",
      "printing_stamping 0.0\n",
      "toy_gift_sets 0.0\n",
      "canned_jarred_food 0.0\n",
      "fresh_vegetables 0.0\n",
      "apparel_accessories 0.0\n",
      "chewing_gum 0.0\n",
      "puzzle_play_mats 0.0\n",
      "electrical_safety 0.0\n",
      "marble_games 0.0\n",
      "miniatures 0.0\n",
      "finger_boards_finger_bikes 0.0\n",
      "coconut_water 0.0\n",
      "handheld_games 0.0\n",
      "slime_putty_toys 0.0\n",
      "pastries 0.0\n",
      "health_baby_care 0.0\n",
      "teethers 0.0\n",
      "butter 0.0\n",
      "breakfast_bakery 0.0\n",
      "stickers 0.0\n",
      "soaps_cleansers 0.0\n",
      "fitness_equipment 0.0\n",
      "water 0.0\n",
      "portable_changing_pads 0.0\n",
      "dice_gaming_dice 0.0\n",
      "pacifiers_accessories 0.0\n",
      "cocktail_mixers 0.0\n",
      "aquariums 0.0\n",
      "ball_pits_accessories 0.0\n",
      "seafood 0.0\n",
      "bags_cases 0.0\n",
      "jelly_beans 0.0\n",
      "novelty_spinning_tops 0.0\n",
      "automatic_feeders 0.0\n",
      "mints 0.0\n",
      "makeup_sets 0.0\n",
      "cleaners 0.0\n",
      "fresh_cut_flowers 0.0\n",
      "prams 0.0\n",
      "nuts_seeds 0.0\n",
      "taffy 0.0\n",
      "bunny_rabbit_central 0.0\n",
      "rabbit_hutches 0.0\n",
      "aquarium_d_cor 0.0\n",
      "viewfinders 0.0\n",
      "harnesses_leashes 0.0\n",
      "game_accessories 0.0\n",
      "game_room_games 0.0\n",
      "cages 0.0\n",
      "non_slip_bath_mats 0.0\n",
      "halva 0.0\n",
      "stimulants 0.0\n",
      "beanbags_foot_bags 0.0\n",
      "shampoo_conditioner_sets 0.0\n",
      "breadcrumbs 0.0\n",
      "extracts_flavoring 0.0\n",
      "plush_puppets 0.0\n",
      "shampoo_plus_conditioner 0.0\n",
      "memorials 0.0\n",
      "die_cast_toy_vehicles 0.0\n",
      "aquarium_starter_kits 0.0\n",
      "coffee_gifts 0.0\n",
      "air_fresheners 0.0\n",
      "sugar_substitutes 0.0\n",
      "bacon 0.0\n",
      "cat_flaps 0.0\n",
      "aquarium_heaters 0.0\n",
      "hair_relaxers 0.0\n",
      "breads 0.0\n",
      "packaged_breads 0.0\n",
      "dessert_toppings 0.0\n",
      "diaper_stackers_caddies 0.0\n",
      "prisms_kaleidoscopes 0.0\n",
      "maternity 0.0\n",
      "crackers_biscuits 0.0\n",
      "coin_collecting 0.0\n",
      "kickball_playground_balls 0.0\n",
      "hair_perms_texturizers 0.0\n",
      "yo_yos 0.0\n",
      "flours_meals 0.0\n",
      "beef 0.0\n",
      "molding_sculpting_sticks 0.0\n",
      "washcloths_towels 0.0\n",
      "stuffing 0.0\n",
      "baking_powder 0.0\n",
      "cereal 0.0\n",
      "exotic_meats 0.0\n",
      "breadsticks 0.0\n",
      "cloth_diaper_accessories 0.0\n",
      "carriers 0.0\n",
      "toffee 0.0\n",
      "hair_coloring_tools 0.0\n",
      "caramels 0.0\n",
      "aromatherapy 0.0\n",
      "seat_covers 0.0\n",
      "bondage_gear_accessories 0.0\n",
      "sun_protection 0.0\n",
      "dinners 0.0\n",
      "aquarium_stands 0.0\n",
      "teaching_clocks 0.0\n",
      "milk_substitutes 0.0\n",
      "bubble_bath 0.0\n",
      "novelties 0.0\n",
      "beads 0.0\n",
      "fish_bowls 0.0\n",
      "odor_stain_removers 0.0\n",
      "food_coloring 0.0\n",
      "children_s 0.0\n",
      "ice_cream_frozen_desserts 0.0\n",
      "pastry_decorations 0.0\n",
      "chicken 0.0\n",
      "sports_drinks 0.0\n",
      "aprons_smocks 0.0\n",
      "electronics 0.0\n",
      "sex_furniture 0.0\n",
      "pork 0.0\n",
      "dried_fruit 0.0\n",
      "flying_toys 0.0\n",
      "shampoo 0.0\n",
      "coatings_batters 0.0\n",
      "hydrometers 0.0\n",
      "lamb 0.0\n",
      "exercise_wheels 0.0\n",
      "breeding_tanks 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, ConcatDataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "def load_label_file(path: str) -> str:\n",
    "    \"\"\"key: value1,value2,... 형식으로 된 .txt 파일을 통째로 읽어서 문자열로 반환\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def parse_key_value_lines(text: str):\n",
    "    \"\"\"'key:val1,val2,...' 여러 줄을 딕셔너리로 변환\"\"\"\n",
    "    id2label = {}\n",
    "    for line in text.splitlines():\n",
    "        line = line.strip()\n",
    "        if not line or \":\" not in line:\n",
    "            continue\n",
    "        key, vals = line.split(\":\", 1)\n",
    "        id2label[key.strip()] = vals.strip()\n",
    "    return id2label\n",
    "\n",
    "def preprocess_label_text(label_path_str: str):\n",
    "    cleaned = label_path_str.lower()\n",
    "    cleaned = re.sub(r\"[:,]\", \" \", cleaned)\n",
    "    cleaned = re.sub(r\"_\", \" \", cleaned)\n",
    "    cleaned = re.sub(r\"[^a-z0-9 ]\", \" \", cleaned)\n",
    "    cleaned = re.sub(r\"\\s+\", \" \", cleaned).strip()\n",
    "    return cleaned\n",
    "\n",
    "def build_tfidf_vectorizer(label_texts):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    label_tfidf = vectorizer.fit_transform(label_texts)\n",
    "    return vectorizer, label_tfidf\n",
    "\n",
    "def compute_lexical_similarity(doc_text, vectorizer, label_tfidf):\n",
    "    doc_vec = vectorizer.transform([doc_text])\n",
    "    sims = cosine_similarity(doc_vec, label_tfidf)[0]\n",
    "    return sims\n",
    "\n",
    "    \n",
    "label_raw_text = load_label_file(\"Amazon_products/class_related_keywords.txt\")  # 네 파일 이름에 맞춰 바꿔\n",
    "id2label = parse_key_value_lines(label_raw_text)\n",
    "\n",
    "# 2) 라벨 텍스트 전처리해서 TF-IDF 학습\n",
    "label_keys = list(id2label.keys())\n",
    "label_texts = [\n",
    "    preprocess_label_text(f\"{k} {id2label[k]}\")\n",
    "    for k in label_keys\n",
    "]\n",
    "vectorizer, label_tfidf = build_tfidf_vectorizer(label_texts)\n",
    "\n",
    "# 3) 테스트용 문서 하나 넣어보기\n",
    "doc = \"gourmet organic chocolate snack\"\n",
    "doc_clean = preprocess_label_text(doc)\n",
    "sims = compute_lexical_similarity(doc_clean, vectorizer, label_tfidf)\n",
    "\n",
    "# 4) 결과 보기\n",
    "label_sims = list(zip(label_keys, sims))\n",
    "label_sims.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for lbl, score in label_sims:\n",
    "    print(lbl, round(score, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19ba131e-72d7-4e8c-b1fe-2f3ef78b1466",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T07:49:41.592677Z",
     "iopub.status.busy": "2025-11-13T07:49:41.592423Z",
     "iopub.status.idle": "2025-11-13T07:49:41.604171Z",
     "shell.execute_reply": "2025-11-13T07:49:41.603764Z",
     "shell.execute_reply.started": "2025-11-13T07:49:41.592661Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3466,)\n"
     ]
    }
   ],
   "source": [
    "def build_label_embeddings(label_keys, label_tfidf, dense: bool = True):\n",
    "    \"\"\"\n",
    "    label_keys: 라벨 이름 리스트 (vectorize할 때 썼던 순서와 같아야 함)\n",
    "    label_tfidf: shape = (n_labels, vocab_size) 인 sparse matrix\n",
    "    dense: True면 numpy array로 바꿔서 돌려줌\n",
    "\n",
    "    return:\n",
    "        dict: {label_name: embedding_vector}\n",
    "    \"\"\"\n",
    "    embeddings = {}\n",
    "    if dense:\n",
    "        label_tfidf_dense = label_tfidf.toarray()\n",
    "        for i, label in enumerate(label_keys):\n",
    "            embeddings[label] = label_tfidf_dense[i]\n",
    "    else:\n",
    "        # sparse 그대로\n",
    "        for i, label in enumerate(label_keys):\n",
    "            embeddings[label] = label_tfidf[i]\n",
    "    return embeddings\n",
    "\n",
    "label_embeddings = build_label_embeddings(label_keys, label_tfidf, dense=True)\n",
    "print(label_embeddings[\"grocery_gourmet_food\"].shape)  # (vocab_size,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e1bcacb-8765-49a0-b6d6-d18d6f5e5531",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T01:28:59.947460Z",
     "iopub.status.busy": "2025-11-11T01:28:59.947322Z",
     "iopub.status.idle": "2025-11-11T01:28:59.988985Z",
     "shell.execute_reply": "2025-11-11T01:28:59.988493Z",
     "shell.execute_reply.started": "2025-11-11T01:28:59.947446Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5efad2e-4006-4b90-920b-1e70cc6b7ca7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T07:49:41.605748Z",
     "iopub.status.busy": "2025-11-13T07:49:41.605617Z",
     "iopub.status.idle": "2025-11-13T07:49:41.610905Z",
     "shell.execute_reply": "2025-11-13T07:49:41.610465Z",
     "shell.execute_reply.started": "2025-11-13T07:49:41.605734Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roots: [0, 3, 10, 23, 40, 169]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_edges(path):\n",
    "    edges = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            raw = line.strip()\n",
    "            if not raw or raw.startswith(\"#\"):\n",
    "                continue\n",
    "            parts = raw.split()\n",
    "            if len(parts) < 2:\n",
    "                continue\n",
    "            try:\n",
    "                u, v = int(parts[0]), int(parts[1])\n",
    "            except ValueError:\n",
    "                continue\n",
    "            edges.append((u, v))\n",
    "    return edges\n",
    "\n",
    "def find_roots(edges):\n",
    "    parents = set()\n",
    "    children = set()\n",
    "    for u, v in edges:\n",
    "        parents.add(u)\n",
    "        children.add(v)\n",
    "    # 부모로만 나온 애들 = 루트들\n",
    "    roots = parents - children\n",
    "    return sorted(roots)\n",
    "\n",
    "# --- 사용 ---\n",
    "E = load_edges(\"Amazon_products/class_hierarchy.txt\")\n",
    "\n",
    "N = 531\n",
    "A = np.zeros((N, N), dtype=np.uint8)\n",
    "for u, v in E:\n",
    "    A[u, v] = 1\n",
    "    A[v, u] = 1   # 탐색용으로는 무방향 인접행렬 써도 됨\n",
    "\n",
    "B = np.zeros((N, N), dtype=np.uint8)\n",
    "for u, v in E:\n",
    "    B[u, v] = 1\n",
    "\n",
    "roots = find_roots(E)\n",
    "print(\"roots:\", roots)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cde32d7f-3c5e-4810-ab28-07a7abd05c59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T04:51:03.300617Z",
     "iopub.status.busy": "2025-11-13T04:51:03.300194Z",
     "iopub.status.idle": "2025-11-13T04:51:03.307868Z",
     "shell.execute_reply": "2025-11-13T04:51:03.307464Z",
     "shell.execute_reply.started": "2025-11-13T04:51:03.300601Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------\n",
    "# GAT \n",
    "# ---------------------------\n",
    "\n",
    "class SimpleGATLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, heads=4, concat=True, dropout=0.2, negative_slope=0.2, residual=True):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.out_dim = out_dim\n",
    "        self.concat = concat\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.leaky_relu = nn.LeakyReLU(negative_slope)\n",
    "        self.lin = nn.Linear(in_dim, heads * out_dim, bias=False)\n",
    "        self.a_src = nn.Parameter(torch.Tensor(heads, out_dim))\n",
    "        self.a_dst = nn.Parameter(torch.Tensor(heads, out_dim))\n",
    "        self.residual = residual\n",
    "        if residual and (in_dim == (heads * out_dim if concat else out_dim)):\n",
    "            self.res_proj = nn.Identity()\n",
    "        elif residual:\n",
    "            self.res_proj = nn.Linear(in_dim, heads * out_dim if concat else out_dim, bias=False)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.lin.weight)\n",
    "        nn.init.xavier_uniform_(self.a_src)\n",
    "        nn.init.xavier_uniform_(self.a_dst)\n",
    "        if self.residual and not isinstance(getattr(self, \"res_proj\", None), nn.Identity):\n",
    "            nn.init.xavier_uniform_(self.res_proj.weight)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        \"\"\"\n",
    "        x: [N, Fin]\n",
    "        adj: [N, N] (0/1; self-loop 없음)\n",
    "        \"\"\"\n",
    "        N = x.size(0)\n",
    "        Wh = self.lin(x).view(N, self.heads, self.out_dim)  # [N, H, F]\n",
    "\n",
    "        e_src = (Wh * self.a_src).sum(dim=-1)  # [N, H]\n",
    "        e_dst = (Wh * self.a_dst).sum(dim=-1)  # [N, H]\n",
    "        e = e_src.unsqueeze(1) + e_dst.unsqueeze(0)  # [N, N, H]\n",
    "        e = self.leaky_relu(e)\n",
    "        # --- 안전한 masked softmax ---\n",
    "        mask = (adj > 0).unsqueeze(-1)                    # [N, N, 1]\n",
    "        e = e.masked_fill(~mask, -1e9)                    # -inf 대신 -1e9로 NaN 방지\n",
    "        alpha = torch.softmax(e, dim=1)                   # 소프트맥스\n",
    "        alpha = alpha * mask.float()                      # 마스크로 0 처리\n",
    "        denom = alpha.sum(dim=1, keepdim=True).clamp(min=1e-12)  # 이웃 없을 때 0 분모 방지\n",
    "        alpha = alpha / denom                             # 이웃들로 정규화\n",
    "\n",
    "        out = torch.einsum(\"ijh,jhf->ihf\", alpha, Wh)     # [N, H, F]\n",
    "        out = out.reshape(N, self.heads * self.out_dim) if self.concat else out.mean(dim=1)\n",
    "        out = self.dropout(out)\n",
    "        if self.residual:\n",
    "            out = out + self.res_proj(x)                  # self-loop 없는 대신 residual로 자기정보 유지\n",
    "        return out\n",
    "\n",
    "class GATEncoder(nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim=64, out_dim=768, heads1=4, heads2=4, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.gat1 = SimpleGATLayer(in_dim, hid_dim, heads=heads1, concat=True,  dropout=dropout, residual=True)\n",
    "        self.gat2 = SimpleGATLayer(hid_dim*heads1, out_dim, heads=heads2, concat=False, dropout=dropout, residual=True)\n",
    "        self.act = nn.ELU(); self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x, adj):\n",
    "        h = self.gat1(x, adj); h = self.act(h); h = self.dropout(h)\n",
    "        z = self.gat2(h, adj)\n",
    "        return z  # [N, out_dim]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b080c703-91ce-4a83-b745-e87986fcddc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T04:51:05.585330Z",
     "iopub.status.busy": "2025-11-13T04:51:05.584976Z",
     "iopub.status.idle": "2025-11-13T04:51:05.587902Z",
     "shell.execute_reply": "2025-11-13T04:51:05.587465Z",
     "shell.execute_reply.started": "2025-11-13T04:51:05.585315Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da4d3075-be19-4053-9b2f-827bcd31d952",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T04:51:25.412435Z",
     "iopub.status.busy": "2025-11-13T04:51:25.412193Z",
     "iopub.status.idle": "2025-11-13T04:51:53.325519Z",
     "shell.execute_reply": "2025-11-13T04:51:53.325032Z",
     "shell.execute_reply.started": "2025-11-13T04:51:25.412419Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001/700] loss=0.6678 | pos=0.120 neg=0.010 | val AUC=0.7806\n",
      "[002/700] loss=0.6270 | pos=0.303 neg=0.011 | val AUC=0.8425\n",
      "[003/700] loss=0.5963 | pos=0.452 neg=0.009 | val AUC=0.8243\n",
      "[004/700] loss=0.5750 | pos=0.566 neg=0.008 | val AUC=0.8485\n",
      "[005/700] loss=0.5626 | pos=0.652 neg=0.016 | val AUC=0.8638\n",
      "[006/700] loss=0.5477 | pos=0.712 neg=-0.003 | val AUC=0.8221\n",
      "[007/700] loss=0.5384 | pos=0.762 neg=-0.012 | val AUC=0.8323\n",
      "[008/700] loss=0.5343 | pos=0.795 neg=-0.010 | val AUC=0.8536\n",
      "[009/700] loss=0.5267 | pos=0.820 neg=-0.029 | val AUC=0.8093\n",
      "[010/700] loss=0.5323 | pos=0.842 neg=0.002 | val AUC=0.8473\n",
      "[011/700] loss=0.5279 | pos=0.859 neg=-0.008 | val AUC=0.8042\n",
      "[012/700] loss=0.5269 | pos=0.872 neg=-0.005 | val AUC=0.8508\n",
      "[013/700] loss=0.5281 | pos=0.881 neg=0.004 | val AUC=0.8217\n",
      "[014/700] loss=0.5197 | pos=0.889 neg=-0.026 | val AUC=0.8689\n",
      "[015/700] loss=0.5234 | pos=0.894 neg=-0.012 | val AUC=0.8383\n",
      "[016/700] loss=0.5224 | pos=0.901 neg=-0.015 | val AUC=0.7924\n",
      "[017/700] loss=0.5311 | pos=0.907 neg=0.022 | val AUC=0.8048\n",
      "[018/700] loss=0.5137 | pos=0.911 neg=-0.043 | val AUC=0.8230\n",
      "[019/700] loss=0.5192 | pos=0.915 neg=-0.018 | val AUC=0.7988\n",
      "[020/700] loss=0.5160 | pos=0.917 neg=-0.034 | val AUC=0.8160\n",
      "[021/700] loss=0.5230 | pos=0.921 neg=-0.006 | val AUC=0.8186\n",
      "[022/700] loss=0.5273 | pos=0.922 neg=0.014 | val AUC=0.7663\n",
      "[023/700] loss=0.5238 | pos=0.925 neg=0.001 | val AUC=0.7363\n",
      "[024/700] loss=0.5262 | pos=0.925 neg=0.009 | val AUC=0.7985\n",
      "[025/700] loss=0.5213 | pos=0.926 neg=-0.004 | val AUC=0.7497\n",
      "[026/700] loss=0.5293 | pos=0.927 neg=0.021 | val AUC=0.6958\n",
      "[027/700] loss=0.5227 | pos=0.928 neg=0.000 | val AUC=0.7733\n",
      "[028/700] loss=0.5164 | pos=0.928 neg=-0.025 | val AUC=0.7809\n",
      "[029/700] loss=0.5232 | pos=0.929 neg=0.003 | val AUC=0.7471\n",
      "[030/700] loss=0.5224 | pos=0.930 neg=-0.001 | val AUC=0.7860\n",
      "[031/700] loss=0.5223 | pos=0.931 neg=-0.000 | val AUC=0.7532\n",
      "[032/700] loss=0.5276 | pos=0.933 neg=0.019 | val AUC=0.7500\n",
      "[033/700] loss=0.5207 | pos=0.933 neg=-0.007 | val AUC=0.7455\n",
      "[034/700] loss=0.5213 | pos=0.935 neg=-0.001 | val AUC=0.7121\n",
      "[035/700] loss=0.5181 | pos=0.937 neg=-0.015 | val AUC=0.7181\n",
      "[036/700] loss=0.5256 | pos=0.937 neg=0.014 | val AUC=0.6964\n",
      "[037/700] loss=0.5206 | pos=0.937 neg=-0.008 | val AUC=0.7076\n",
      "[038/700] loss=0.5252 | pos=0.938 neg=0.011 | val AUC=0.6971\n",
      "[039/700] loss=0.5177 | pos=0.937 neg=-0.019 | val AUC=0.6760\n",
      "[040/700] loss=0.5291 | pos=0.934 neg=0.025 | val AUC=0.7223\n",
      "[041/700] loss=0.5206 | pos=0.934 neg=-0.006 | val AUC=0.7376\n",
      "[042/700] loss=0.5308 | pos=0.932 neg=0.032 | val AUC=0.6805\n",
      "[043/700] loss=0.5130 | pos=0.931 neg=-0.039 | val AUC=0.6553\n",
      "[044/700] loss=0.5202 | pos=0.929 neg=-0.009 | val AUC=0.6645\n",
      "[045/700] loss=0.5282 | pos=0.929 neg=0.019 | val AUC=0.6610\n",
      "[046/700] loss=0.5316 | pos=0.929 neg=0.030 | val AUC=0.7344\n",
      "[047/700] loss=0.5263 | pos=0.930 neg=0.011 | val AUC=0.6779\n",
      "[048/700] loss=0.5179 | pos=0.933 neg=-0.018 | val AUC=0.6547\n",
      "[049/700] loss=0.5226 | pos=0.932 neg=0.002 | val AUC=0.7548\n",
      "[050/700] loss=0.5138 | pos=0.931 neg=-0.031 | val AUC=0.7270\n",
      "[051/700] loss=0.5297 | pos=0.932 neg=0.022 | val AUC=0.7274\n",
      "[052/700] loss=0.5203 | pos=0.932 neg=-0.010 | val AUC=0.7490\n",
      "[053/700] loss=0.5228 | pos=0.933 neg=-0.001 | val AUC=0.6983\n",
      "[054/700] loss=0.5154 | pos=0.934 neg=-0.026 | val AUC=0.7143\n",
      "[055/700] loss=0.5233 | pos=0.934 neg=0.002 | val AUC=0.7484\n",
      "[056/700] loss=0.5237 | pos=0.938 neg=0.007 | val AUC=0.7127\n",
      "[057/700] loss=0.5129 | pos=0.940 neg=-0.036 | val AUC=0.7302\n",
      "[058/700] loss=0.5247 | pos=0.940 neg=0.012 | val AUC=0.7669\n",
      "[059/700] loss=0.5214 | pos=0.940 neg=-0.001 | val AUC=0.7592\n",
      "[060/700] loss=0.5189 | pos=0.939 neg=-0.013 | val AUC=0.7790\n",
      "[061/700] loss=0.5267 | pos=0.936 neg=0.019 | val AUC=0.7634\n",
      "[062/700] loss=0.5258 | pos=0.938 neg=0.019 | val AUC=0.7765\n",
      "[063/700] loss=0.5165 | pos=0.937 neg=-0.026 | val AUC=0.7506\n",
      "[064/700] loss=0.5258 | pos=0.937 neg=0.014 | val AUC=0.7784\n",
      "[065/700] loss=0.5259 | pos=0.936 neg=0.014 | val AUC=0.7848\n",
      "[066/700] loss=0.5262 | pos=0.936 neg=0.015 | val AUC=0.8042\n",
      "[067/700] loss=0.5211 | pos=0.937 neg=-0.001 | val AUC=0.7812\n",
      "[068/700] loss=0.5225 | pos=0.938 neg=0.005 | val AUC=0.7503\n",
      "[069/700] loss=0.5201 | pos=0.938 neg=-0.007 | val AUC=0.7835\n",
      "[070/700] loss=0.5156 | pos=0.939 neg=-0.023 | val AUC=0.7844\n",
      "[071/700] loss=0.5224 | pos=0.938 neg=-0.001 | val AUC=0.7586\n",
      "[072/700] loss=0.5109 | pos=0.935 neg=-0.045 | val AUC=0.6999\n",
      "[073/700] loss=0.5259 | pos=0.935 neg=0.012 | val AUC=0.7538\n",
      "[074/700] loss=0.5145 | pos=0.936 neg=-0.031 | val AUC=0.7624\n",
      "[075/700] loss=0.5267 | pos=0.938 neg=0.016 | val AUC=0.7554\n",
      "[076/700] loss=0.5259 | pos=0.938 neg=0.015 | val AUC=0.7057\n",
      "[077/700] loss=0.5258 | pos=0.938 neg=0.016 | val AUC=0.7551\n",
      "[078/700] loss=0.5206 | pos=0.936 neg=-0.001 | val AUC=0.7044\n",
      "[079/700] loss=0.5129 | pos=0.936 neg=-0.031 | val AUC=0.7050\n",
      "[080/700] loss=0.5216 | pos=0.938 neg=0.002 | val AUC=0.7643\n",
      "[081/700] loss=0.5176 | pos=0.938 neg=-0.017 | val AUC=0.7312\n",
      "[082/700] loss=0.5188 | pos=0.938 neg=-0.011 | val AUC=0.6620\n",
      "[083/700] loss=0.5284 | pos=0.937 neg=0.025 | val AUC=0.7079\n",
      "[084/700] loss=0.5222 | pos=0.938 neg=0.005 | val AUC=0.6550\n",
      "[085/700] loss=0.5251 | pos=0.938 neg=0.017 | val AUC=0.6974\n",
      "[086/700] loss=0.5206 | pos=0.940 neg=-0.002 | val AUC=0.7060\n",
      "[087/700] loss=0.5178 | pos=0.940 neg=-0.010 | val AUC=0.6814\n",
      "[088/700] loss=0.5145 | pos=0.942 neg=-0.023 | val AUC=0.7219\n",
      "[089/700] loss=0.5159 | pos=0.942 neg=-0.018 | val AUC=0.6712\n",
      "[090/700] loss=0.5152 | pos=0.943 neg=-0.024 | val AUC=0.7210\n",
      "[091/700] loss=0.5140 | pos=0.946 neg=-0.032 | val AUC=0.7136\n",
      "[092/700] loss=0.5176 | pos=0.947 neg=-0.016 | val AUC=0.7239\n",
      "[093/700] loss=0.5225 | pos=0.947 neg=-0.002 | val AUC=0.7124\n",
      "[094/700] loss=0.5178 | pos=0.948 neg=-0.020 | val AUC=0.7219\n",
      "[095/700] loss=0.5269 | pos=0.947 neg=0.015 | val AUC=0.7567\n",
      "[096/700] loss=0.5171 | pos=0.948 neg=-0.020 | val AUC=0.5953\n",
      "[097/700] loss=0.5207 | pos=0.947 neg=-0.008 | val AUC=0.7254\n",
      "[098/700] loss=0.5108 | pos=0.944 neg=-0.053 | val AUC=0.7258\n",
      "[099/700] loss=0.5279 | pos=0.941 neg=0.014 | val AUC=0.7066\n",
      "[100/700] loss=0.5309 | pos=0.939 neg=0.024 | val AUC=0.7066\n",
      "[101/700] loss=0.5296 | pos=0.941 neg=0.022 | val AUC=0.7742\n",
      "[102/700] loss=0.5184 | pos=0.938 neg=-0.022 | val AUC=0.7742\n",
      "[103/700] loss=0.5195 | pos=0.939 neg=-0.014 | val AUC=0.7905\n",
      "[104/700] loss=0.5236 | pos=0.938 neg=0.002 | val AUC=0.7902\n",
      "[105/700] loss=0.5270 | pos=0.940 neg=0.013 | val AUC=0.7526\n",
      "[106/700] loss=0.5146 | pos=0.939 neg=-0.034 | val AUC=0.7465\n",
      "[107/700] loss=0.5163 | pos=0.938 neg=-0.023 | val AUC=0.6735\n",
      "[108/700] loss=0.5207 | pos=0.940 neg=-0.006 | val AUC=0.7216\n",
      "[109/700] loss=0.5189 | pos=0.940 neg=-0.014 | val AUC=0.7296\n",
      "[110/700] loss=0.5140 | pos=0.940 neg=-0.034 | val AUC=0.6849\n",
      "[111/700] loss=0.5212 | pos=0.941 neg=-0.003 | val AUC=0.7274\n",
      "[112/700] loss=0.5244 | pos=0.942 neg=0.011 | val AUC=0.7283\n",
      "[113/700] loss=0.5126 | pos=0.943 neg=-0.043 | val AUC=0.7592\n",
      "[114/700] loss=0.5165 | pos=0.944 neg=-0.022 | val AUC=0.7478\n",
      "[115/700] loss=0.5239 | pos=0.944 neg=0.006 | val AUC=0.7631\n",
      "[116/700] loss=0.5276 | pos=0.944 neg=0.022 | val AUC=0.7538\n",
      "[117/700] loss=0.5197 | pos=0.945 neg=-0.011 | val AUC=0.7557\n",
      "[118/700] loss=0.5192 | pos=0.945 neg=-0.011 | val AUC=0.7114\n",
      "[119/700] loss=0.5137 | pos=0.944 neg=-0.030 | val AUC=0.7771\n",
      "[120/700] loss=0.5177 | pos=0.945 neg=-0.019 | val AUC=0.7777\n",
      "[121/700] loss=0.5223 | pos=0.945 neg=0.006 | val AUC=0.7797\n",
      "[122/700] loss=0.5249 | pos=0.945 neg=0.012 | val AUC=0.7631\n",
      "[123/700] loss=0.5192 | pos=0.944 neg=-0.011 | val AUC=0.7924\n",
      "[124/700] loss=0.5197 | pos=0.943 neg=-0.009 | val AUC=0.7577\n",
      "[125/700] loss=0.5187 | pos=0.941 neg=-0.014 | val AUC=0.6945\n",
      "[126/700] loss=0.5224 | pos=0.941 neg=-0.001 | val AUC=0.7133\n",
      "[127/700] loss=0.5241 | pos=0.940 neg=0.007 | val AUC=0.7695\n",
      "[128/700] loss=0.5166 | pos=0.942 neg=-0.024 | val AUC=0.7309\n",
      "[129/700] loss=0.5258 | pos=0.940 neg=0.011 | val AUC=0.7462\n",
      "[130/700] loss=0.5164 | pos=0.941 neg=-0.027 | val AUC=0.7717\n",
      "[131/700] loss=0.5216 | pos=0.940 neg=-0.003 | val AUC=0.7621\n",
      "[132/700] loss=0.5176 | pos=0.940 neg=-0.018 | val AUC=0.8099\n",
      "[133/700] loss=0.5223 | pos=0.940 neg=0.002 | val AUC=0.7994\n",
      "[134/700] loss=0.5186 | pos=0.941 neg=-0.015 | val AUC=0.8042\n",
      "[135/700] loss=0.5201 | pos=0.941 neg=-0.009 | val AUC=0.8026\n",
      "[136/700] loss=0.5214 | pos=0.943 neg=-0.005 | val AUC=0.7270\n",
      "[137/700] loss=0.5243 | pos=0.942 neg=0.004 | val AUC=0.7529\n",
      "[138/700] loss=0.5223 | pos=0.942 neg=0.000 | val AUC=0.7930\n",
      "[139/700] loss=0.5240 | pos=0.944 neg=0.009 | val AUC=0.8157\n",
      "[140/700] loss=0.5222 | pos=0.943 neg=-0.004 | val AUC=0.7510\n",
      "[141/700] loss=0.5118 | pos=0.944 neg=-0.039 | val AUC=0.7774\n",
      "[142/700] loss=0.5206 | pos=0.942 neg=-0.007 | val AUC=0.7545\n",
      "[143/700] loss=0.5191 | pos=0.942 neg=-0.011 | val AUC=0.7730\n",
      "[144/700] loss=0.5199 | pos=0.942 neg=-0.005 | val AUC=0.8291\n",
      "[145/700] loss=0.5173 | pos=0.941 neg=-0.017 | val AUC=0.7720\n",
      "[146/700] loss=0.5154 | pos=0.940 neg=-0.027 | val AUC=0.7643\n",
      "[147/700] loss=0.5214 | pos=0.940 neg=-0.004 | val AUC=0.7988\n",
      "[148/700] loss=0.5149 | pos=0.938 neg=-0.031 | val AUC=0.7730\n",
      "[149/700] loss=0.5249 | pos=0.937 neg=0.009 | val AUC=0.7459\n",
      "[150/700] loss=0.5158 | pos=0.936 neg=-0.023 | val AUC=0.7934\n",
      "[151/700] loss=0.5227 | pos=0.937 neg=0.006 | val AUC=0.7800\n",
      "[152/700] loss=0.5294 | pos=0.937 neg=0.031 | val AUC=0.7175\n",
      "[153/700] loss=0.5220 | pos=0.937 neg=0.001 | val AUC=0.8182\n",
      "[154/700] loss=0.5230 | pos=0.937 neg=0.005 | val AUC=0.7733\n",
      "[155/700] loss=0.5169 | pos=0.937 neg=-0.019 | val AUC=0.7899\n",
      "[156/700] loss=0.5226 | pos=0.936 neg=0.009 | val AUC=0.7675\n",
      "[157/700] loss=0.5264 | pos=0.938 neg=0.019 | val AUC=0.7325\n",
      "[158/700] loss=0.5205 | pos=0.941 neg=-0.004 | val AUC=0.8307\n",
      "[159/700] loss=0.5182 | pos=0.940 neg=-0.012 | val AUC=0.7768\n",
      "[160/700] loss=0.5258 | pos=0.942 neg=0.020 | val AUC=0.8010\n",
      "[161/700] loss=0.5190 | pos=0.943 neg=-0.009 | val AUC=0.7200\n",
      "[162/700] loss=0.5253 | pos=0.942 neg=0.009 | val AUC=0.7312\n",
      "[163/700] loss=0.5165 | pos=0.941 neg=-0.020 | val AUC=0.7098\n",
      "[164/700] loss=0.5105 | pos=0.943 neg=-0.045 | val AUC=0.6926\n",
      "[165/700] loss=0.5248 | pos=0.943 neg=0.011 | val AUC=0.7784\n",
      "[166/700] loss=0.5224 | pos=0.942 neg=0.001 | val AUC=0.7082\n",
      "[167/700] loss=0.5198 | pos=0.942 neg=-0.006 | val AUC=0.6967\n",
      "[168/700] loss=0.5222 | pos=0.943 neg=-0.000 | val AUC=0.8045\n",
      "[169/700] loss=0.5209 | pos=0.943 neg=-0.003 | val AUC=0.7420\n",
      "[170/700] loss=0.5174 | pos=0.943 neg=-0.015 | val AUC=0.7395\n",
      "[171/700] loss=0.5261 | pos=0.944 neg=0.014 | val AUC=0.7851\n",
      "[172/700] loss=0.5310 | pos=0.945 neg=0.033 | val AUC=0.7730\n",
      "[173/700] loss=0.5179 | pos=0.946 neg=-0.016 | val AUC=0.7239\n",
      "[174/700] loss=0.5170 | pos=0.945 neg=-0.023 | val AUC=0.7899\n",
      "[175/700] loss=0.5185 | pos=0.946 neg=-0.015 | val AUC=0.7835\n",
      "[176/700] loss=0.5196 | pos=0.948 neg=-0.007 | val AUC=0.8013\n",
      "[177/700] loss=0.5179 | pos=0.948 neg=-0.017 | val AUC=0.7280\n",
      "[178/700] loss=0.5121 | pos=0.947 neg=-0.039 | val AUC=0.7468\n",
      "[179/700] loss=0.5202 | pos=0.944 neg=-0.012 | val AUC=0.7730\n",
      "[180/700] loss=0.5249 | pos=0.945 neg=0.009 | val AUC=0.7455\n",
      "[181/700] loss=0.5235 | pos=0.943 neg=0.005 | val AUC=0.7363\n",
      "[182/700] loss=0.5271 | pos=0.941 neg=0.017 | val AUC=0.7615\n",
      "[183/700] loss=0.5254 | pos=0.940 neg=0.016 | val AUC=0.7768\n",
      "[184/700] loss=0.5151 | pos=0.937 neg=-0.030 | val AUC=0.7213\n",
      "[185/700] loss=0.5258 | pos=0.938 neg=0.014 | val AUC=0.7283\n",
      "[186/700] loss=0.5272 | pos=0.937 neg=0.018 | val AUC=0.7761\n",
      "[187/700] loss=0.5233 | pos=0.936 neg=0.002 | val AUC=0.7315\n",
      "[188/700] loss=0.5248 | pos=0.935 neg=0.008 | val AUC=0.7879\n",
      "[189/700] loss=0.5192 | pos=0.936 neg=-0.016 | val AUC=0.7481\n",
      "[190/700] loss=0.5239 | pos=0.936 neg=0.007 | val AUC=0.7723\n",
      "[191/700] loss=0.5211 | pos=0.937 neg=-0.001 | val AUC=0.7519\n",
      "[192/700] loss=0.5245 | pos=0.936 neg=0.005 | val AUC=0.7439\n",
      "[193/700] loss=0.5244 | pos=0.938 neg=0.011 | val AUC=0.7079\n",
      "[194/700] loss=0.5231 | pos=0.936 neg=0.003 | val AUC=0.6834\n",
      "[195/700] loss=0.5331 | pos=0.937 neg=0.042 | val AUC=0.7277\n",
      "[196/700] loss=0.5186 | pos=0.937 neg=-0.012 | val AUC=0.7956\n",
      "[197/700] loss=0.5187 | pos=0.937 neg=-0.009 | val AUC=0.7497\n",
      "[198/700] loss=0.5225 | pos=0.937 neg=0.001 | val AUC=0.7835\n",
      "[199/700] loss=0.5250 | pos=0.936 neg=0.010 | val AUC=0.6865\n",
      "[200/700] loss=0.5250 | pos=0.938 neg=0.011 | val AUC=0.7085\n",
      "[201/700] loss=0.5216 | pos=0.938 neg=0.001 | val AUC=0.7235\n",
      "[202/700] loss=0.5228 | pos=0.938 neg=0.009 | val AUC=0.6811\n",
      "[203/700] loss=0.5171 | pos=0.938 neg=-0.014 | val AUC=0.7245\n",
      "[204/700] loss=0.5131 | pos=0.938 neg=-0.034 | val AUC=0.7369\n",
      "[205/700] loss=0.5226 | pos=0.938 neg=-0.001 | val AUC=0.6725\n",
      "[206/700] loss=0.5230 | pos=0.939 neg=0.006 | val AUC=0.7229\n",
      "[207/700] loss=0.5143 | pos=0.940 neg=-0.025 | val AUC=0.6939\n",
      "[208/700] loss=0.5261 | pos=0.941 neg=0.013 | val AUC=0.6840\n",
      "[209/700] loss=0.5182 | pos=0.941 neg=-0.014 | val AUC=0.7522\n",
      "[210/700] loss=0.5331 | pos=0.942 neg=0.042 | val AUC=0.6830\n",
      "[211/700] loss=0.5287 | pos=0.940 neg=0.023 | val AUC=0.7353\n",
      "[212/700] loss=0.5229 | pos=0.940 neg=0.003 | val AUC=0.6703\n",
      "[213/700] loss=0.5126 | pos=0.940 neg=-0.031 | val AUC=0.7018\n",
      "[214/700] loss=0.5238 | pos=0.938 neg=0.009 | val AUC=0.7133\n",
      "[215/700] loss=0.5193 | pos=0.938 neg=-0.012 | val AUC=0.7315\n",
      "[216/700] loss=0.5179 | pos=0.938 neg=-0.013 | val AUC=0.7277\n",
      "[217/700] loss=0.5185 | pos=0.938 neg=-0.015 | val AUC=0.7168\n",
      "[218/700] loss=0.5164 | pos=0.941 neg=-0.016 | val AUC=0.7229\n",
      "[219/700] loss=0.5256 | pos=0.941 neg=0.014 | val AUC=0.7006\n",
      "[220/700] loss=0.5287 | pos=0.943 neg=0.028 | val AUC=0.6907\n",
      "[221/700] loss=0.5160 | pos=0.942 neg=-0.021 | val AUC=0.7079\n",
      "[222/700] loss=0.5182 | pos=0.943 neg=-0.008 | val AUC=0.7328\n",
      "[223/700] loss=0.5118 | pos=0.941 neg=-0.034 | val AUC=0.7742\n",
      "[224/700] loss=0.5176 | pos=0.941 neg=-0.012 | val AUC=0.7490\n",
      "[225/700] loss=0.5194 | pos=0.942 neg=-0.007 | val AUC=0.7752\n",
      "[226/700] loss=0.5204 | pos=0.942 neg=-0.001 | val AUC=0.7666\n",
      "[227/700] loss=0.5171 | pos=0.943 neg=-0.013 | val AUC=0.7229\n",
      "[228/700] loss=0.5121 | pos=0.944 neg=-0.030 | val AUC=0.6958\n",
      "[229/700] loss=0.5196 | pos=0.944 neg=-0.005 | val AUC=0.7439\n",
      "[230/700] loss=0.5208 | pos=0.944 neg=-0.000 | val AUC=0.7235\n",
      "[231/700] loss=0.5225 | pos=0.945 neg=0.010 | val AUC=0.7159\n",
      "[232/700] loss=0.5205 | pos=0.944 neg=-0.001 | val AUC=0.6967\n",
      "[233/700] loss=0.5210 | pos=0.943 neg=-0.004 | val AUC=0.7411\n",
      "[234/700] loss=0.5236 | pos=0.943 neg=0.006 | val AUC=0.7280\n",
      "[235/700] loss=0.5271 | pos=0.944 neg=0.023 | val AUC=0.7191\n",
      "[236/700] loss=0.5237 | pos=0.944 neg=0.010 | val AUC=0.7366\n",
      "[237/700] loss=0.5188 | pos=0.945 neg=-0.007 | val AUC=0.7420\n",
      "[238/700] loss=0.5246 | pos=0.944 neg=0.011 | val AUC=0.6987\n",
      "[239/700] loss=0.5166 | pos=0.943 neg=-0.021 | val AUC=0.6932\n",
      "[240/700] loss=0.5219 | pos=0.942 neg=-0.001 | val AUC=0.7500\n",
      "[241/700] loss=0.5233 | pos=0.942 neg=0.007 | val AUC=0.7028\n",
      "[242/700] loss=0.5311 | pos=0.942 neg=0.040 | val AUC=0.6849\n",
      "[243/700] loss=0.5244 | pos=0.942 neg=0.012 | val AUC=0.7529\n",
      "[244/700] loss=0.5201 | pos=0.941 neg=-0.007 | val AUC=0.7085\n",
      "[245/700] loss=0.5167 | pos=0.941 neg=-0.019 | val AUC=0.7551\n",
      "[246/700] loss=0.5189 | pos=0.941 neg=-0.014 | val AUC=0.7592\n",
      "[247/700] loss=0.5246 | pos=0.943 neg=0.011 | val AUC=0.7879\n",
      "[248/700] loss=0.5202 | pos=0.945 neg=-0.011 | val AUC=0.6916\n",
      "[249/700] loss=0.5223 | pos=0.945 neg=-0.000 | val AUC=0.6763\n",
      "[250/700] loss=0.5275 | pos=0.948 neg=0.016 | val AUC=0.7851\n",
      "[251/700] loss=0.5216 | pos=0.948 neg=-0.003 | val AUC=0.7586\n",
      "[252/700] loss=0.5326 | pos=0.948 neg=0.039 | val AUC=0.7561\n",
      "[253/700] loss=0.5131 | pos=0.947 neg=-0.036 | val AUC=0.6929\n",
      "[254/700] loss=0.5267 | pos=0.947 neg=0.020 | val AUC=0.7223\n",
      "[255/700] loss=0.5235 | pos=0.948 neg=0.008 | val AUC=0.7474\n",
      "[256/700] loss=0.5221 | pos=0.949 neg=0.003 | val AUC=0.7589\n",
      "[257/700] loss=0.5162 | pos=0.947 neg=-0.018 | val AUC=0.7774\n",
      "[258/700] loss=0.5187 | pos=0.947 neg=-0.004 | val AUC=0.7883\n",
      "[259/700] loss=0.5260 | pos=0.945 neg=0.020 | val AUC=0.7650\n",
      "[260/700] loss=0.5215 | pos=0.944 neg=0.005 | val AUC=0.7602\n",
      "[261/700] loss=0.5222 | pos=0.943 neg=0.007 | val AUC=0.7245\n",
      "[262/700] loss=0.5161 | pos=0.944 neg=-0.019 | val AUC=0.7532\n",
      "[263/700] loss=0.5152 | pos=0.944 neg=-0.017 | val AUC=0.7239\n",
      "[264/700] loss=0.5175 | pos=0.944 neg=-0.014 | val AUC=0.7280\n",
      "[265/700] loss=0.5234 | pos=0.946 neg=0.013 | val AUC=0.7551\n",
      "[266/700] loss=0.5260 | pos=0.946 neg=0.023 | val AUC=0.7038\n",
      "[267/700] loss=0.5212 | pos=0.945 neg=0.000 | val AUC=0.7382\n",
      "[268/700] loss=0.5146 | pos=0.945 neg=-0.025 | val AUC=0.7666\n",
      "[269/700] loss=0.5231 | pos=0.945 neg=0.009 | val AUC=0.7972\n",
      "[270/700] loss=0.5186 | pos=0.945 neg=-0.008 | val AUC=0.8080\n",
      "[271/700] loss=0.5197 | pos=0.943 neg=-0.002 | val AUC=0.7787\n",
      "[272/700] loss=0.5163 | pos=0.942 neg=-0.017 | val AUC=0.8020\n",
      "[273/700] loss=0.5177 | pos=0.943 neg=-0.015 | val AUC=0.8240\n",
      "[274/700] loss=0.5187 | pos=0.942 neg=-0.011 | val AUC=0.7417\n",
      "[275/700] loss=0.5295 | pos=0.942 neg=0.030 | val AUC=0.7401\n",
      "[276/700] loss=0.5248 | pos=0.943 neg=0.009 | val AUC=0.7411\n",
      "[277/700] loss=0.5163 | pos=0.941 neg=-0.022 | val AUC=0.7478\n",
      "[278/700] loss=0.5258 | pos=0.941 neg=0.014 | val AUC=0.7341\n",
      "[279/700] loss=0.5133 | pos=0.942 neg=-0.032 | val AUC=0.7892\n",
      "[280/700] loss=0.5138 | pos=0.942 neg=-0.030 | val AUC=0.7245\n",
      "[281/700] loss=0.5190 | pos=0.942 neg=-0.009 | val AUC=0.7510\n",
      "[282/700] loss=0.5281 | pos=0.942 neg=0.025 | val AUC=0.7765\n",
      "[283/700] loss=0.5185 | pos=0.943 neg=-0.009 | val AUC=0.7117\n",
      "[284/700] loss=0.5272 | pos=0.944 neg=0.021 | val AUC=0.7503\n",
      "[285/700] loss=0.5219 | pos=0.945 neg=0.002 | val AUC=0.7175\n",
      "[286/700] loss=0.5191 | pos=0.947 neg=-0.003 | val AUC=0.7628\n",
      "[287/700] loss=0.5203 | pos=0.945 neg=-0.006 | val AUC=0.6958\n",
      "[288/700] loss=0.5243 | pos=0.945 neg=0.009 | val AUC=0.7446\n",
      "[289/700] loss=0.5197 | pos=0.947 neg=-0.005 | val AUC=0.6655\n",
      "[290/700] loss=0.5149 | pos=0.945 neg=-0.024 | val AUC=0.7653\n",
      "[291/700] loss=0.5163 | pos=0.945 neg=-0.019 | val AUC=0.6747\n",
      "[292/700] loss=0.5276 | pos=0.946 neg=0.023 | val AUC=0.6585\n",
      "[293/700] loss=0.5200 | pos=0.945 neg=-0.008 | val AUC=0.7070\n",
      "[294/700] loss=0.5197 | pos=0.944 neg=-0.006 | val AUC=0.7031\n",
      "[295/700] loss=0.5272 | pos=0.944 neg=0.024 | val AUC=0.6709\n",
      "[296/700] loss=0.5184 | pos=0.944 neg=-0.015 | val AUC=0.7656\n",
      "[297/700] loss=0.5203 | pos=0.943 neg=-0.005 | val AUC=0.7746\n",
      "[298/700] loss=0.5139 | pos=0.943 neg=-0.033 | val AUC=0.6881\n",
      "[299/700] loss=0.5321 | pos=0.942 neg=0.038 | val AUC=0.7615\n",
      "[300/700] loss=0.5146 | pos=0.941 neg=-0.027 | val AUC=0.7755\n",
      "[301/700] loss=0.5221 | pos=0.940 neg=0.004 | val AUC=0.7510\n",
      "[302/700] loss=0.5226 | pos=0.939 neg=0.002 | val AUC=0.7468\n",
      "[303/700] loss=0.5243 | pos=0.940 neg=0.013 | val AUC=0.7991\n",
      "[304/700] loss=0.5188 | pos=0.941 neg=-0.008 | val AUC=0.7347\n",
      "[305/700] loss=0.5183 | pos=0.942 neg=-0.010 | val AUC=0.7726\n",
      "[306/700] loss=0.5184 | pos=0.941 neg=-0.008 | val AUC=0.7698\n",
      "[307/700] loss=0.5087 | pos=0.942 neg=-0.044 | val AUC=0.7258\n",
      "[308/700] loss=0.5160 | pos=0.942 neg=-0.019 | val AUC=0.7465\n",
      "[309/700] loss=0.5217 | pos=0.942 neg=0.004 | val AUC=0.7975\n",
      "[310/700] loss=0.5218 | pos=0.941 neg=0.002 | val AUC=0.7500\n",
      "[311/700] loss=0.5176 | pos=0.941 neg=-0.018 | val AUC=0.7510\n",
      "[312/700] loss=0.5133 | pos=0.941 neg=-0.035 | val AUC=0.7321\n",
      "[313/700] loss=0.5138 | pos=0.944 neg=-0.030 | val AUC=0.7749\n",
      "[314/700] loss=0.5206 | pos=0.945 neg=-0.006 | val AUC=0.7006\n",
      "[315/700] loss=0.5253 | pos=0.944 neg=0.010 | val AUC=0.6776\n",
      "[316/700] loss=0.5218 | pos=0.944 neg=-0.002 | val AUC=0.7666\n",
      "[317/700] loss=0.5251 | pos=0.945 neg=0.010 | val AUC=0.6276\n",
      "[318/700] loss=0.5200 | pos=0.946 neg=-0.004 | val AUC=0.7302\n",
      "[319/700] loss=0.5199 | pos=0.945 neg=-0.010 | val AUC=0.7468\n",
      "[320/700] loss=0.5218 | pos=0.945 neg=0.001 | val AUC=0.7510\n",
      "[321/700] loss=0.5159 | pos=0.944 neg=-0.020 | val AUC=0.6368\n",
      "[322/700] loss=0.5246 | pos=0.944 neg=0.013 | val AUC=0.7191\n",
      "[323/700] loss=0.5158 | pos=0.943 neg=-0.021 | val AUC=0.7057\n",
      "[324/700] loss=0.5230 | pos=0.944 neg=0.006 | val AUC=0.7073\n",
      "[325/700] loss=0.5117 | pos=0.944 neg=-0.037 | val AUC=0.7280\n",
      "[326/700] loss=0.5252 | pos=0.944 neg=0.012 | val AUC=0.7481\n",
      "[327/700] loss=0.5143 | pos=0.945 neg=-0.027 | val AUC=0.7522\n",
      "[328/700] loss=0.5196 | pos=0.945 neg=-0.006 | val AUC=0.7832\n",
      "[329/700] loss=0.5290 | pos=0.945 neg=0.031 | val AUC=0.7369\n",
      "[330/700] loss=0.5161 | pos=0.945 neg=-0.019 | val AUC=0.7634\n",
      "[331/700] loss=0.5149 | pos=0.945 neg=-0.022 | val AUC=0.7411\n",
      "[332/700] loss=0.5234 | pos=0.945 neg=0.012 | val AUC=0.7736\n",
      "[333/700] loss=0.5137 | pos=0.946 neg=-0.029 | val AUC=0.7462\n",
      "[334/700] loss=0.5159 | pos=0.944 neg=-0.016 | val AUC=0.7503\n",
      "[335/700] loss=0.5278 | pos=0.945 neg=0.027 | val AUC=0.7819\n",
      "[336/700] loss=0.5189 | pos=0.945 neg=-0.009 | val AUC=0.7296\n",
      "[337/700] loss=0.5199 | pos=0.945 neg=-0.002 | val AUC=0.7283\n",
      "[338/700] loss=0.5208 | pos=0.947 neg=-0.000 | val AUC=0.7465\n",
      "[339/700] loss=0.5148 | pos=0.946 neg=-0.022 | val AUC=0.7254\n",
      "[340/700] loss=0.5156 | pos=0.947 neg=-0.017 | val AUC=0.6629\n",
      "[341/700] loss=0.5201 | pos=0.948 neg=-0.003 | val AUC=0.7781\n",
      "[342/700] loss=0.5212 | pos=0.948 neg=0.000 | val AUC=0.7921\n",
      "[343/700] loss=0.5176 | pos=0.948 neg=-0.014 | val AUC=0.7143\n",
      "[344/700] loss=0.5253 | pos=0.948 neg=0.021 | val AUC=0.7203\n",
      "[345/700] loss=0.5195 | pos=0.947 neg=-0.004 | val AUC=0.7242\n",
      "[346/700] loss=0.5229 | pos=0.947 neg=0.008 | val AUC=0.7382\n",
      "[347/700] loss=0.5178 | pos=0.944 neg=-0.010 | val AUC=0.7573\n",
      "[348/700] loss=0.5144 | pos=0.943 neg=-0.025 | val AUC=0.7538\n",
      "[349/700] loss=0.5172 | pos=0.942 neg=-0.015 | val AUC=0.7592\n",
      "[350/700] loss=0.5222 | pos=0.942 neg=0.003 | val AUC=0.7500\n",
      "[351/700] loss=0.5130 | pos=0.945 neg=-0.027 | val AUC=0.7487\n",
      "[352/700] loss=0.5206 | pos=0.946 neg=-0.000 | val AUC=0.7121\n",
      "[353/700] loss=0.5181 | pos=0.948 neg=-0.010 | val AUC=0.7191\n",
      "[354/700] loss=0.5075 | pos=0.946 neg=-0.048 | val AUC=0.7315\n",
      "[355/700] loss=0.5197 | pos=0.945 neg=-0.002 | val AUC=0.8042\n",
      "[356/700] loss=0.5223 | pos=0.946 neg=0.007 | val AUC=0.8457\n",
      "[357/700] loss=0.5180 | pos=0.947 neg=-0.008 | val AUC=0.7953\n",
      "[358/700] loss=0.5227 | pos=0.947 neg=0.006 | val AUC=0.7465\n",
      "[359/700] loss=0.5188 | pos=0.948 neg=-0.008 | val AUC=0.7876\n",
      "[360/700] loss=0.5222 | pos=0.948 neg=0.004 | val AUC=0.7615\n",
      "[361/700] loss=0.5185 | pos=0.948 neg=-0.012 | val AUC=0.7857\n",
      "[362/700] loss=0.5176 | pos=0.947 neg=-0.011 | val AUC=0.7357\n",
      "[363/700] loss=0.5245 | pos=0.946 neg=0.014 | val AUC=0.8001\n",
      "[364/700] loss=0.5173 | pos=0.947 neg=-0.016 | val AUC=0.7781\n",
      "[365/700] loss=0.5147 | pos=0.946 neg=-0.025 | val AUC=0.7557\n",
      "[366/700] loss=0.5158 | pos=0.947 neg=-0.020 | val AUC=0.7302\n",
      "[367/700] loss=0.5213 | pos=0.947 neg=0.003 | val AUC=0.7545\n",
      "[368/700] loss=0.5196 | pos=0.946 neg=-0.006 | val AUC=0.8284\n",
      "[369/700] loss=0.5217 | pos=0.945 neg=-0.000 | val AUC=0.7207\n",
      "[370/700] loss=0.5234 | pos=0.945 neg=0.011 | val AUC=0.7474\n",
      "[371/700] loss=0.5187 | pos=0.943 neg=-0.006 | val AUC=0.7589\n",
      "[372/700] loss=0.5195 | pos=0.942 neg=-0.007 | val AUC=0.7433\n",
      "[373/700] loss=0.5152 | pos=0.944 neg=-0.021 | val AUC=0.7455\n",
      "[374/700] loss=0.5167 | pos=0.943 neg=-0.016 | val AUC=0.7768\n",
      "[375/700] loss=0.5190 | pos=0.944 neg=-0.007 | val AUC=0.7376\n",
      "[376/700] loss=0.5229 | pos=0.942 neg=0.009 | val AUC=0.7640\n",
      "[377/700] loss=0.5211 | pos=0.942 neg=0.000 | val AUC=0.7768\n",
      "[378/700] loss=0.5155 | pos=0.941 neg=-0.022 | val AUC=0.7012\n",
      "[379/700] loss=0.5251 | pos=0.942 neg=0.013 | val AUC=0.7465\n",
      "[380/700] loss=0.5240 | pos=0.940 neg=0.007 | val AUC=0.7296\n",
      "[381/700] loss=0.5206 | pos=0.941 neg=-0.003 | val AUC=0.7347\n",
      "[382/700] loss=0.5116 | pos=0.943 neg=-0.036 | val AUC=0.7165\n",
      "[383/700] loss=0.5206 | pos=0.943 neg=-0.001 | val AUC=0.6936\n",
      "[384/700] loss=0.5200 | pos=0.946 neg=-0.003 | val AUC=0.7650\n",
      "[385/700] loss=0.5219 | pos=0.947 neg=0.002 | val AUC=0.7232\n",
      "[386/700] loss=0.5187 | pos=0.945 neg=-0.012 | val AUC=0.7385\n",
      "[387/700] loss=0.5208 | pos=0.947 neg=-0.003 | val AUC=0.7666\n",
      "[388/700] loss=0.5216 | pos=0.946 neg=0.000 | val AUC=0.7089\n",
      "[389/700] loss=0.5207 | pos=0.944 neg=-0.003 | val AUC=0.7720\n",
      "[390/700] loss=0.5233 | pos=0.945 neg=0.009 | val AUC=0.7366\n",
      "[391/700] loss=0.5198 | pos=0.945 neg=-0.000 | val AUC=0.7267\n",
      "[392/700] loss=0.5164 | pos=0.943 neg=-0.016 | val AUC=0.7411\n",
      "[393/700] loss=0.5246 | pos=0.943 neg=0.015 | val AUC=0.7787\n",
      "[394/700] loss=0.5238 | pos=0.941 neg=0.012 | val AUC=0.7946\n",
      "[395/700] loss=0.5211 | pos=0.939 neg=0.001 | val AUC=0.7663\n",
      "[396/700] loss=0.5165 | pos=0.938 neg=-0.019 | val AUC=0.7637\n",
      "[397/700] loss=0.5203 | pos=0.938 neg=-0.003 | val AUC=0.7637\n",
      "[398/700] loss=0.5172 | pos=0.936 neg=-0.020 | val AUC=0.7433\n",
      "[399/700] loss=0.5203 | pos=0.937 neg=-0.008 | val AUC=0.7235\n",
      "[400/700] loss=0.5173 | pos=0.937 neg=-0.018 | val AUC=0.8001\n",
      "[401/700] loss=0.5254 | pos=0.939 neg=0.017 | val AUC=0.7449\n",
      "[402/700] loss=0.5342 | pos=0.939 neg=0.051 | val AUC=0.7015\n",
      "[403/700] loss=0.5096 | pos=0.939 neg=-0.046 | val AUC=0.7430\n",
      "[404/700] loss=0.5197 | pos=0.940 neg=-0.004 | val AUC=0.7022\n",
      "[405/700] loss=0.5246 | pos=0.940 neg=0.014 | val AUC=0.7286\n",
      "[406/700] loss=0.5260 | pos=0.941 neg=0.017 | val AUC=0.7073\n",
      "[407/700] loss=0.5149 | pos=0.941 neg=-0.025 | val AUC=0.7529\n",
      "[408/700] loss=0.5181 | pos=0.941 neg=-0.014 | val AUC=0.7781\n",
      "[409/700] loss=0.5127 | pos=0.942 neg=-0.032 | val AUC=0.7290\n",
      "[410/700] loss=0.5187 | pos=0.944 neg=-0.007 | val AUC=0.7098\n",
      "[411/700] loss=0.5117 | pos=0.944 neg=-0.035 | val AUC=0.6620\n",
      "[412/700] loss=0.5151 | pos=0.945 neg=-0.022 | val AUC=0.6977\n",
      "[413/700] loss=0.5228 | pos=0.945 neg=0.005 | val AUC=0.6738\n",
      "[414/700] loss=0.5210 | pos=0.946 neg=-0.001 | val AUC=0.7044\n",
      "[415/700] loss=0.5177 | pos=0.947 neg=-0.015 | val AUC=0.7784\n",
      "[416/700] loss=0.5177 | pos=0.946 neg=-0.015 | val AUC=0.7302\n",
      "[417/700] loss=0.5225 | pos=0.946 neg=0.002 | val AUC=0.6617\n",
      "[418/700] loss=0.5117 | pos=0.945 neg=-0.041 | val AUC=0.7245\n",
      "[419/700] loss=0.5283 | pos=0.946 neg=0.021 | val AUC=0.7223\n",
      "[420/700] loss=0.5260 | pos=0.946 neg=0.016 | val AUC=0.7140\n",
      "[421/700] loss=0.5267 | pos=0.945 neg=0.019 | val AUC=0.7318\n",
      "[422/700] loss=0.5158 | pos=0.943 neg=-0.022 | val AUC=0.7034\n",
      "[423/700] loss=0.5146 | pos=0.942 neg=-0.028 | val AUC=0.6983\n",
      "[424/700] loss=0.5139 | pos=0.939 neg=-0.034 | val AUC=0.6661\n",
      "[425/700] loss=0.5136 | pos=0.941 neg=-0.036 | val AUC=0.7076\n",
      "[426/700] loss=0.5194 | pos=0.943 neg=-0.012 | val AUC=0.6904\n",
      "[427/700] loss=0.5240 | pos=0.941 neg=0.004 | val AUC=0.7347\n",
      "[428/700] loss=0.5212 | pos=0.942 neg=-0.007 | val AUC=0.6661\n",
      "[429/700] loss=0.5211 | pos=0.940 neg=-0.008 | val AUC=0.7066\n",
      "[430/700] loss=0.5207 | pos=0.939 neg=-0.008 | val AUC=0.7127\n",
      "[431/700] loss=0.5201 | pos=0.937 neg=-0.008 | val AUC=0.7149\n",
      "[432/700] loss=0.5148 | pos=0.937 neg=-0.034 | val AUC=0.6901\n",
      "[433/700] loss=0.5201 | pos=0.939 neg=-0.007 | val AUC=0.6945\n",
      "[434/700] loss=0.5172 | pos=0.939 neg=-0.018 | val AUC=0.6457\n",
      "[435/700] loss=0.5272 | pos=0.939 neg=0.022 | val AUC=0.7328\n",
      "[436/700] loss=0.5284 | pos=0.941 neg=0.026 | val AUC=0.6537\n",
      "[437/700] loss=0.5248 | pos=0.941 neg=0.011 | val AUC=0.7184\n",
      "[438/700] loss=0.5247 | pos=0.939 neg=0.011 | val AUC=0.6865\n",
      "[439/700] loss=0.5213 | pos=0.940 neg=0.001 | val AUC=0.7714\n",
      "[440/700] loss=0.5129 | pos=0.939 neg=-0.031 | val AUC=0.7490\n",
      "[441/700] loss=0.5185 | pos=0.938 neg=-0.010 | val AUC=0.6818\n",
      "[442/700] loss=0.5181 | pos=0.939 neg=-0.009 | val AUC=0.7500\n",
      "[443/700] loss=0.5181 | pos=0.940 neg=-0.009 | val AUC=0.7516\n",
      "[444/700] loss=0.5153 | pos=0.939 neg=-0.020 | val AUC=0.6757\n",
      "[445/700] loss=0.5173 | pos=0.938 neg=-0.017 | val AUC=0.7063\n",
      "[446/700] loss=0.5238 | pos=0.938 neg=0.008 | val AUC=0.7207\n",
      "[447/700] loss=0.5181 | pos=0.939 neg=-0.015 | val AUC=0.6872\n",
      "[448/700] loss=0.5182 | pos=0.942 neg=-0.010 | val AUC=0.7331\n",
      "[449/700] loss=0.5229 | pos=0.943 neg=0.007 | val AUC=0.7121\n",
      "[450/700] loss=0.5229 | pos=0.945 neg=0.010 | val AUC=0.7050\n",
      "[451/700] loss=0.5223 | pos=0.947 neg=0.000 | val AUC=0.7749\n",
      "[452/700] loss=0.5188 | pos=0.945 neg=-0.017 | val AUC=0.6629\n",
      "[453/700] loss=0.5319 | pos=0.946 neg=0.037 | val AUC=0.7573\n",
      "[454/700] loss=0.5171 | pos=0.946 neg=-0.019 | val AUC=0.7427\n",
      "[455/700] loss=0.5184 | pos=0.946 neg=-0.014 | val AUC=0.7146\n",
      "[456/700] loss=0.5252 | pos=0.946 neg=0.016 | val AUC=0.7736\n",
      "[457/700] loss=0.5215 | pos=0.945 neg=-0.004 | val AUC=0.7943\n",
      "[458/700] loss=0.5176 | pos=0.946 neg=-0.016 | val AUC=0.7749\n",
      "[459/700] loss=0.5226 | pos=0.946 neg=0.005 | val AUC=0.6952\n",
      "[460/700] loss=0.5226 | pos=0.946 neg=0.004 | val AUC=0.6974\n",
      "[461/700] loss=0.5252 | pos=0.948 neg=0.020 | val AUC=0.7353\n",
      "[462/700] loss=0.5204 | pos=0.946 neg=-0.001 | val AUC=0.7334\n",
      "[463/700] loss=0.5109 | pos=0.946 neg=-0.036 | val AUC=0.7516\n",
      "[464/700] loss=0.5162 | pos=0.946 neg=-0.016 | val AUC=0.7130\n",
      "[465/700] loss=0.5126 | pos=0.946 neg=-0.030 | val AUC=0.6993\n",
      "[466/700] loss=0.5193 | pos=0.946 neg=-0.009 | val AUC=0.6945\n",
      "[467/700] loss=0.5195 | pos=0.946 neg=-0.002 | val AUC=0.6929\n",
      "[468/700] loss=0.5213 | pos=0.945 neg=0.002 | val AUC=0.7526\n",
      "[469/700] loss=0.5248 | pos=0.946 neg=0.013 | val AUC=0.7449\n",
      "[470/700] loss=0.5176 | pos=0.945 neg=-0.013 | val AUC=0.7500\n",
      "[471/700] loss=0.5169 | pos=0.944 neg=-0.015 | val AUC=0.7427\n",
      "[472/700] loss=0.5252 | pos=0.945 neg=0.019 | val AUC=0.7516\n",
      "[473/700] loss=0.5141 | pos=0.944 neg=-0.028 | val AUC=0.7184\n",
      "[474/700] loss=0.5200 | pos=0.945 neg=-0.003 | val AUC=0.6977\n",
      "[475/700] loss=0.5198 | pos=0.945 neg=-0.005 | val AUC=0.7679\n",
      "[476/700] loss=0.5277 | pos=0.945 neg=0.027 | val AUC=0.7790\n",
      "[477/700] loss=0.5294 | pos=0.945 neg=0.030 | val AUC=0.6665\n",
      "[478/700] loss=0.5211 | pos=0.943 neg=0.001 | val AUC=0.6735\n",
      "[479/700] loss=0.5230 | pos=0.943 neg=0.009 | val AUC=0.7465\n",
      "[480/700] loss=0.5191 | pos=0.942 neg=-0.003 | val AUC=0.7710\n",
      "[481/700] loss=0.5179 | pos=0.942 neg=-0.012 | val AUC=0.7430\n",
      "[482/700] loss=0.5187 | pos=0.943 neg=-0.007 | val AUC=0.7465\n",
      "[483/700] loss=0.5222 | pos=0.943 neg=0.001 | val AUC=0.7328\n",
      "[484/700] loss=0.5180 | pos=0.946 neg=-0.011 | val AUC=0.7022\n",
      "[485/700] loss=0.5295 | pos=0.945 neg=0.027 | val AUC=0.7302\n",
      "[486/700] loss=0.5149 | pos=0.945 neg=-0.026 | val AUC=0.6722\n",
      "[487/700] loss=0.5250 | pos=0.944 neg=0.014 | val AUC=0.6722\n",
      "[488/700] loss=0.5219 | pos=0.944 neg=0.002 | val AUC=0.6929\n",
      "[489/700] loss=0.5171 | pos=0.944 neg=-0.013 | val AUC=0.6964\n",
      "[490/700] loss=0.5195 | pos=0.944 neg=-0.010 | val AUC=0.6744\n",
      "[491/700] loss=0.5134 | pos=0.944 neg=-0.027 | val AUC=0.7494\n",
      "[492/700] loss=0.5208 | pos=0.944 neg=-0.001 | val AUC=0.7050\n",
      "[493/700] loss=0.5203 | pos=0.945 neg=-0.005 | val AUC=0.7267\n",
      "[494/700] loss=0.5198 | pos=0.945 neg=-0.000 | val AUC=0.7478\n",
      "[495/700] loss=0.5271 | pos=0.946 neg=0.026 | val AUC=0.6575\n",
      "[496/700] loss=0.5143 | pos=0.946 neg=-0.027 | val AUC=0.6862\n",
      "[497/700] loss=0.5272 | pos=0.944 neg=0.021 | val AUC=0.7105\n",
      "[498/700] loss=0.5181 | pos=0.944 neg=-0.011 | val AUC=0.7430\n",
      "[499/700] loss=0.5164 | pos=0.944 neg=-0.020 | val AUC=0.7293\n",
      "[500/700] loss=0.5212 | pos=0.945 neg=0.001 | val AUC=0.7353\n",
      "[501/700] loss=0.5216 | pos=0.946 neg=0.002 | val AUC=0.7022\n",
      "[502/700] loss=0.5210 | pos=0.946 neg=-0.002 | val AUC=0.7577\n",
      "[503/700] loss=0.5224 | pos=0.946 neg=0.004 | val AUC=0.7296\n",
      "[504/700] loss=0.5201 | pos=0.946 neg=-0.002 | val AUC=0.7790\n",
      "[505/700] loss=0.5196 | pos=0.946 neg=-0.005 | val AUC=0.7114\n",
      "[506/700] loss=0.5220 | pos=0.944 neg=0.005 | val AUC=0.6999\n",
      "[507/700] loss=0.5158 | pos=0.945 neg=-0.019 | val AUC=0.6856\n",
      "[508/700] loss=0.5172 | pos=0.944 neg=-0.011 | val AUC=0.7121\n",
      "[509/700] loss=0.5223 | pos=0.943 neg=0.005 | val AUC=0.7608\n",
      "[510/700] loss=0.5267 | pos=0.943 neg=0.025 | val AUC=0.6658\n",
      "[511/700] loss=0.5226 | pos=0.945 neg=0.005 | val AUC=0.6977\n",
      "[512/700] loss=0.5197 | pos=0.944 neg=-0.003 | val AUC=0.7044\n",
      "[513/700] loss=0.5140 | pos=0.946 neg=-0.028 | val AUC=0.7197\n",
      "[514/700] loss=0.5188 | pos=0.946 neg=-0.009 | val AUC=0.6891\n",
      "[515/700] loss=0.5188 | pos=0.945 neg=-0.007 | val AUC=0.7018\n",
      "[516/700] loss=0.5203 | pos=0.943 neg=0.000 | val AUC=0.6980\n",
      "[517/700] loss=0.5189 | pos=0.943 neg=-0.011 | val AUC=0.6747\n",
      "[518/700] loss=0.5200 | pos=0.942 neg=-0.009 | val AUC=0.7047\n",
      "[519/700] loss=0.5152 | pos=0.941 neg=-0.022 | val AUC=0.7258\n",
      "[520/700] loss=0.5146 | pos=0.941 neg=-0.027 | val AUC=0.7044\n",
      "[521/700] loss=0.5204 | pos=0.942 neg=-0.006 | val AUC=0.6942\n",
      "[522/700] loss=0.5136 | pos=0.942 neg=-0.026 | val AUC=0.7028\n",
      "[523/700] loss=0.5141 | pos=0.940 neg=-0.026 | val AUC=0.7089\n",
      "[524/700] loss=0.5175 | pos=0.941 neg=-0.012 | val AUC=0.6974\n",
      "[525/700] loss=0.5202 | pos=0.942 neg=-0.004 | val AUC=0.7529\n",
      "[526/700] loss=0.5159 | pos=0.940 neg=-0.018 | val AUC=0.7044\n",
      "[527/700] loss=0.5202 | pos=0.941 neg=-0.001 | val AUC=0.7197\n",
      "[528/700] loss=0.5213 | pos=0.939 neg=0.001 | val AUC=0.7296\n",
      "[529/700] loss=0.5216 | pos=0.940 neg=0.003 | val AUC=0.7573\n",
      "[530/700] loss=0.5168 | pos=0.942 neg=-0.015 | val AUC=0.7341\n",
      "[531/700] loss=0.5189 | pos=0.943 neg=-0.007 | val AUC=0.7299\n",
      "[532/700] loss=0.5218 | pos=0.944 neg=0.001 | val AUC=0.7127\n",
      "[533/700] loss=0.5200 | pos=0.944 neg=-0.004 | val AUC=0.7111\n",
      "[534/700] loss=0.5148 | pos=0.946 neg=-0.020 | val AUC=0.6923\n",
      "[535/700] loss=0.5220 | pos=0.946 neg=0.003 | val AUC=0.7404\n",
      "[536/700] loss=0.5257 | pos=0.946 neg=0.016 | val AUC=0.7133\n",
      "[537/700] loss=0.5201 | pos=0.947 neg=-0.005 | val AUC=0.7274\n",
      "[538/700] loss=0.5155 | pos=0.946 neg=-0.021 | val AUC=0.7592\n",
      "[539/700] loss=0.5288 | pos=0.945 neg=0.031 | val AUC=0.7018\n",
      "[540/700] loss=0.5223 | pos=0.943 neg=0.004 | val AUC=0.7714\n",
      "[541/700] loss=0.5223 | pos=0.942 neg=0.005 | val AUC=0.7372\n",
      "[542/700] loss=0.5258 | pos=0.942 neg=0.017 | val AUC=0.7041\n",
      "[543/700] loss=0.5180 | pos=0.942 neg=-0.008 | val AUC=0.7698\n",
      "[544/700] loss=0.5137 | pos=0.943 neg=-0.026 | val AUC=0.7465\n",
      "[545/700] loss=0.5206 | pos=0.943 neg=-0.002 | val AUC=0.7305\n",
      "[546/700] loss=0.5165 | pos=0.944 neg=-0.016 | val AUC=0.7404\n",
      "[547/700] loss=0.5259 | pos=0.943 neg=0.019 | val AUC=0.7066\n",
      "[548/700] loss=0.5177 | pos=0.942 neg=-0.014 | val AUC=0.7105\n",
      "[549/700] loss=0.5198 | pos=0.941 neg=-0.005 | val AUC=0.7229\n",
      "[550/700] loss=0.5120 | pos=0.940 neg=-0.036 | val AUC=0.7758\n",
      "[551/700] loss=0.5239 | pos=0.938 neg=0.009 | val AUC=0.7570\n",
      "[552/700] loss=0.5252 | pos=0.939 neg=0.016 | val AUC=0.7092\n",
      "[553/700] loss=0.5195 | pos=0.941 neg=-0.006 | val AUC=0.7656\n",
      "[554/700] loss=0.5185 | pos=0.940 neg=-0.006 | val AUC=0.7628\n",
      "[555/700] loss=0.5190 | pos=0.940 neg=-0.005 | val AUC=0.7749\n",
      "[556/700] loss=0.5227 | pos=0.941 neg=0.009 | val AUC=0.7768\n",
      "[557/700] loss=0.5178 | pos=0.942 neg=-0.008 | val AUC=0.7777\n",
      "[558/700] loss=0.5214 | pos=0.941 neg=0.006 | val AUC=0.7857\n",
      "[559/700] loss=0.5242 | pos=0.942 neg=0.014 | val AUC=0.7915\n",
      "[560/700] loss=0.5188 | pos=0.941 neg=-0.007 | val AUC=0.7478\n",
      "[561/700] loss=0.5191 | pos=0.942 neg=-0.009 | val AUC=0.7449\n",
      "[562/700] loss=0.5297 | pos=0.942 neg=0.033 | val AUC=0.7800\n",
      "[563/700] loss=0.5186 | pos=0.942 neg=-0.006 | val AUC=0.7427\n",
      "[564/700] loss=0.5190 | pos=0.941 neg=-0.010 | val AUC=0.7073\n",
      "[565/700] loss=0.5218 | pos=0.942 neg=0.004 | val AUC=0.7497\n",
      "[566/700] loss=0.5195 | pos=0.943 neg=-0.006 | val AUC=0.7618\n",
      "[567/700] loss=0.5202 | pos=0.943 neg=-0.008 | val AUC=0.7334\n",
      "[568/700] loss=0.5169 | pos=0.943 neg=-0.017 | val AUC=0.6865\n",
      "[569/700] loss=0.5153 | pos=0.944 neg=-0.018 | val AUC=0.7497\n",
      "[570/700] loss=0.5155 | pos=0.944 neg=-0.024 | val AUC=0.6814\n",
      "[571/700] loss=0.5213 | pos=0.944 neg=0.002 | val AUC=0.7500\n",
      "[572/700] loss=0.5234 | pos=0.944 neg=0.011 | val AUC=0.6543\n",
      "[573/700] loss=0.5141 | pos=0.944 neg=-0.029 | val AUC=0.7136\n",
      "[574/700] loss=0.5214 | pos=0.945 neg=-0.001 | val AUC=0.7286\n",
      "[575/700] loss=0.5178 | pos=0.944 neg=-0.009 | val AUC=0.7519\n",
      "[576/700] loss=0.5194 | pos=0.946 neg=-0.002 | val AUC=0.6955\n",
      "[577/700] loss=0.5134 | pos=0.947 neg=-0.027 | val AUC=0.7130\n",
      "[578/700] loss=0.5217 | pos=0.946 neg=0.004 | val AUC=0.7089\n",
      "[579/700] loss=0.5161 | pos=0.946 neg=-0.018 | val AUC=0.7474\n",
      "[580/700] loss=0.5188 | pos=0.946 neg=-0.008 | val AUC=0.7494\n",
      "[581/700] loss=0.5119 | pos=0.944 neg=-0.036 | val AUC=0.7197\n",
      "[582/700] loss=0.5183 | pos=0.942 neg=-0.012 | val AUC=0.8023\n",
      "[583/700] loss=0.5192 | pos=0.944 neg=-0.007 | val AUC=0.7411\n",
      "[584/700] loss=0.5158 | pos=0.943 neg=-0.024 | val AUC=0.7449\n",
      "[585/700] loss=0.5169 | pos=0.944 neg=-0.017 | val AUC=0.7551\n",
      "[586/700] loss=0.5171 | pos=0.946 neg=-0.011 | val AUC=0.7207\n",
      "[587/700] loss=0.5155 | pos=0.947 neg=-0.017 | val AUC=0.7369\n",
      "[588/700] loss=0.5215 | pos=0.947 neg=-0.003 | val AUC=0.7838\n",
      "[589/700] loss=0.5164 | pos=0.948 neg=-0.018 | val AUC=0.7200\n",
      "[590/700] loss=0.5210 | pos=0.946 neg=-0.002 | val AUC=0.7436\n",
      "[591/700] loss=0.5178 | pos=0.946 neg=-0.012 | val AUC=0.7532\n",
      "[592/700] loss=0.5137 | pos=0.946 neg=-0.028 | val AUC=0.7497\n",
      "[593/700] loss=0.5314 | pos=0.945 neg=0.040 | val AUC=0.7908\n",
      "[594/700] loss=0.5172 | pos=0.945 neg=-0.015 | val AUC=0.7545\n",
      "[595/700] loss=0.5209 | pos=0.946 neg=-0.001 | val AUC=0.7586\n",
      "[596/700] loss=0.5153 | pos=0.946 neg=-0.026 | val AUC=0.7647\n",
      "[597/700] loss=0.5152 | pos=0.948 neg=-0.023 | val AUC=0.7484\n",
      "[598/700] loss=0.5184 | pos=0.947 neg=-0.014 | val AUC=0.7736\n",
      "[599/700] loss=0.5308 | pos=0.947 neg=0.037 | val AUC=0.7605\n",
      "[600/700] loss=0.5286 | pos=0.947 neg=0.031 | val AUC=0.7765\n",
      "[601/700] loss=0.5184 | pos=0.947 neg=-0.006 | val AUC=0.7851\n",
      "[602/700] loss=0.5242 | pos=0.947 neg=0.014 | val AUC=0.8001\n",
      "[603/700] loss=0.5161 | pos=0.945 neg=-0.018 | val AUC=0.7739\n",
      "[604/700] loss=0.5223 | pos=0.946 neg=0.008 | val AUC=0.8345\n",
      "[605/700] loss=0.5221 | pos=0.944 neg=0.001 | val AUC=0.7331\n",
      "[606/700] loss=0.5218 | pos=0.943 neg=0.001 | val AUC=0.7392\n",
      "[607/700] loss=0.5234 | pos=0.942 neg=0.008 | val AUC=0.7714\n",
      "[608/700] loss=0.5140 | pos=0.940 neg=-0.030 | val AUC=0.7044\n",
      "[609/700] loss=0.5270 | pos=0.941 neg=0.024 | val AUC=0.8055\n",
      "[610/700] loss=0.5241 | pos=0.941 neg=0.012 | val AUC=0.7726\n",
      "[611/700] loss=0.5172 | pos=0.938 neg=-0.016 | val AUC=0.7991\n",
      "[612/700] loss=0.5194 | pos=0.936 neg=-0.009 | val AUC=0.7816\n",
      "[613/700] loss=0.5173 | pos=0.937 neg=-0.015 | val AUC=0.8033\n",
      "[614/700] loss=0.5235 | pos=0.935 neg=0.008 | val AUC=0.8591\n",
      "[615/700] loss=0.5278 | pos=0.937 neg=0.024 | val AUC=0.8128\n",
      "[616/700] loss=0.5202 | pos=0.938 neg=-0.007 | val AUC=0.8055\n",
      "[617/700] loss=0.5222 | pos=0.939 neg=0.000 | val AUC=0.8195\n",
      "[618/700] loss=0.5198 | pos=0.940 neg=-0.009 | val AUC=0.8393\n",
      "[619/700] loss=0.5180 | pos=0.941 neg=-0.015 | val AUC=0.8536\n",
      "[620/700] loss=0.5154 | pos=0.942 neg=-0.020 | val AUC=0.8307\n",
      "[621/700] loss=0.5201 | pos=0.943 neg=-0.008 | val AUC=0.8310\n",
      "[622/700] loss=0.5195 | pos=0.944 neg=-0.005 | val AUC=0.8186\n",
      "[623/700] loss=0.5182 | pos=0.943 neg=-0.009 | val AUC=0.8125\n",
      "[624/700] loss=0.5176 | pos=0.943 neg=-0.015 | val AUC=0.8320\n",
      "[625/700] loss=0.5221 | pos=0.943 neg=0.005 | val AUC=0.7522\n",
      "[626/700] loss=0.5212 | pos=0.945 neg=0.001 | val AUC=0.7946\n",
      "[627/700] loss=0.5209 | pos=0.946 neg=0.002 | val AUC=0.8179\n",
      "[628/700] loss=0.5198 | pos=0.947 neg=-0.003 | val AUC=0.8154\n",
      "[629/700] loss=0.5196 | pos=0.947 neg=-0.006 | val AUC=0.8358\n",
      "[630/700] loss=0.5165 | pos=0.948 neg=-0.012 | val AUC=0.8125\n",
      "[631/700] loss=0.5273 | pos=0.947 neg=0.027 | val AUC=0.7761\n",
      "[632/700] loss=0.5186 | pos=0.947 neg=-0.009 | val AUC=0.7707\n",
      "[633/700] loss=0.5161 | pos=0.948 neg=-0.016 | val AUC=0.7388\n",
      "[634/700] loss=0.5157 | pos=0.947 neg=-0.017 | val AUC=0.7803\n",
      "[635/700] loss=0.5103 | pos=0.947 neg=-0.043 | val AUC=0.8189\n",
      "[636/700] loss=0.5298 | pos=0.948 neg=0.036 | val AUC=0.8307\n",
      "[637/700] loss=0.5245 | pos=0.947 neg=0.013 | val AUC=0.7854\n",
      "[638/700] loss=0.5140 | pos=0.945 neg=-0.028 | val AUC=0.8406\n",
      "[639/700] loss=0.5165 | pos=0.947 neg=-0.015 | val AUC=0.7216\n",
      "[640/700] loss=0.5193 | pos=0.947 neg=-0.006 | val AUC=0.7344\n",
      "[641/700] loss=0.5228 | pos=0.948 neg=0.010 | val AUC=0.7666\n",
      "[642/700] loss=0.5167 | pos=0.948 neg=-0.016 | val AUC=0.7596\n",
      "[643/700] loss=0.5215 | pos=0.948 neg=0.002 | val AUC=0.7589\n",
      "[644/700] loss=0.5147 | pos=0.947 neg=-0.023 | val AUC=0.8460\n",
      "[645/700] loss=0.5195 | pos=0.947 neg=-0.006 | val AUC=0.8371\n",
      "[646/700] loss=0.5244 | pos=0.945 neg=0.012 | val AUC=0.7366\n",
      "[647/700] loss=0.5138 | pos=0.946 neg=-0.028 | val AUC=0.7707\n",
      "[648/700] loss=0.5214 | pos=0.947 neg=0.002 | val AUC=0.7455\n",
      "[649/700] loss=0.5240 | pos=0.948 neg=0.013 | val AUC=0.7736\n",
      "[650/700] loss=0.5189 | pos=0.949 neg=-0.006 | val AUC=0.8536\n",
      "[651/700] loss=0.5237 | pos=0.949 neg=0.014 | val AUC=0.8026\n",
      "[652/700] loss=0.5231 | pos=0.948 neg=0.009 | val AUC=0.7978\n",
      "[653/700] loss=0.5215 | pos=0.948 neg=0.002 | val AUC=0.7685\n",
      "[654/700] loss=0.5255 | pos=0.946 neg=0.015 | val AUC=0.6553\n",
      "[655/700] loss=0.5186 | pos=0.948 neg=-0.007 | val AUC=0.7465\n",
      "[656/700] loss=0.5170 | pos=0.948 neg=-0.016 | val AUC=0.8026\n",
      "[657/700] loss=0.5185 | pos=0.948 neg=-0.008 | val AUC=0.7500\n",
      "[658/700] loss=0.5194 | pos=0.947 neg=-0.003 | val AUC=0.8192\n",
      "[659/700] loss=0.5143 | pos=0.944 neg=-0.027 | val AUC=0.8007\n",
      "[660/700] loss=0.5205 | pos=0.944 neg=-0.004 | val AUC=0.7417\n",
      "[661/700] loss=0.5199 | pos=0.943 neg=-0.003 | val AUC=0.8262\n",
      "[662/700] loss=0.5232 | pos=0.943 neg=0.008 | val AUC=0.7921\n",
      "[663/700] loss=0.5238 | pos=0.943 neg=0.010 | val AUC=0.7398\n",
      "[664/700] loss=0.5142 | pos=0.943 neg=-0.028 | val AUC=0.8173\n",
      "[665/700] loss=0.5188 | pos=0.943 neg=-0.009 | val AUC=0.7774\n",
      "[666/700] loss=0.5189 | pos=0.942 neg=-0.007 | val AUC=0.8195\n",
      "[667/700] loss=0.5143 | pos=0.942 neg=-0.023 | val AUC=0.8151\n",
      "[668/700] loss=0.5130 | pos=0.943 neg=-0.033 | val AUC=0.7816\n",
      "[669/700] loss=0.5174 | pos=0.941 neg=-0.014 | val AUC=0.7608\n",
      "[670/700] loss=0.5172 | pos=0.943 neg=-0.012 | val AUC=0.7793\n",
      "[671/700] loss=0.5248 | pos=0.944 neg=0.014 | val AUC=0.8036\n",
      "[672/700] loss=0.5208 | pos=0.944 neg=0.000 | val AUC=0.7720\n",
      "[673/700] loss=0.5256 | pos=0.944 neg=0.020 | val AUC=0.7723\n",
      "[674/700] loss=0.5173 | pos=0.944 neg=-0.012 | val AUC=0.7653\n",
      "[675/700] loss=0.5224 | pos=0.943 neg=0.005 | val AUC=0.7580\n",
      "[676/700] loss=0.5228 | pos=0.944 neg=0.003 | val AUC=0.8084\n",
      "[677/700] loss=0.5267 | pos=0.943 neg=0.019 | val AUC=0.7710\n",
      "[678/700] loss=0.5187 | pos=0.945 neg=-0.010 | val AUC=0.8071\n",
      "[679/700] loss=0.5201 | pos=0.946 neg=-0.004 | val AUC=0.6942\n",
      "[680/700] loss=0.5217 | pos=0.944 neg=-0.000 | val AUC=0.7643\n",
      "[681/700] loss=0.5114 | pos=0.942 neg=-0.038 | val AUC=0.8237\n",
      "[682/700] loss=0.5201 | pos=0.940 neg=-0.002 | val AUC=0.7446\n",
      "[683/700] loss=0.5194 | pos=0.940 neg=-0.004 | val AUC=0.7350\n",
      "[684/700] loss=0.5131 | pos=0.941 neg=-0.031 | val AUC=0.7561\n",
      "[685/700] loss=0.5187 | pos=0.941 neg=-0.008 | val AUC=0.7596\n",
      "[686/700] loss=0.5138 | pos=0.942 neg=-0.027 | val AUC=0.7248\n",
      "[687/700] loss=0.5236 | pos=0.943 neg=0.011 | val AUC=0.7408\n",
      "[688/700] loss=0.5171 | pos=0.945 neg=-0.014 | val AUC=0.7679\n",
      "[689/700] loss=0.5149 | pos=0.944 neg=-0.025 | val AUC=0.8119\n",
      "[690/700] loss=0.5268 | pos=0.945 neg=0.020 | val AUC=0.7449\n",
      "[691/700] loss=0.5194 | pos=0.945 neg=-0.004 | val AUC=0.7707\n",
      "[692/700] loss=0.5223 | pos=0.945 neg=0.008 | val AUC=0.7768\n",
      "[693/700] loss=0.5201 | pos=0.947 neg=-0.002 | val AUC=0.7822\n",
      "[694/700] loss=0.5175 | pos=0.947 neg=-0.010 | val AUC=0.8358\n",
      "[695/700] loss=0.5202 | pos=0.948 neg=-0.003 | val AUC=0.7749\n",
      "[696/700] loss=0.5208 | pos=0.949 neg=0.002 | val AUC=0.8176\n",
      "[697/700] loss=0.5166 | pos=0.949 neg=-0.015 | val AUC=0.7675\n",
      "[698/700] loss=0.5250 | pos=0.950 neg=0.018 | val AUC=0.7372\n",
      "[699/700] loss=0.5189 | pos=0.951 neg=-0.007 | val AUC=0.7972\n",
      "[700/700] loss=0.5169 | pos=0.950 neg=-0.017 | val AUC=0.8294\n",
      "[OK] saved GAT label embeddings → Amazon_products/label_emb_tf  shape=(531, 3467)\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 학습 유틸: 음성 엣지 샘플/로스\n",
    "# ---------------------------\n",
    "def to_upper_pos_edges(A):\n",
    "    pos = []\n",
    "    N = A.shape[0]\n",
    "    for i in range(N):\n",
    "        for j in range(i+1, N):\n",
    "            if A[i, j] == 1:\n",
    "                pos.append((i, j))\n",
    "    return pos\n",
    "\n",
    "def sample_neg(A, k):\n",
    "    N = A.shape[0]\n",
    "    neg = set()\n",
    "    while len(neg) < k:\n",
    "        u = np.random.randint(0, N); v = np.random.randint(0, N)\n",
    "        if u == v: continue\n",
    "        a, b = (u, v) if u < v else (v, u)\n",
    "        if A[a, b] == 0:\n",
    "            neg.add((a, b))\n",
    "    return list(neg)\n",
    "\n",
    "def sample_neg_excluding(A, k, exclude_edges):\n",
    "    \"\"\"\n",
    "    A: np.array [N,N]  (0/1)\n",
    "    k: 뽑을 음성 개수\n",
    "    exclude_edges: {(u,v), ...}  무조건 빼야 하는 양성(또는 금지) 엣지들 (u<v 형태로 넣기)\n",
    "    \"\"\"\n",
    "    N = A.shape[0]\n",
    "    neg = set()\n",
    "    while len(neg) < k:\n",
    "        u = np.random.randint(0, N); v = np.random.randint(0, N)\n",
    "        if u == v:\n",
    "            continue\n",
    "        a, b = (u, v) if u < v else (v, u)\n",
    "        if A[a, b] == 0 and (a, b) not in exclude_edges:\n",
    "            neg.add((a, b))\n",
    "    return list(neg)\n",
    "\n",
    "\n",
    "def edge_score(z, edges):\n",
    "    u = torch.tensor([a for a, _ in edges], device=z.device, dtype=torch.long)\n",
    "    v = torch.tensor([b for _, b in edges], device=z.device, dtype=torch.long)\n",
    "    return (z[u] * z[v]).sum(dim=1)  # 내적 디코더\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "def eval_auc(z, pos_edges, A_full, k_factor=1.0):\n",
    "    z = F.normalize(z, p=2, dim=1)\n",
    "    neg_edges = sample_neg(A_full, int(len(pos_edges) * k_factor))\n",
    "    s = torch.cat([edge_score(z, pos_edges), edge_score(z, neg_edges)]).detach().cpu().numpy()\n",
    "    y = np.concatenate([np.ones(len(pos_edges)), np.zeros(len(neg_edges))])\n",
    "    return roc_auc_score(y, s)\n",
    "\n",
    "hidden_dim=64\n",
    "out_dim=3466\n",
    "heads1=8\n",
    "heads2=8\n",
    "dropout=0.2\n",
    "epochs=200\n",
    "lr=1e-3\n",
    "weight_decay=5e-4\n",
    "neg_ratio=1.0\n",
    "eval_every=20\n",
    "use_full_graph_for_final=True\n",
    "pad_width=2\n",
    "normalize_out = True\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "ids = np.arange(len(label_keys), dtype=np.int64)\n",
    "X = np.vstack([label_embeddings[k] for k in label_keys]).astype(np.float32)\n",
    "X = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "\n",
    "N, d0 = X.shape\n",
    "pos_edges = to_upper_pos_edges(A)\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "idx = rng.permutation(len(pos_edges))\n",
    "n_val = max(1, int(0.1 * len(pos_edges)))          # 10% val\n",
    "pos_val = [pos_edges[i] for i in idx[:n_val]]\n",
    "pos_train = [pos_edges[i] for i in idx[n_val:]]\n",
    "\n",
    "# train 그래프만으로 학습(누출 방지)\n",
    "A_train = np.zeros_like(A)\n",
    "for u, v in pos_train:\n",
    "    A_train[u, v] = 1; A_train[v, u] = 1\n",
    "\n",
    "adj_train = torch.tensor(A_train, dtype=torch.float32, device=device)\n",
    "# 텐서\n",
    "x = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "adj = torch.tensor(A, dtype=torch.float32, device=device)  # softmax 마스크용\n",
    "\n",
    "model = GATEncoder(in_dim=d0, hid_dim=hidden_dim, out_dim=out_dim, heads1=heads1, heads2=heads2, dropout=dropout).to(device)\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "bce = nn.BCEWithLogitsLoss()\n",
    "\n",
    "maxauc = 0\n",
    "best_ckpt = \"Amazon_products/best_gat.ckpt\"\n",
    "# 금지 엣지 집합 (train+val 모두)\n",
    "forbidden = set()\n",
    "for u, v in pos_edges:        # pos_edges = train+val 전체\n",
    "    a, b = (u, v) if u < v else (v, u)\n",
    "    forbidden.add((a, b))\n",
    "\n",
    "for ep in range(1, epochs+1):\n",
    "    model.train()\n",
    "    # 🔴 여기서 전체 adj 말고 train용 adj만 본다\n",
    "    z = model(x, adj_train)                          # [N, out_dim]\n",
    "    if normalize_out:\n",
    "        z = F.normalize(z, p=2, dim=1)\n",
    "\n",
    "    # 🔴 실제로 학습에 쓰는 양성 수 기준으로 음성 수 결정\n",
    "    num_pos = len(pos_train)\n",
    "    num_neg = int(num_pos * neg_ratio)\n",
    "    # 🔴 train 그래프 기준으로 뽑되, train+val 양성은 무조건 제외\n",
    "    neg_edges = sample_neg_excluding(A_train, num_neg, forbidden)\n",
    "\n",
    "    score_pos = edge_score(z, pos_train)\n",
    "    score_neg = edge_score(z, neg_edges)\n",
    "    scores = torch.cat([score_pos, score_neg], dim=0)\n",
    "    labels = torch.cat([torch.ones_like(score_pos), torch.zeros_like(score_neg)], dim=0)\n",
    "\n",
    "    loss = bce(scores, labels)\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    # 평가 부분은 거의 그대로\n",
    "    if ep % 1 == 0 or ep == 1:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # val은 여전히 train 그래프로 임베딩\n",
    "            z_val = F.normalize(model(x, adj_train), p=2, dim=1)\n",
    "            auc_val = eval_auc(z_val, pos_val, A, k_factor=1.0)\n",
    "        print(f\"[{ep:03d}/{epochs}] loss={loss.item():.4f} | \"\n",
    "              f\"pos={score_pos.mean().item():.3f} neg={score_neg.mean().item():.3f} | \"\n",
    "              f\"val AUC={auc_val:.4f}\")\n",
    "        if maxauc < auc_val:\n",
    "            maxauc = auc_val\n",
    "            torch.save(model.state_dict(), best_ckpt)\n",
    "\n",
    "model.load_state_dict(torch.load(best_ckpt, weights_only=True))\n",
    "\n",
    "# 최종 임베딩 추출\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    z = model(x, adj)\n",
    "    if normalize_out:\n",
    "        z = F.normalize(z, p=2, dim=1)\n",
    "    Z = z.detach().cpu().numpy()  # [N, out_dim]\n",
    "OUT_CSV = \"Amazon_products/label_emb_tf\"\n",
    "# CSV 저장 (id + feat00..)\n",
    "pad = max(2, len(str(out_dim-1)))\n",
    "feat_cols = [f\"feat{str(i).zfill(pad)}\" for i in range(out_dim)]\n",
    "df = pd.DataFrame(Z, columns=feat_cols)\n",
    "df.insert(0, \"id\", ids)\n",
    "df.to_csv(OUT_CSV, index=False)\n",
    "print(f\"[OK] saved GAT label embeddings → {OUT_CSV}  shape={df.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3db08a7-6154-49f7-b54f-07b725579112",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T07:49:48.493611Z",
     "iopub.status.busy": "2025-11-13T07:49:48.493213Z",
     "iopub.status.idle": "2025-11-13T07:49:50.612707Z",
     "shell.execute_reply": "2025-11-13T07:49:50.612133Z",
     "shell.execute_reply.started": "2025-11-13T07:49:48.493580Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_docs_txt(path):\n",
    "    \"\"\"\n",
    "    'idx<TAB>text' 형태의 파일을 읽어서\n",
    "    ids: [int, ...]\n",
    "    texts: [str, ...]\n",
    "    을 리턴\n",
    "    \"\"\"\n",
    "    ids = []\n",
    "    texts = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            # 탭 기준\n",
    "            idx_str, txt = line.split(\"\\t\", 1)\n",
    "            ids.append(int(idx_str))\n",
    "            texts.append(txt)\n",
    "    return ids, texts\n",
    "\n",
    "\n",
    "\n",
    "def build_doc_embeddings_from_existing_vectorizer(doc_texts, vectorizer):\n",
    "    \"\"\"\n",
    "    doc_texts: 전처리 전의 원문 리스트\n",
    "    vectorizer: 라벨에 대해 fit되어 있는 TfidfVectorizer\n",
    "    return: dense numpy array [N_docs, vocab]\n",
    "    \"\"\"\n",
    "    # 라벨이랑 동일 규칙으로 전처리\n",
    "    cleaned_docs = [preprocess_label_text(t) for t in doc_texts]\n",
    "    doc_tfidf = vectorizer.transform(cleaned_docs)   # sparse\n",
    "    doc_emb = doc_tfidf.toarray().astype(np.float32)\n",
    "    return doc_emb\n",
    "\n",
    "# 사용 예시\n",
    "# 1) 문서 읽기\n",
    "doc_ids, doc_texts = load_docs_txt(\"Amazon_products/train/train_corpus.txt\")\n",
    "\n",
    "# 2) 라벨 때 만든 vectorizer 재사용해서 임베딩 만들기\n",
    "doc_embeddings = build_doc_embeddings_from_existing_vectorizer(doc_texts, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bd3ba19-4969-452b-9bbe-e2cdbfce97e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T07:40:46.260813Z",
     "iopub.status.busy": "2025-11-13T07:40:46.260666Z",
     "iopub.status.idle": "2025-11-13T07:40:46.263271Z",
     "shell.execute_reply": "2025-11-13T07:40:46.262894Z",
     "shell.execute_reply.started": "2025-11-13T07:40:46.260799Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29487, 3466)\n"
     ]
    }
   ],
   "source": [
    "print(doc_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2df2fb6-6a10-4b74-a909-c9e60bef1f42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T07:49:53.101078Z",
     "iopub.status.busy": "2025-11-13T07:49:53.100658Z",
     "iopub.status.idle": "2025-11-13T07:49:53.105402Z",
     "shell.execute_reply": "2025-11-13T07:49:53.104545Z",
     "shell.execute_reply.started": "2025-11-13T07:49:53.101058Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "[0, 3, 10, 23, 40, 169]\n"
     ]
    }
   ],
   "source": [
    "N = 531 \n",
    "B = np.zeros((N, N), dtype=np.uint8)\n",
    "\n",
    "for u, v in E:\n",
    "    B[u, v] = 1\n",
    "print(B)\n",
    "print(roots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cd05771-6906-4acf-8372-ba0bfd7d4a19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T07:49:53.758134Z",
     "iopub.status.busy": "2025-11-13T07:49:53.757835Z",
     "iopub.status.idle": "2025-11-13T07:49:53.766231Z",
     "shell.execute_reply": "2025-11-13T07:49:53.765780Z",
     "shell.execute_reply.started": "2025-11-13T07:49:53.758109Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy as np\n",
    "\n",
    "def hierarchical_beam_similarity_avg(\n",
    "    doc_vec: np.ndarray,\n",
    "    label_emb: np.ndarray,\n",
    "    adj_upper: np.ndarray,\n",
    "    roots: list[int] = [0],       # 여러 루트\n",
    "    beam: int = 5,\n",
    "    per_parent: str | int = \"l+2\",\n",
    "    tau: float = 0.35,\n",
    "    eps: float = 1e-9,\n",
    "    max_depth: int | None = None,\n",
    "    normalize: bool = False,      # 필요하면 True로\n",
    "):\n",
    "    doc = np.asarray(doc_vec, dtype=np.float32)\n",
    "    L = np.asarray(label_emb, dtype=np.float32)\n",
    "    A = np.asarray(adj_upper).astype(bool)\n",
    "    N, d = L.shape\n",
    "\n",
    "    if normalize:\n",
    "        doc = doc / (np.linalg.norm(doc) + eps)\n",
    "        L = L / (np.linalg.norm(L, axis=1, keepdims=True) + eps)\n",
    "\n",
    "    # 로컬 점수\n",
    "    sims = L @ doc\n",
    "    p = 1.0 / (1.0 + np.exp(-sims / max(tau, 1e-6)))\n",
    "\n",
    "    children = [np.flatnonzero(A[i]) for i in range(N)]\n",
    "\n",
    "    S = np.full(N, -np.inf, dtype=np.float32)\n",
    "    K = np.full(N, -np.inf, dtype=np.float32)\n",
    "    Llen = np.zeros(N, dtype=np.int32)\n",
    "\n",
    "    roots = list(roots)\n",
    "    for r in roots:\n",
    "        S[r] = 0.0\n",
    "        Llen[r] = 0\n",
    "        K[r] = -np.inf\n",
    "\n",
    "    levels = [roots[:]]\n",
    "    cur = roots[:]\n",
    "    level_id = 0\n",
    "\n",
    "    while True:\n",
    "        cand_best = {}\n",
    "        k_parent = (level_id + 2) if (per_parent == \"l+2\") else int(per_parent)\n",
    "\n",
    "        for par in cur:\n",
    "            ch = children[par]\n",
    "            if ch.size == 0:\n",
    "                continue\n",
    "            if ch.size > k_parent:\n",
    "                idx = np.argpartition(-sims[ch], k_parent - 1)[:k_parent]\n",
    "                ch = ch[idx]\n",
    "            for c in ch:\n",
    "                S_c = S[par] + float(p[c])\n",
    "                L_c = Llen[par] + 1\n",
    "                K_c = S_c / (L_c + eps)\n",
    "                if (c not in cand_best) or (K_c > cand_best[c][2]):\n",
    "                    cand_best[c] = (S_c, L_c, K_c)\n",
    "\n",
    "        if not cand_best:\n",
    "            break\n",
    "\n",
    "        kept = sorted(cand_best.items(), key=lambda x: x[1][2], reverse=True)[:min(beam, len(cand_best))]\n",
    "        next_level = [i for i, _ in kept]\n",
    "        for i, (Si, Li, Ki) in kept:\n",
    "            S[i], Llen[i], K[i] = Si, Li, Ki\n",
    "\n",
    "        levels.append(next_level)\n",
    "        cur = next_level\n",
    "        level_id += 1\n",
    "        if max_depth is not None and level_id >= max_depth:\n",
    "            break\n",
    "\n",
    "    return K, levels, sims, p\n",
    "\n",
    "\n",
    "\n",
    "def topk_labels_by_avg(\n",
    "    doc_vec, label_emb, adj_upper, rootㄴ=(0,), beam=5, per_parent=\"l+2\", k=5, **kw\n",
    "):\n",
    "    \"\"\"평균 점수 기반 최종 상위 k 라벨(루트 제외).\"\"\"\n",
    "    K, levels, sims, p = hierarchical_beam_similarity_avg(\n",
    "        doc_vec, label_emb, adj_upper, root=list(roots), beam=beam, per_parent=per_parent, **kw\n",
    "    )\n",
    "    root_set = set(roots)\n",
    "    order = np.argsort(-K)\n",
    "    order = [i for i in order if i not in root_set and np.isfinite(K[i])]\n",
    "    top = order[:k]\n",
    "    return top, K[top]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb5f7b9f-1627-4a14-aca0-769891ef07db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T04:06:01.589232Z",
     "iopub.status.busy": "2025-11-11T04:06:01.588974Z",
     "iopub.status.idle": "2025-11-11T04:06:01.597957Z",
     "shell.execute_reply": "2025-11-11T04:06:01.597468Z",
     "shell.execute_reply.started": "2025-11-11T04:06:01.589213Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e4244ac-1ed8-470c-bc63-1470f460ea33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T07:49:56.981944Z",
     "iopub.status.busy": "2025-11-13T07:49:56.981613Z",
     "iopub.status.idle": "2025-11-13T07:49:56.997764Z",
     "shell.execute_reply": "2025-11-13T07:49:56.997285Z",
     "shell.execute_reply.started": "2025-11-13T07:49:56.981927Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Self-training pipeline with hierarchical silver labeling and dynamic dataloaders.\n",
    "\n",
    "- Reads document/label embeddings CSVs (first column \"id\", rest feat000..feat127)\n",
    "- Reads upper-triangular adjacency (A[i,j]=1 means i->j)\n",
    "- Makes initial silver labels via hierarchical beam search (average score)\n",
    "- Splits into train/val on silver set; keeps the rest as unlabeled pool\n",
    "- Trains a multi-label classifier (Linear/MLP) with BCEWithLogitsLoss\n",
    "- Each epoch, pseudo-labels unlabeled docs whose predicted probs exceed a threshold\n",
    "- Adds them to the training set (up to top_k per doc), with patience-based early stopping\n",
    "\n",
    "Run example\n",
    "-----------\n",
    "python self_training_pipeline.py \\\n",
    "  --doc_csv docs.csv \\\n",
    "  --label_csv labels.csv \\\n",
    "  --adj adj.npy \\\n",
    "  --val_ratio 0.2 --epochs 50 --patience 5 \\\n",
    "  --silver_threshold 0.60 --silver_topk 3 --beam 5 --tau 0.35 --root_id 0 \\\n",
    "  --pseudo_threshold 0.70 --pseudo_topk 3 --batch_size 256 --lr 1e-3\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import argparse\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "def load_embeddings_csv(path: str | Path, id_col: str = \"id\") -> Tuple[List[int], np.ndarray]:\n",
    "    \"\"\"Load embeddings from CSV where the first column is an id and the rest are feature columns.\n",
    "    Returns (ids, float32 matrix).\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    cols = list(df.columns)\n",
    "    if id_col in df.columns:\n",
    "        id_series = df[id_col]\n",
    "        X = df.drop(columns=[id_col])\n",
    "    else:\n",
    "        # Fallback: use the first column as id\n",
    "        id_series = df.iloc[:, 0]\n",
    "        X = df.iloc[:, 1:]\n",
    "    ids = id_series.astype(int).tolist()\n",
    "    X = X.to_numpy(dtype=np.float32)\n",
    "    return ids, X\n",
    "\n",
    "\n",
    "# ----------------------------- Datasets -----------------------------\n",
    "\n",
    "class MultiLabelDataset(Dataset):\n",
    "    def __init__(self, X: np.ndarray, Y: np.ndarray, indices: List[int] | None = None):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.indices = np.array(indices if indices is not None else np.arange(X.shape[0]), dtype=np.int64)\n",
    "    def __len__(self):\n",
    "        return self.indices.shape[0]\n",
    "    def __getitem__(self, idx: int):\n",
    "        i = int(self.indices[idx])\n",
    "        x = torch.from_numpy(self.X[i])\n",
    "        y = torch.from_numpy(self.Y[i])\n",
    "        return x, y\n",
    "\n",
    "class UnlabeledDataset(Dataset):\n",
    "    def __init__(self, X: np.ndarray, indices: List[int]):\n",
    "        self.X = X\n",
    "        self.indices = np.array(indices, dtype=np.int64)\n",
    "    def __len__(self):\n",
    "        return self.indices.shape[0]\n",
    "    def __getitem__(self, idx: int):\n",
    "        i = int(self.indices[idx])\n",
    "        x = torch.from_numpy(self.X[i])\n",
    "        return x, i\n",
    "\n",
    "# ----------------------------- Model -----------------------------\n",
    "\n",
    "class MLPHead(nn.Module):\n",
    "    def __init__(self, in_dim: int, out_dim: int, hidden: int | None = 256, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        if hidden is None or hidden <= 0:\n",
    "            self.net = nn.Sequential(\n",
    "                nn.LayerNorm(in_dim),\n",
    "                nn.Linear(in_dim, out_dim),\n",
    "            )\n",
    "        else:\n",
    "            self.net = nn.Sequential(\n",
    "                nn.LayerNorm(in_dim),\n",
    "                nn.Linear(in_dim, hidden),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(hidden, out_dim),\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# ----------------------------- Utils -----------------------------\n",
    "\n",
    "def to_device(batch, device):\n",
    "    if isinstance(batch, (tuple, list)):\n",
    "        return [b.to(device) if torch.is_tensor(b) else b for b in batch]\n",
    "    return batch.to(device)\n",
    "\n",
    "\n",
    "def micro_f1(y_true: np.ndarray, y_prob: np.ndarray, thr: float = 0.5, eps: float = 1e-9) -> float:\n",
    "    y_pred = (y_prob >= thr).astype(np.float32)\n",
    "    tp = (y_true * y_pred).sum()\n",
    "    fp = ((1 - y_true) * y_pred).sum()\n",
    "    fn = (y_true * (1 - y_pred)).sum()\n",
    "    prec = tp / (tp + fp + eps)\n",
    "    rec = tp / (tp + fn + eps)\n",
    "    f1 = 2 * prec * rec / (prec + rec + eps)\n",
    "    return float(f1)\n",
    "\n",
    "# -------- Initial silver labeling (no CSV save; in-memory) --------\n",
    "def make_initial_silver_hier(\n",
    "    docs: np.ndarray,\n",
    "    labels: np.ndarray,\n",
    "    adj: np.ndarray,\n",
    "    roots: list[int] = [0],\n",
    "    silver_threshold: float = 0.6,    # 이건 avg(K) 기준\n",
    "    silver_topk: int = 3,\n",
    "    beam: int = 5,\n",
    "    per_parent: str | int = \"l+2\",\n",
    "    tau: float = 0.35,\n",
    ") -> list[list[int]]:\n",
    "    \"\"\"\n",
    "    계층 빔 서치로 각 문서의 라벨 후보를 뽑는다.\n",
    "    - 계층 밖 라벨은 애초에 안 들어옴\n",
    "    - 루트들은 결과에서 제외\n",
    "    - K(경로 평균) >= silver_threshold 인 애들 중 top-k\n",
    "    \"\"\"\n",
    "    N = labels.shape[0]\n",
    "    silver: list[list[int]] = []\n",
    "    root_set = set(roots)\n",
    "\n",
    "    for d in docs:\n",
    "        K, levels, sims, p = hierarchical_beam_similarity_avg(\n",
    "            d, labels, adj,\n",
    "            roots=roots,\n",
    "            beam=beam,\n",
    "            per_parent=per_parent,\n",
    "            tau=tau,\n",
    "            normalize=False,   # 너 임베딩이 이미 L2라면 False\n",
    "        )\n",
    "        # 평균 점수로 정렬\n",
    "        order = np.argsort(-K)\n",
    "        # 루트는 제외, 유한한 것만\n",
    "        order = [i for i in order if (i not in root_set) and np.isfinite(K[i])]\n",
    "        # threshold 통과한 것만\n",
    "        cand = [i for i in order if K[i] >= silver_threshold]\n",
    "        selected = cand[:silver_topk]\n",
    "        silver.append(selected)\n",
    "\n",
    "    return silver\n",
    "\n",
    "def make_initial_silver(\n",
    "    docs: np.ndarray,\n",
    "    labels: np.ndarray,\n",
    "    adj: np.ndarray,              # 이제 안 씀 (호환용으로만 둠)\n",
    "    silver_threshold: float = 0.9,\n",
    "    silver_topk: int = 3,\n",
    "    beam: int = 5,                # 이제 안 씀\n",
    "    tau: float = 0.35,\n",
    "    root_id: int = 0,\n",
    ") -> List[List[int]]:\n",
    "    \"\"\"\n",
    "    문서마다 전 라벨 임베딩과의 유사도를 보고 초기 silver label을 만든다.\n",
    "    - 트리/경로 탐색 안 함\n",
    "    - root_id는 결과에서 제외\n",
    "    - p >= silver_threshold인 라벨 중에서 상위 silver_topk만 남김\n",
    "    \"\"\"\n",
    "    N = labels.shape[0]\n",
    "    silver: List[List[int]] = []\n",
    "\n",
    "    for d in docs:\n",
    "        # 문서 vs 모든 라벨 점수\n",
    "        sims, p = all_label_similarity(d, labels, tau=tau, normalize=True)\n",
    "\n",
    "        # threshold 통과 + root 제외\n",
    "        cand = [\n",
    "            (i, float(p[i]))\n",
    "            for i in range(N)\n",
    "            if i != root_id and np.isfinite(p[i]) and p[i] >= silver_threshold\n",
    "        ]\n",
    "\n",
    "        # 점수 높은 순\n",
    "        cand.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # label index만 추출\n",
    "        selected = [i for i, _ in cand[:silver_topk]]\n",
    "        silver.append(selected)\n",
    "\n",
    "    return silver\n",
    "# ------------------------ Train / Self-Training ------------------------\n",
    "\n",
    "def train_epoch(model, loader, optim, device, criterion):\n",
    "    model.train()\n",
    "    total = 0.0\n",
    "    for x, y in loader:\n",
    "        x, y = to_device(x, device), to_device(y, device)\n",
    "        optim.zero_grad(set_to_none=True)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        total += float(loss.detach().cpu().item()) * x.size(0)\n",
    "    return total / max(1, len(loader.dataset))\n",
    "\n",
    "\n",
    "def eval_epoch(model, loader, device, criterion, thr=0.5):\n",
    "    model.eval()\n",
    "    total = 0.0\n",
    "    ys = []\n",
    "    ps = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = to_device(x, device), to_device(y, device)\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            total += float(loss.detach().cpu().item()) * x.size(0)\n",
    "            prob = torch.sigmoid(logits).detach().cpu().numpy()\n",
    "            ys.append(y.detach().cpu().numpy())\n",
    "            ps.append(prob)\n",
    "    y_true = np.concatenate(ys, axis=0)\n",
    "    y_prob = np.concatenate(ps, axis=0)\n",
    "    f1 = micro_f1(y_true, y_prob, thr=thr)\n",
    "    return total / max(1, len(loader.dataset)), f1, y_prob\n",
    "\n",
    "\n",
    "def pseudo_label_and_grow(model, unl_ds: UnlabeledDataset,\n",
    "                          num_labels: int,\n",
    "                          pseudo_threshold: float = 0.9, pseudo_topk: int = 3,\n",
    "                          device: str = \"cpu\", batch_size: int = 512):\n",
    "    \"\"\"Infer on unlabeled, select labels with prob>=threshold (top-k), and return new_indices and Y matrix.\"\"\"\n",
    "    if len(unl_ds) == 0:\n",
    "        return [], np.zeros((0, num_labels), dtype=np.float32)\n",
    "    loader = DataLoader(unl_ds, batch_size=batch_size, shuffle=False)\n",
    "    model.eval()\n",
    "    all_idx: List[int] = []\n",
    "    all_y: List[np.ndarray] = []\n",
    "    with torch.no_grad():\n",
    "        for xb, idxs in loader:\n",
    "            xb = xb.to(device)\n",
    "            logits = model(xb)\n",
    "            prob = torch.sigmoid(logits).detach().cpu().numpy()\n",
    "            for p, i in zip(prob, idxs.numpy().tolist()):\n",
    "                sel = np.flatnonzero(p >= pseudo_threshold)\n",
    "                if sel.size > 0:\n",
    "                    # keep at most top-k by prob\n",
    "                    if sel.size > pseudo_topk:\n",
    "                        top = np.argpartition(-p[sel], pseudo_topk - 1)[:pseudo_topk]\n",
    "                        sel = sel[top]\n",
    "                    y = np.zeros(num_labels, dtype=np.float32)\n",
    "                    y[sel] = 1.0\n",
    "                    all_idx.append(int(i))\n",
    "                    all_y.append(y)\n",
    "    if len(all_idx) == 0:\n",
    "        return [], np.zeros((0, num_labels), dtype=np.float32)\n",
    "    Y_new = np.stack(all_y, axis=0)\n",
    "    return all_idx, Y_new\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cccc039-f93f-49de-8843-19c7b5e4338d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d15867a-f278-41ab-88a0-e751c62e5873",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0b4cfc-c593-4841-b847-2759ad663984",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c63ecfd-663d-4ed8-88b2-e8ecbc14de5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T07:57:23.534832Z",
     "iopub.status.busy": "2025-11-13T07:57:23.534611Z",
     "iopub.status.idle": "2025-11-13T07:59:56.725630Z",
     "shell.execute_reply": "2025-11-13T07:59:56.724781Z",
     "shell.execute_reply.started": "2025-11-13T07:57:23.534816Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 29487\n",
      "with silver: 12014\n",
      "unlabeled : 17473\n",
      "9612 2402 17473\n",
      "Epoch 001 | train_loss=0.446  val_loss=0.115  val_f1=0.006\n",
      "  + (skip pseudo-labeling on warmup epoch)\n",
      "Epoch 002 | train_loss=0.060  val_loss=0.042  val_f1=0.150\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 003 | train_loss=0.039  val_loss=0.037  val_f1=0.150\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 004 | train_loss=0.036  val_loss=0.034  val_f1=0.165\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 005 | train_loss=0.034  val_loss=0.033  val_f1=0.165\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 006 | train_loss=0.033  val_loss=0.033  val_f1=0.165\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 007 | train_loss=0.032  val_loss=0.032  val_f1=0.165\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 008 | train_loss=0.032  val_loss=0.032  val_f1=0.193\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 009 | train_loss=0.032  val_loss=0.031  val_f1=0.166\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 010 | train_loss=0.031  val_loss=0.031  val_f1=0.191\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 011 | train_loss=0.031  val_loss=0.031  val_f1=0.177\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 012 | train_loss=0.031  val_loss=0.030  val_f1=0.209\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 013 | train_loss=0.030  val_loss=0.030  val_f1=0.263\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 014 | train_loss=0.029  val_loss=0.029  val_f1=0.299\n",
      "  + Added 12 pseudo-labeled docs (unl pool → 17461 left)\n",
      "Epoch 015 | train_loss=0.028  val_loss=0.028  val_f1=0.342\n",
      "  + Added 93 pseudo-labeled docs (unl pool → 17368 left)\n",
      "Epoch 016 | train_loss=0.027  val_loss=0.027  val_f1=0.357\n",
      "  + Added 184 pseudo-labeled docs (unl pool → 17184 left)\n",
      "Epoch 017 | train_loss=0.026  val_loss=0.026  val_f1=0.373\n",
      "  + Added 293 pseudo-labeled docs (unl pool → 16891 left)\n",
      "Epoch 018 | train_loss=0.025  val_loss=0.025  val_f1=0.384\n",
      "  + Added 226 pseudo-labeled docs (unl pool → 16665 left)\n",
      "Epoch 019 | train_loss=0.024  val_loss=0.024  val_f1=0.399\n",
      "  + Added 176 pseudo-labeled docs (unl pool → 16489 left)\n",
      "Epoch 020 | train_loss=0.023  val_loss=0.023  val_f1=0.413\n",
      "  + Added 96 pseudo-labeled docs (unl pool → 16393 left)\n",
      "Epoch 021 | train_loss=0.022  val_loss=0.022  val_f1=0.424\n",
      "  + Added 65 pseudo-labeled docs (unl pool → 16328 left)\n",
      "Epoch 022 | train_loss=0.021  val_loss=0.021  val_f1=0.439\n",
      "  + Added 61 pseudo-labeled docs (unl pool → 16267 left)\n",
      "Epoch 023 | train_loss=0.020  val_loss=0.020  val_f1=0.452\n",
      "  + Added 144 pseudo-labeled docs (unl pool → 16123 left)\n",
      "Epoch 024 | train_loss=0.019  val_loss=0.019  val_f1=0.464\n",
      "  + Added 69 pseudo-labeled docs (unl pool → 16054 left)\n",
      "Epoch 025 | train_loss=0.019  val_loss=0.019  val_f1=0.473\n",
      "  + Added 70 pseudo-labeled docs (unl pool → 15984 left)\n",
      "Epoch 026 | train_loss=0.018  val_loss=0.018  val_f1=0.481\n",
      "  + Added 148 pseudo-labeled docs (unl pool → 15836 left)\n",
      "Epoch 027 | train_loss=0.017  val_loss=0.017  val_f1=0.492\n",
      "  + Added 108 pseudo-labeled docs (unl pool → 15728 left)\n",
      "Epoch 028 | train_loss=0.017  val_loss=0.017  val_f1=0.504\n",
      "  + Added 121 pseudo-labeled docs (unl pool → 15607 left)\n",
      "Epoch 029 | train_loss=0.016  val_loss=0.016  val_f1=0.514\n",
      "  + Added 143 pseudo-labeled docs (unl pool → 15464 left)\n",
      "Epoch 030 | train_loss=0.015  val_loss=0.016  val_f1=0.525\n",
      "  + Added 93 pseudo-labeled docs (unl pool → 15371 left)\n",
      "Epoch 031 | train_loss=0.015  val_loss=0.015  val_f1=0.536\n",
      "  + Added 74 pseudo-labeled docs (unl pool → 15297 left)\n",
      "Epoch 032 | train_loss=0.014  val_loss=0.015  val_f1=0.543\n",
      "  + Added 129 pseudo-labeled docs (unl pool → 15168 left)\n",
      "Epoch 033 | train_loss=0.014  val_loss=0.014  val_f1=0.551\n",
      "  + Added 104 pseudo-labeled docs (unl pool → 15064 left)\n",
      "Epoch 034 | train_loss=0.013  val_loss=0.014  val_f1=0.559\n",
      "  + Added 46 pseudo-labeled docs (unl pool → 15018 left)\n",
      "Epoch 035 | train_loss=0.013  val_loss=0.013  val_f1=0.563\n",
      "  + Added 96 pseudo-labeled docs (unl pool → 14922 left)\n",
      "Epoch 036 | train_loss=0.012  val_loss=0.013  val_f1=0.572\n",
      "  + Added 121 pseudo-labeled docs (unl pool → 14801 left)\n",
      "Epoch 037 | train_loss=0.012  val_loss=0.012  val_f1=0.577\n",
      "  + Added 64 pseudo-labeled docs (unl pool → 14737 left)\n",
      "Epoch 038 | train_loss=0.011  val_loss=0.012  val_f1=0.581\n",
      "  + Added 136 pseudo-labeled docs (unl pool → 14601 left)\n",
      "Epoch 039 | train_loss=0.011  val_loss=0.012  val_f1=0.586\n",
      "  + Added 53 pseudo-labeled docs (unl pool → 14548 left)\n",
      "Epoch 040 | train_loss=0.010  val_loss=0.011  val_f1=0.591\n",
      "  + Added 148 pseudo-labeled docs (unl pool → 14400 left)\n",
      "Epoch 041 | train_loss=0.010  val_loss=0.011  val_f1=0.597\n",
      "  + Added 30 pseudo-labeled docs (unl pool → 14370 left)\n",
      "Epoch 042 | train_loss=0.010  val_loss=0.011  val_f1=0.601\n",
      "  + Added 152 pseudo-labeled docs (unl pool → 14218 left)\n",
      "Epoch 043 | train_loss=0.009  val_loss=0.010  val_f1=0.608\n",
      "  + Added 46 pseudo-labeled docs (unl pool → 14172 left)\n",
      "Epoch 044 | train_loss=0.009  val_loss=0.010  val_f1=0.610\n",
      "  + Added 121 pseudo-labeled docs (unl pool → 14051 left)\n",
      "Epoch 045 | train_loss=0.009  val_loss=0.010  val_f1=0.614\n",
      "  + Added 84 pseudo-labeled docs (unl pool → 13967 left)\n",
      "Epoch 046 | train_loss=0.008  val_loss=0.010  val_f1=0.617\n",
      "  + Added 74 pseudo-labeled docs (unl pool → 13893 left)\n",
      "Epoch 047 | train_loss=0.008  val_loss=0.009  val_f1=0.624\n",
      "  + Added 62 pseudo-labeled docs (unl pool → 13831 left)\n",
      "Epoch 048 | train_loss=0.008  val_loss=0.009  val_f1=0.628\n",
      "  + Added 102 pseudo-labeled docs (unl pool → 13729 left)\n",
      "Epoch 049 | train_loss=0.007  val_loss=0.009  val_f1=0.633\n",
      "  + Added 64 pseudo-labeled docs (unl pool → 13665 left)\n",
      "Epoch 050 | train_loss=0.007  val_loss=0.009  val_f1=0.635\n",
      "  + Added 183 pseudo-labeled docs (unl pool → 13482 left)\n",
      "Epoch 051 | train_loss=0.007  val_loss=0.009  val_f1=0.640\n",
      "  + Added 114 pseudo-labeled docs (unl pool → 13368 left)\n",
      "Epoch 052 | train_loss=0.007  val_loss=0.008  val_f1=0.642\n",
      "  + Added 61 pseudo-labeled docs (unl pool → 13307 left)\n",
      "Epoch 053 | train_loss=0.006  val_loss=0.008  val_f1=0.645\n",
      "  + Added 48 pseudo-labeled docs (unl pool → 13259 left)\n",
      "Epoch 054 | train_loss=0.006  val_loss=0.008  val_f1=0.647\n",
      "  + Added 72 pseudo-labeled docs (unl pool → 13187 left)\n",
      "Epoch 055 | train_loss=0.006  val_loss=0.008  val_f1=0.649\n",
      "  + Added 122 pseudo-labeled docs (unl pool → 13065 left)\n",
      "Epoch 056 | train_loss=0.006  val_loss=0.008  val_f1=0.651\n",
      "  + Added 69 pseudo-labeled docs (unl pool → 12996 left)\n",
      "Epoch 057 | train_loss=0.005  val_loss=0.008  val_f1=0.653\n",
      "  + Added 99 pseudo-labeled docs (unl pool → 12897 left)\n",
      "Epoch 058 | train_loss=0.005  val_loss=0.008  val_f1=0.657\n",
      "  + Added 123 pseudo-labeled docs (unl pool → 12774 left)\n",
      "Epoch 059 | train_loss=0.005  val_loss=0.008  val_f1=0.655\n",
      "  + Added 58 pseudo-labeled docs (unl pool → 12716 left)\n",
      "Epoch 060 | train_loss=0.005  val_loss=0.007  val_f1=0.659\n",
      "  + Added 56 pseudo-labeled docs (unl pool → 12660 left)\n",
      "Epoch 061 | train_loss=0.005  val_loss=0.007  val_f1=0.660\n",
      "  + Added 149 pseudo-labeled docs (unl pool → 12511 left)\n",
      "Epoch 062 | train_loss=0.005  val_loss=0.007  val_f1=0.662\n",
      "  + Added 101 pseudo-labeled docs (unl pool → 12410 left)\n",
      "Epoch 063 | train_loss=0.004  val_loss=0.007  val_f1=0.664\n",
      "  + Added 64 pseudo-labeled docs (unl pool → 12346 left)\n",
      "Epoch 064 | train_loss=0.004  val_loss=0.007  val_f1=0.666\n",
      "  + Added 68 pseudo-labeled docs (unl pool → 12278 left)\n",
      "Epoch 065 | train_loss=0.004  val_loss=0.007  val_f1=0.668\n",
      "  + Added 53 pseudo-labeled docs (unl pool → 12225 left)\n",
      "Epoch 066 | train_loss=0.004  val_loss=0.007  val_f1=0.669\n",
      "  + Added 162 pseudo-labeled docs (unl pool → 12063 left)\n",
      "Epoch 067 | train_loss=0.004  val_loss=0.007  val_f1=0.670\n",
      "  + Added 58 pseudo-labeled docs (unl pool → 12005 left)\n",
      "Epoch 068 | train_loss=0.004  val_loss=0.007  val_f1=0.671\n",
      "  + Added 82 pseudo-labeled docs (unl pool → 11923 left)\n",
      "Epoch 069 | train_loss=0.003  val_loss=0.007  val_f1=0.675\n",
      "  + Added 26 pseudo-labeled docs (unl pool → 11897 left)\n",
      "Epoch 070 | train_loss=0.003  val_loss=0.007  val_f1=0.676\n",
      "  + Added 144 pseudo-labeled docs (unl pool → 11753 left)\n",
      "Epoch 071 | train_loss=0.003  val_loss=0.007  val_f1=0.677\n",
      "  + Added 58 pseudo-labeled docs (unl pool → 11695 left)\n",
      "Epoch 072 | train_loss=0.003  val_loss=0.006  val_f1=0.677\n",
      "  + Added 57 pseudo-labeled docs (unl pool → 11638 left)\n",
      "Epoch 073 | train_loss=0.003  val_loss=0.006  val_f1=0.681\n",
      "  + Added 50 pseudo-labeled docs (unl pool → 11588 left)\n",
      "Epoch 074 | train_loss=0.003  val_loss=0.006  val_f1=0.681\n",
      "  + Added 49 pseudo-labeled docs (unl pool → 11539 left)\n",
      "Epoch 075 | train_loss=0.003  val_loss=0.006  val_f1=0.680\n",
      "  + Added 129 pseudo-labeled docs (unl pool → 11410 left)\n",
      "Epoch 076 | train_loss=0.003  val_loss=0.006  val_f1=0.683\n",
      "  + Added 108 pseudo-labeled docs (unl pool → 11302 left)\n",
      "Epoch 077 | train_loss=0.003  val_loss=0.006  val_f1=0.684\n",
      "  + Added 46 pseudo-labeled docs (unl pool → 11256 left)\n",
      "Epoch 078 | train_loss=0.003  val_loss=0.006  val_f1=0.685\n",
      "  + Added 33 pseudo-labeled docs (unl pool → 11223 left)\n",
      "Epoch 079 | train_loss=0.002  val_loss=0.006  val_f1=0.685\n",
      "  + Added 36 pseudo-labeled docs (unl pool → 11187 left)\n",
      "Epoch 080 | train_loss=0.002  val_loss=0.006  val_f1=0.686\n",
      "  + Added 53 pseudo-labeled docs (unl pool → 11134 left)\n",
      "Epoch 081 | train_loss=0.002  val_loss=0.006  val_f1=0.686\n",
      "  + Added 63 pseudo-labeled docs (unl pool → 11071 left)\n",
      "Epoch 082 | train_loss=0.002  val_loss=0.006  val_f1=0.687\n",
      "  + Added 37 pseudo-labeled docs (unl pool → 11034 left)\n",
      "Epoch 083 | train_loss=0.002  val_loss=0.006  val_f1=0.688\n",
      "  + Added 45 pseudo-labeled docs (unl pool → 10989 left)\n",
      "Epoch 084 | train_loss=0.002  val_loss=0.006  val_f1=0.689\n",
      "  + Added 47 pseudo-labeled docs (unl pool → 10942 left)\n",
      "Epoch 085 | train_loss=0.002  val_loss=0.006  val_f1=0.689\n",
      "  + Added 128 pseudo-labeled docs (unl pool → 10814 left)\n",
      "Epoch 086 | train_loss=0.002  val_loss=0.006  val_f1=0.689\n",
      "  + Added 31 pseudo-labeled docs (unl pool → 10783 left)\n",
      "Epoch 087 | train_loss=0.002  val_loss=0.006  val_f1=0.690\n",
      "  + Added 23 pseudo-labeled docs (unl pool → 10760 left)\n",
      "Epoch 088 | train_loss=0.002  val_loss=0.006  val_f1=0.693\n",
      "  + Added 26 pseudo-labeled docs (unl pool → 10734 left)\n",
      "Epoch 089 | train_loss=0.002  val_loss=0.006  val_f1=0.693\n",
      "  + Added 43 pseudo-labeled docs (unl pool → 10691 left)\n",
      "Epoch 090 | train_loss=0.002  val_loss=0.006  val_f1=0.693\n",
      "  + Added 72 pseudo-labeled docs (unl pool → 10619 left)\n",
      "Epoch 091 | train_loss=0.002  val_loss=0.006  val_f1=0.695\n",
      "  + Added 18 pseudo-labeled docs (unl pool → 10601 left)\n",
      "Epoch 092 | train_loss=0.002  val_loss=0.006  val_f1=0.695\n",
      "  + Added 47 pseudo-labeled docs (unl pool → 10554 left)\n",
      "Epoch 093 | train_loss=0.001  val_loss=0.006  val_f1=0.695\n",
      "  + Added 38 pseudo-labeled docs (unl pool → 10516 left)\n",
      "Epoch 094 | train_loss=0.001  val_loss=0.006  val_f1=0.694\n",
      "  + Added 63 pseudo-labeled docs (unl pool → 10453 left)\n",
      "Epoch 095 | train_loss=0.001  val_loss=0.006  val_f1=0.694\n",
      "  + Added 18 pseudo-labeled docs (unl pool → 10435 left)\n",
      "Epoch 096 | train_loss=0.001  val_loss=0.006  val_f1=0.696\n",
      "  + Added 69 pseudo-labeled docs (unl pool → 10366 left)\n",
      "Epoch 097 | train_loss=0.001  val_loss=0.006  val_f1=0.694\n",
      "  + Added 59 pseudo-labeled docs (unl pool → 10307 left)\n",
      "Epoch 098 | train_loss=0.001  val_loss=0.006  val_f1=0.695\n",
      "  + Added 23 pseudo-labeled docs (unl pool → 10284 left)\n",
      "Epoch 099 | train_loss=0.001  val_loss=0.006  val_f1=0.697\n",
      "  + Added 26 pseudo-labeled docs (unl pool → 10258 left)\n",
      "Epoch 100 | train_loss=0.001  val_loss=0.006  val_f1=0.697\n",
      "  + Added 29 pseudo-labeled docs (unl pool → 10229 left)\n",
      "Epoch 101 | train_loss=0.001  val_loss=0.006  val_f1=0.698\n",
      "  + Added 57 pseudo-labeled docs (unl pool → 10172 left)\n",
      "Epoch 102 | train_loss=0.001  val_loss=0.006  val_f1=0.697\n",
      "  + Added 36 pseudo-labeled docs (unl pool → 10136 left)\n",
      "Epoch 103 | train_loss=0.001  val_loss=0.006  val_f1=0.698\n",
      "  + Added 32 pseudo-labeled docs (unl pool → 10104 left)\n",
      "Epoch 104 | train_loss=0.001  val_loss=0.006  val_f1=0.698\n",
      "  + Added 83 pseudo-labeled docs (unl pool → 10021 left)\n",
      "Epoch 105 | train_loss=0.001  val_loss=0.006  val_f1=0.699\n",
      "  + Added 10 pseudo-labeled docs (unl pool → 10011 left)\n",
      "Epoch 106 | train_loss=0.001  val_loss=0.007  val_f1=0.698\n",
      "  + Added 19 pseudo-labeled docs (unl pool → 9992 left)\n",
      "Epoch 107 | train_loss=0.001  val_loss=0.007  val_f1=0.698\n",
      "  + Added 59 pseudo-labeled docs (unl pool → 9933 left)\n",
      "Epoch 108 | train_loss=0.001  val_loss=0.007  val_f1=0.698\n",
      "  + Added 23 pseudo-labeled docs (unl pool → 9910 left)\n",
      "Epoch 109 | train_loss=0.001  val_loss=0.007  val_f1=0.698\n",
      "  + Added 32 pseudo-labeled docs (unl pool → 9878 left)\n",
      "Epoch 110 | train_loss=0.001  val_loss=0.007  val_f1=0.698\n",
      "  + Added 33 pseudo-labeled docs (unl pool → 9845 left)\n",
      "Epoch 111 | train_loss=0.001  val_loss=0.007  val_f1=0.698\n",
      "  + Added 29 pseudo-labeled docs (unl pool → 9816 left)\n",
      "Epoch 112 | train_loss=0.001  val_loss=0.007  val_f1=0.698\n",
      "  + Added 62 pseudo-labeled docs (unl pool → 9754 left)\n",
      "Epoch 113 | train_loss=0.001  val_loss=0.007  val_f1=0.700\n",
      "  + Added 13 pseudo-labeled docs (unl pool → 9741 left)\n",
      "Epoch 114 | train_loss=0.001  val_loss=0.007  val_f1=0.698\n",
      "  + Added 37 pseudo-labeled docs (unl pool → 9704 left)\n",
      "Epoch 115 | train_loss=0.001  val_loss=0.007  val_f1=0.699\n",
      "  + Added 43 pseudo-labeled docs (unl pool → 9661 left)\n",
      "Epoch 116 | train_loss=0.001  val_loss=0.007  val_f1=0.699\n",
      "  + Added 54 pseudo-labeled docs (unl pool → 9607 left)\n",
      "Epoch 117 | train_loss=0.001  val_loss=0.007  val_f1=0.700\n",
      "  + Added 34 pseudo-labeled docs (unl pool → 9573 left)\n",
      "Epoch 118 | train_loss=0.001  val_loss=0.007  val_f1=0.700\n",
      "  + Added 19 pseudo-labeled docs (unl pool → 9554 left)\n",
      "Epoch 119 | train_loss=0.001  val_loss=0.007  val_f1=0.699\n",
      "  + Added 57 pseudo-labeled docs (unl pool → 9497 left)\n",
      "Epoch 120 | train_loss=0.001  val_loss=0.007  val_f1=0.700\n",
      "  + Added 27 pseudo-labeled docs (unl pool → 9470 left)\n",
      "Epoch 121 | train_loss=0.001  val_loss=0.007  val_f1=0.700\n",
      "  + Added 58 pseudo-labeled docs (unl pool → 9412 left)\n",
      "Epoch 122 | train_loss=0.001  val_loss=0.007  val_f1=0.700\n",
      "  + Added 6 pseudo-labeled docs (unl pool → 9406 left)\n",
      "Epoch 123 | train_loss=0.001  val_loss=0.007  val_f1=0.699\n",
      "  + Added 70 pseudo-labeled docs (unl pool → 9336 left)\n",
      "Epoch 124 | train_loss=0.001  val_loss=0.007  val_f1=0.699\n",
      "  + Added 10 pseudo-labeled docs (unl pool → 9326 left)\n",
      "Epoch 125 | train_loss=0.001  val_loss=0.007  val_f1=0.699\n",
      "  + Added 10 pseudo-labeled docs (unl pool → 9316 left)\n",
      "Epoch 126 | train_loss=0.000  val_loss=0.007  val_f1=0.699\n",
      "  + Added 48 pseudo-labeled docs (unl pool → 9268 left)\n",
      "Epoch 127 | train_loss=0.000  val_loss=0.007  val_f1=0.698\n",
      "  + Added 9 pseudo-labeled docs (unl pool → 9259 left)\n",
      "Epoch 128 | train_loss=0.000  val_loss=0.007  val_f1=0.700\n",
      "  + Added 63 pseudo-labeled docs (unl pool → 9196 left)\n",
      "Epoch 129 | train_loss=0.000  val_loss=0.007  val_f1=0.699\n",
      "  + Added 23 pseudo-labeled docs (unl pool → 9173 left)\n",
      "Epoch 130 | train_loss=0.000  val_loss=0.008  val_f1=0.699\n",
      "  + Added 40 pseudo-labeled docs (unl pool → 9133 left)\n",
      "Epoch 131 | train_loss=0.000  val_loss=0.008  val_f1=0.701\n",
      "  + Added 10 pseudo-labeled docs (unl pool → 9123 left)\n",
      "Epoch 132 | train_loss=0.000  val_loss=0.008  val_f1=0.700\n",
      "  + Added 7 pseudo-labeled docs (unl pool → 9116 left)\n",
      "Epoch 133 | train_loss=0.000  val_loss=0.008  val_f1=0.699\n",
      "  + Added 16 pseudo-labeled docs (unl pool → 9100 left)\n",
      "Epoch 134 | train_loss=0.000  val_loss=0.008  val_f1=0.699\n",
      "  + Added 27 pseudo-labeled docs (unl pool → 9073 left)\n",
      "Epoch 135 | train_loss=0.000  val_loss=0.008  val_f1=0.700\n",
      "  + Added 48 pseudo-labeled docs (unl pool → 9025 left)\n",
      "Epoch 136 | train_loss=0.000  val_loss=0.008  val_f1=0.699\n",
      "  + Added 3 pseudo-labeled docs (unl pool → 9022 left)\n",
      "Epoch 137 | train_loss=0.000  val_loss=0.008  val_f1=0.699\n",
      "  + Added 26 pseudo-labeled docs (unl pool → 8996 left)\n",
      "Epoch 138 | train_loss=0.000  val_loss=0.008  val_f1=0.701\n",
      "  + Added 15 pseudo-labeled docs (unl pool → 8981 left)\n",
      "Epoch 139 | train_loss=0.000  val_loss=0.008  val_f1=0.701\n",
      "  + Added 56 pseudo-labeled docs (unl pool → 8925 left)\n",
      "Epoch 140 | train_loss=0.000  val_loss=0.008  val_f1=0.699\n",
      "  + Added 10 pseudo-labeled docs (unl pool → 8915 left)\n",
      "Epoch 141 | train_loss=0.000  val_loss=0.008  val_f1=0.699\n",
      "  + Added 25 pseudo-labeled docs (unl pool → 8890 left)\n",
      "Epoch 142 | train_loss=0.000  val_loss=0.008  val_f1=0.700\n",
      "  + Added 7 pseudo-labeled docs (unl pool → 8883 left)\n",
      "Epoch 143 | train_loss=0.000  val_loss=0.008  val_f1=0.699\n",
      "  + Added 61 pseudo-labeled docs (unl pool → 8822 left)\n",
      "Epoch 144 | train_loss=0.000  val_loss=0.008  val_f1=0.700\n",
      "  + Added 12 pseudo-labeled docs (unl pool → 8810 left)\n",
      "Epoch 145 | train_loss=0.000  val_loss=0.008  val_f1=0.701\n",
      "  + Added 27 pseudo-labeled docs (unl pool → 8783 left)\n",
      "Epoch 146 | train_loss=0.000  val_loss=0.008  val_f1=0.701\n",
      "  + Added 15 pseudo-labeled docs (unl pool → 8768 left)\n",
      "Epoch 147 | train_loss=0.000  val_loss=0.008  val_f1=0.700\n",
      "  + Added 6 pseudo-labeled docs (unl pool → 8762 left)\n",
      "Epoch 148 | train_loss=0.000  val_loss=0.008  val_f1=0.700\n",
      "  + Added 19 pseudo-labeled docs (unl pool → 8743 left)\n",
      "Epoch 149 | train_loss=0.000  val_loss=0.009  val_f1=0.700\n",
      "  + Added 1 pseudo-labeled docs (unl pool → 8742 left)\n",
      "Epoch 150 | train_loss=0.000  val_loss=0.009  val_f1=0.699\n",
      "  + Added 9 pseudo-labeled docs (unl pool → 8733 left)\n",
      "Epoch 151 | train_loss=0.000  val_loss=0.009  val_f1=0.700\n",
      "  + Added 19 pseudo-labeled docs (unl pool → 8714 left)\n",
      "Epoch 152 | train_loss=0.000  val_loss=0.009  val_f1=0.699\n",
      "  + Added 11 pseudo-labeled docs (unl pool → 8703 left)\n",
      "Epoch 153 | train_loss=0.000  val_loss=0.009  val_f1=0.701\n",
      "  + Added 45 pseudo-labeled docs (unl pool → 8658 left)\n",
      "Epoch 154 | train_loss=0.000  val_loss=0.009  val_f1=0.700\n",
      "  + Added 28 pseudo-labeled docs (unl pool → 8630 left)\n",
      "Epoch 155 | train_loss=0.000  val_loss=0.009  val_f1=0.699\n",
      "  + Added 9 pseudo-labeled docs (unl pool → 8621 left)\n",
      "Epoch 156 | train_loss=0.000  val_loss=0.009  val_f1=0.699\n",
      "  + Added 32 pseudo-labeled docs (unl pool → 8589 left)\n",
      "Epoch 157 | train_loss=0.000  val_loss=0.009  val_f1=0.700\n",
      "  + Added 17 pseudo-labeled docs (unl pool → 8572 left)\n",
      "Epoch 158 | train_loss=0.000  val_loss=0.009  val_f1=0.700\n",
      "Early stopping at epoch 158 (best f1=0.7014)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import copy\n",
    "\n",
    "\n",
    "\n",
    "doc_ids = np.arange(len(doc_embeddings), dtype=np.int64)   # 0..num_docs-1\n",
    "X = doc_embeddings.astype(np.float32)                      # [num_docs, d_doc]\n",
    "\n",
    "# 라벨 임베딩 세팅\n",
    "label_ids = np.arange(len(label_keys), dtype=np.int64)     # 0..530\n",
    "L = np.vstack([label_embeddings[k] for k in label_keys]).astype(np.float32)   # [531, d_label]\n",
    "\n",
    "\n",
    "# 1) 라벨 순서와 B(부모->자식) 맞추기\n",
    "order = np.argsort(label_ids)\n",
    "label_ids = [label_ids[i] for i in order]\n",
    "L = L[order]\n",
    "assert B.shape == (L.shape[0], L.shape[0]), \"Adjacency/label size mismatch\"\n",
    "\n",
    "# 2) 계층 silver 만들기\n",
    "silver = make_initial_silver_hier(\n",
    "    X,          # docs (N, d)\n",
    "    L,          # label_emb (C, d)\n",
    "    B,          # upper adj (C, C)\n",
    "    roots=roots,\n",
    "    silver_threshold=0.6,\n",
    "    silver_topk=3,\n",
    "    beam=5,\n",
    "    per_parent=\"l+2\",\n",
    "    tau=0.35,\n",
    ")\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 1) 계층 정보에서 parents / children 뽑기\n",
    "#    B[parent, child] = 1 이라고 했으니까 그대로 씀\n",
    "# -------------------------------------------------\n",
    "# B: [C, C] (parent -> child)\n",
    "def build_parents_children(adj):\n",
    "    C = adj.shape[0]\n",
    "    parents = [np.flatnonzero(adj[:, j]).astype(np.int64) for j in range(C)]\n",
    "    children = [np.flatnonzero(adj[j]).astype(np.int64) for j in range(C)]\n",
    "    return parents, children\n",
    "\n",
    "parents, children = build_parents_children(B)\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2) silver → 계층 pos/neg 마스크로 변환\n",
    "# -------------------------------------------------\n",
    "def build_pos_neg_masks(silver, parents, children, num_labels):\n",
    "    \"\"\"\n",
    "    silver: list[list[int]]  # 문서마다 core label index들\n",
    "    parents / children: list[np.ndarray]\n",
    "    return:\n",
    "      pos_masks: np.array [N_docs, C]\n",
    "      neg_masks: np.array [N_docs, C]\n",
    "    \"\"\"\n",
    "    N = len(silver)\n",
    "    C = num_labels\n",
    "    pos_masks = np.zeros((N, C), dtype=np.float32)\n",
    "    neg_masks = np.zeros((N, C), dtype=np.float32)\n",
    "\n",
    "    all_idx = np.arange(C)\n",
    "\n",
    "    for i, core in enumerate(silver):\n",
    "        core = list(core)\n",
    "        # 1) core의 부모까지 positive\n",
    "        pos_set = set(core)\n",
    "        for c in core:\n",
    "            for p in parents[c]:\n",
    "                pos_set.add(int(p))\n",
    "\n",
    "        # 2) children은 나중에 negative에서 제외\n",
    "        child_set = set()\n",
    "        for c in core:\n",
    "            for ch in children[c]:\n",
    "                child_set.add(int(ch))\n",
    "\n",
    "        # pos 마스크\n",
    "        for p in pos_set:\n",
    "            pos_masks[i, p] = 1.0\n",
    "\n",
    "        # neg = 전체 - pos - children\n",
    "        for j in all_idx:\n",
    "            if j in pos_set:\n",
    "                continue\n",
    "            if j in child_set:\n",
    "                continue\n",
    "            neg_masks[i, j] = 1.0\n",
    "\n",
    "    return pos_masks, neg_masks\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3) Dataset: 문서 임베딩 + pos/neg 마스크\n",
    "# -------------------------------------------------\n",
    "class HierMultiLabelDataset(Dataset):\n",
    "    def __init__(self, X, pos_masks, neg_masks, indices=None):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.pos = pos_masks.astype(np.float32)\n",
    "        self.neg = neg_masks.astype(np.float32)\n",
    "        if indices is None:\n",
    "            self.indices = np.arange(self.X.shape[0], dtype=np.int64)\n",
    "        else:\n",
    "            self.indices = np.array(indices, dtype=np.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.indices.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i = int(self.indices[idx])\n",
    "        x = torch.from_numpy(self.X[i])\n",
    "        pos = torch.from_numpy(self.pos[i])\n",
    "        neg = torch.from_numpy(self.neg[i])\n",
    "        return x, pos, neg\n",
    "\n",
    "class UnlabeledDataset(Dataset):\n",
    "    def __init__(self, X, indices):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.indices = np.array(indices, dtype=np.int64)\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    def __getitem__(self, idx):\n",
    "        i = int(self.indices[idx])\n",
    "        return torch.from_numpy(self.X[i]), i\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 4) Bilinear classifier\n",
    "#    doc_emb: [B, d_doc]\n",
    "#    label_emb: [C, d_lab]  (미리 GAT로 만든 거)\n",
    "#    점수: doc @ W @ label_emb^T\n",
    "# -------------------------------------------------\n",
    "class BilinearHierClassifier(nn.Module):\n",
    "    def __init__(self, doc_dim, label_emb, hidden_dim=None):\n",
    "        super().__init__()\n",
    "        # label_emb는 파라미터로 들고있되, 업데이트 안 한다고 가정(원하면 nn.Parameter로)\n",
    "        self.register_buffer(\"label_emb\", torch.tensor(label_emb, dtype=torch.float32))\n",
    "        C, d_lab = self.label_emb.shape\n",
    "        self.doc_dim = doc_dim\n",
    "        self.label_dim = d_lab\n",
    "\n",
    "        if hidden_dim is None:\n",
    "            # 바로 doc_dim -> label_dim\n",
    "            self.interaction = nn.Linear(doc_dim, d_lab, bias=False)\n",
    "            self.proj = None\n",
    "        else:\n",
    "            # doc_dim -> hidden -> label_dim 같은 것도 가능\n",
    "            self.interaction = nn.Sequential(\n",
    "                nn.Linear(doc_dim, hidden_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(hidden_dim, d_lab, bias=False),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [B, d_doc]\n",
    "        return: logits [B, C]\n",
    "        \"\"\"\n",
    "        # x -> same dim as label\n",
    "        h = self.interaction(x)                             # [B, d_lab]\n",
    "        # [B, d_lab] @ [d_lab, C] -> [B, C]\n",
    "        logits = torch.matmul(h, self.label_emb.t())\n",
    "        return logits\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 5) loss: 계층 마스크를 씌운 BCE\n",
    "# -------------------------------------------------\n",
    "def hierarchical_bce_loss(logits, pos_mask, neg_mask):\n",
    "    # logits: [B, C]\n",
    "    # pos_mask, neg_mask: [B, C]\n",
    "    loss_pos = -(pos_mask * F.logsigmoid(logits)).sum()\n",
    "    loss_neg = -(neg_mask * F.logsigmoid(-logits)).sum()\n",
    "    denom = (pos_mask.sum() + neg_mask.sum()).clamp(min=1.0)\n",
    "    return (loss_pos + loss_neg) / denom\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 6) 학습 루프 예시\n",
    "# -------------------------------------------------\n",
    "# 이미 있는 것들: X (문서 BERT 임베딩) : [N_docs, d_doc]\n",
    "#                  L (라벨 GAT 임베딩)  : [C, d_lab]\n",
    "#                  B_adj (부모->자식)   : [C, C]\n",
    "#                  silver (list[list[int]]) : 문서별 core label index\n",
    "def train_epoch_hier(model, loader, opt, device):\n",
    "    model.train()\n",
    "    total = 0.0\n",
    "    for xb, posb, negb in loader:\n",
    "        xb = xb.to(device)\n",
    "        posb = posb.to(device)\n",
    "        negb = negb.to(device)\n",
    "        logits = model(xb)\n",
    "        loss = hierarchical_bce_loss(logits, posb, negb)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        @torch.no_grad()\n",
    "        def update_ema(student, teacher, m):\n",
    "            for p_s, p_t in zip(student.parameters(), teacher.parameters()):\n",
    "                # teacher = m * teacher + (1-m) * student\n",
    "                p_t.data.mul_(m).add_(p_s.data, alpha=1.0 - m)\n",
    "        opt.step()\n",
    "        update_ema(model, model_ema, ema_momentum)\n",
    "        total += loss.item() * xb.size(0)\n",
    "    return total / len(loader.dataset)\n",
    "\n",
    "# 1) micro F1 계산\n",
    "def micro_f1_from_logits(logits, pos_mask, thr=0.5, eps=1e-9):\n",
    "    \"\"\"\n",
    "    logits: [B, C]\n",
    "    pos_mask: [B, C]  (1: positive, 0: else)\n",
    "    \"\"\"\n",
    "    probs = torch.sigmoid(logits)\n",
    "    preds = (probs >= thr).float()\n",
    "\n",
    "    y_true = pos_mask\n",
    "    y_pred = preds\n",
    "\n",
    "    tp = (y_true * y_pred).sum()\n",
    "    fp = ((1 - y_true) * y_pred).sum()\n",
    "    fn = (y_true * (1 - y_pred)).sum()\n",
    "\n",
    "    precision = tp / (tp + fp + eps)\n",
    "    recall    = tp / (tp + fn + eps)\n",
    "    f1 = 2 * precision * recall / (precision + recall + eps)\n",
    "    return f1.item()\n",
    "\n",
    "# 2) eval 함수 수정: loss + f1 둘 다\n",
    "def eval_epoch_hier(model, loader, device, k=3, thr=None):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    f1_list = []\n",
    "    with torch.no_grad():\n",
    "        for xb, posb, negb in loader:\n",
    "            xb = xb.to(device)\n",
    "            posb = posb.to(device)\n",
    "            negb = negb.to(device)\n",
    "\n",
    "            logits = model(xb)\n",
    "            loss = hierarchical_bce_loss(logits, posb, negb)  # 위에 바꾼 버전\n",
    "            total_loss += loss.item() * xb.size(0)\n",
    "\n",
    "            probs = torch.sigmoid(logits)\n",
    "\n",
    "            if thr is not None:\n",
    "                pred = (probs >= thr).float()\n",
    "            else:\n",
    "                # top-k 방식\n",
    "                B, C = probs.shape\n",
    "                pred = torch.zeros_like(probs)\n",
    "                topk = probs.topk(k, dim=1).indices\n",
    "                pred.scatter_(1, topk, 1.0)\n",
    "\n",
    "            # micro-f1\n",
    "            y_true = posb\n",
    "            y_pred = pred\n",
    "            tp = (y_true * y_pred).sum().item()\n",
    "            fp = ((1 - y_true) * y_pred).sum().item()\n",
    "            fn = (y_true * (1 - y_pred)).sum().item()\n",
    "            prec = tp / (tp + fp + 1e-9)\n",
    "            rec  = tp / (tp + fn + 1e-9)\n",
    "            f1   = 2 * prec * rec / (prec + rec + 1e-9)\n",
    "            f1_list.append(f1)\n",
    "\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    avg_f1 = float(np.mean(f1_list)) if f1_list else 0.0\n",
    "    return avg_loss, avg_f1\n",
    "def pseudo_label_and_grow_hier(\n",
    "    model,\n",
    "    unl_ds,             # UnlabeledDataset\n",
    "    X_all,              # 전체 문서 임베딩 (numpy)\n",
    "    parents, children,\n",
    "    num_labels,\n",
    "    device,\n",
    "    pseudo_threshold=0.45,\n",
    "    pseudo_topk=3,\n",
    "    batch_size=512,\n",
    "):\n",
    "    if len(unl_ds) == 0:\n",
    "        return [], None, None\n",
    "\n",
    "    loader = DataLoader(unl_ds, batch_size=batch_size, shuffle=False)\n",
    "    model.eval()\n",
    "    new_idx = []\n",
    "    new_pos_list = []\n",
    "    new_neg_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, idxs in loader:\n",
    "            xb = xb.to(device)\n",
    "            logits = model(xb)\n",
    "            prob = torch.sigmoid(logits).cpu().numpy()\n",
    "\n",
    "            for p, i_doc in zip(prob, idxs.numpy().tolist()):\n",
    "                order = np.argsort(-p)\n",
    "                top1 = p[order[0]]\n",
    "                # 1) top-1이 threshold를 못 넘으면 그냥 버린다\n",
    "                if top1 < pseudo_threshold:\n",
    "                    continue\n",
    "                core = [j for j in order if p[j] >= pseudo_threshold][:pseudo_topk]\n",
    "                if len(core) == 0:\n",
    "                    # 아예 이 문서는 이번 epoch에 안 넣음\n",
    "                    continue\n",
    "\n",
    "                # 계층 pos/neg 구성\n",
    "                pos = set(core)\n",
    "                for c in core:\n",
    "                    for pa in parents[c]:\n",
    "                        pos.add(int(pa))\n",
    "                child = set()\n",
    "                for c in core:\n",
    "                    for ch in children[c]:\n",
    "                        child.add(int(ch))\n",
    "\n",
    "                pos_mask = np.zeros(num_labels, dtype=np.float32)\n",
    "                neg_mask = np.zeros(num_labels, dtype=np.float32)\n",
    "                for j in pos:\n",
    "                    pos_mask[j] = 1.0\n",
    "                for j in range(num_labels):\n",
    "                    if j in pos:    # 이미 양성\n",
    "                        continue\n",
    "                    if j in child:  # 모르겠음 → negative에서 제외\n",
    "                        continue\n",
    "                    neg_mask[j] = 1.0\n",
    "\n",
    "                new_idx.append(int(i_doc))\n",
    "                new_pos_list.append(pos_mask)\n",
    "                new_neg_list.append(neg_mask)\n",
    "\n",
    "\n",
    "\n",
    "    if len(new_idx) == 0:\n",
    "        return [], None, None\n",
    "\n",
    "    new_pos = np.stack(new_pos_list, axis=0)\n",
    "    new_neg = np.stack(new_neg_list, axis=0)\n",
    "    return new_idx, new_pos, new_neg\n",
    "\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "has_silver = np.array([len(lbls) > 0 for lbls in silver], dtype=bool)\n",
    "N_docs = X.shape[0]\n",
    "C = L.shape[0]\n",
    "\n",
    "# silver 있는 문서 / 없는 문서\n",
    "has_silver = np.array([len(lbls) > 0 for lbls in silver], dtype=bool)\n",
    "idx_silver = np.flatnonzero(has_silver)      # 여기가 train/val 후보\n",
    "idx_unl    = np.flatnonzero(~has_silver)     # 진짜 unl\n",
    "\n",
    "print(\"total:\", N_docs)\n",
    "print(\"with silver:\", len(idx_silver))\n",
    "print(\"unlabeled :\", len(idx_unl))\n",
    "\n",
    "# 이제 train/val은 silver 있는 애들만 섞어서 나눈다\n",
    "rng = np.random.default_rng(42)\n",
    "rng.shuffle(idx_silver)\n",
    "n_val = int(len(idx_silver) * 0.2)\n",
    "idx_val   = idx_silver[:n_val]\n",
    "idx_train = idx_silver[n_val:]\n",
    "\n",
    "# parents, children 만들기\n",
    "def build_parents_children(adj):\n",
    "    C = adj.shape[0]\n",
    "    parents = [np.flatnonzero(adj[:, j]).astype(np.int64) for j in range(C)]\n",
    "    children = [np.flatnonzero(adj[j]).astype(np.int64) for j in range(C)]\n",
    "    return parents, children\n",
    "\n",
    "parents, children = build_parents_children(B)\n",
    "\n",
    "pos_masks = np.zeros((N_docs, C), dtype=np.float32)\n",
    "neg_masks = np.zeros((N_docs, C), dtype=np.float32)\n",
    "\n",
    "for i in idx_silver:  # silver 있는 애만 돈다\n",
    "    core = silver[i]\n",
    "\n",
    "    # 1) core + parents\n",
    "    pos = set(core)\n",
    "    for c in core:\n",
    "        for p in parents[c]:\n",
    "            pos.add(int(p))\n",
    "\n",
    "    # 2) children은 모름\n",
    "    child = set()\n",
    "    for c in core:\n",
    "        for ch in children[c]:\n",
    "            child.add(int(ch))\n",
    "\n",
    "    for p in pos:\n",
    "        pos_masks[i, p] = 1.0\n",
    "\n",
    "    for j in range(C):\n",
    "        if j in pos:      # 이미 양성\n",
    "            continue\n",
    "        if j in child:    # 모름\n",
    "            continue\n",
    "        neg_masks[i, j] = 1.0\n",
    "\n",
    "\n",
    "\n",
    "train_ds = HierMultiLabelDataset(X, pos_masks, neg_masks, indices=idx_train)\n",
    "val_ds   = HierMultiLabelDataset(X, pos_masks, neg_masks, indices=idx_val) if len(idx_val) > 0 else None\n",
    "unl_ds   = UnlabeledDataset(X, idx_unl.tolist())\n",
    "print(len(train_ds),len(val_ds),len(unl_ds))\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=256, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=256, shuffle=False)\n",
    "\n",
    "model = BilinearHierClassifier(doc_dim=X.shape[1], label_emb=L, hidden_dim=256).to(device)\n",
    "model_ema = copy.deepcopy(model).to(device)\n",
    "for p in model_ema.parameters():\n",
    "    p.requires_grad = False  # teacher는 gradient 안 받음\n",
    "\n",
    "ema_momentum = 0.99  # 또는 0.997, 0.999 등\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "epochs = 500\n",
    "\n",
    "N_labels = L.shape[0]\n",
    "best_f1 = -1.0\n",
    "patience = 20\n",
    "no_improve = 0\n",
    "warmup_self = 1   # 1 epoch은 self-training 안 하게 해서 한 번 안정화\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    # train\n",
    "    tr_loss = train_epoch_hier(model, train_loader, opt, device)\n",
    "\n",
    "    # val: f1 기준\n",
    "    if val_loader is not None and len(val_ds) > 0:\n",
    "        va_loss, va_f1 = eval_epoch_hier(model, val_loader, device, k=3)\n",
    "        print(f\"Epoch {epoch:03d} | train_loss={tr_loss:.3f}  val_loss={va_loss:.3f}  val_f1={va_f1:.3f}\")\n",
    "\n",
    "        # early stopping을 f1로\n",
    "        if va_f1 > best_f1 + 1e-6:\n",
    "            best_f1 = va_f1\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch} (best f1={best_f1:.4f})\")\n",
    "                break\n",
    "    else:\n",
    "        print(f\"Epoch {epoch:03d} | train_loss={tr_loss:.3f}\")\n",
    "\n",
    "    # self-training: 1에폭에 전부 들어가는 거 방지용으로 warmup 넣음\n",
    "    if epoch <= warmup_self:\n",
    "        print(\"  + (skip pseudo-labeling on warmup epoch)\")\n",
    "        continue\n",
    "\n",
    "    new_idx, new_pos, new_neg = pseudo_label_and_grow_hier(\n",
    "        model,\n",
    "        unl_ds,\n",
    "        X,\n",
    "        parents,\n",
    "        children,\n",
    "        C,                   # num_labels\n",
    "        device=device,\n",
    "        pseudo_threshold=0.45,\n",
    "        pseudo_topk=3,\n",
    "        batch_size=512,\n",
    "    )\n",
    "\n",
    "    if len(new_idx) > 0:\n",
    "        # 전역 마스크 갱신\n",
    "        pos_masks[new_idx] = new_pos\n",
    "        neg_masks[new_idx] = new_neg\n",
    "\n",
    "        # unl에서 제거\n",
    "        keep_mask = ~np.isin(unl_ds.indices, np.array(new_idx, dtype=np.int64))\n",
    "        unl_ds.indices = unl_ds.indices[keep_mask]\n",
    "\n",
    "        # train에 추가\n",
    "        train_ds.indices = np.concatenate([train_ds.indices, np.array(new_idx, dtype=np.int64)])\n",
    "        train_loader = DataLoader(train_ds, batch_size=256, shuffle=True, drop_last=False)\n",
    "\n",
    "        print(f\"  + Added {len(new_idx)} pseudo-labeled docs (unl pool → {len(unl_ds)} left)\")\n",
    "    else:\n",
    "        print(\"  + No pseudo-labeled docs added this epoch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec157776-e172-4c16-93a1-aacdf0a962d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd03ade-045a-4c94-89e4-de3d1e523595",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b88df0-bac0-49b4-9f58-d0c5e77583e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574d819b-4455-4b10-8541-4a9a9f2ddec4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7411b961-6f29-4ac7-ba7a-97098db48b1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T07:59:56.726641Z",
     "iopub.status.busy": "2025-11-13T07:59:56.726487Z",
     "iopub.status.idle": "2025-11-13T07:59:58.161711Z",
     "shell.execute_reply": "2025-11-13T07:59:58.160834Z",
     "shell.execute_reply.started": "2025-11-13T07:59:56.726627Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv, os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ------------ Paths (edit if needed) ------------\n",
    "TEST_CORPUS = \"Amazon_products/test/test_corpus.txt\"   # lines: pid \\t text\n",
    "OUT_PATH    = \"submission_bda.csv\"\n",
    "# ------------ Hyperparams ------------\n",
    "MIN_LABS  = 2\n",
    "MAX_LABS  = 3\n",
    "BATCH = 1024\n",
    "doc_ids, doc_texts = load_docs_txt(TEST_CORPUS)\n",
    "\n",
    "# 2) 라벨 때 만든 vectorizer 재사용해서 임베딩 만들기\n",
    "test_embeddings = build_doc_embeddings_from_existing_vectorizer(doc_texts, vectorizer)\n",
    "test_embeddings = test_embeddings.astype(np.float32)   # [num_test, d]\n",
    "# load test pids\n",
    "pids = doc_ids   # 이미 문자열 id\n",
    "if \"L\" in globals():\n",
    "    if not isinstance(L, np.ndarray):\n",
    "        # 예: L이 torch.Tensor인 경우\n",
    "        L = L.detach().cpu().numpy().astype(np.float32)\n",
    "else:\n",
    "    raise ValueError(\"라벨 임베딩 L이 메모리에 없어! GAT 끝난 뒤의 임베딩을 L로 둬야 해.\")\n",
    "\n",
    "# 5) 라벨 id는 0..N-1로 생성 (네가 말한 대로 adjacency랑 순서가 이미 맞다고 했으니까)\n",
    "lab_ids = np.arange(L.shape[0], dtype=np.int64)\n",
    "\n",
    "# 6) adjacency도 메모리에 있는 걸 그대로 쓴다\n",
    "#    여기서 A는 531x531 같은 numpy array라고 가정\n",
    "assert B.shape == (L.shape[0], L.shape[0]), \"Adjacency/label size mismatch\"\n",
    "\n",
    "# 7) children 리스트 미리 만들어두기\n",
    "children = [np.flatnonzero(B[i]) for i in range(B.shape[0])]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ef344a9-c996-4932-b7ab-9f7c5e7578b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T07:59:58.162462Z",
     "iopub.status.busy": "2025-11-13T07:59:58.162314Z",
     "iopub.status.idle": "2025-11-13T08:00:12.776326Z",
     "shell.execute_reply": "2025-11-13T08:00:12.775731Z",
     "shell.execute_reply.started": "2025-11-13T07:59:58.162447Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: submission_bda.csv | samples=19658 | min-max labels per sample=2-3 | missing_pids=0\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "def ancestors_of(node, adj):\n",
    "    # adj[parent, child] = 1 가정\n",
    "    parents = np.flatnonzero(adj[:, node])  # (N,)\n",
    "    return parents.tolist()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "IN_DIM = test_embeddings.shape[1]\n",
    "missing = 0  # 지금은 쓸 일 없지만 원래 코드랑 형태 맞춰둠\n",
    "\n",
    "with open(OUT_PATH, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow([\"id\", \"label\"])\n",
    "\n",
    "    buf_x, buf_pid = [], []\n",
    "\n",
    "    def flush():\n",
    "        if not buf_x:\n",
    "            return\n",
    "        xb = torch.from_numpy(np.stack(buf_x, axis=0).astype(np.float32)).to(device)\n",
    "        with torch.inference_mode():\n",
    "            prob = torch.sigmoid(model(xb)).detach().cpu().numpy()\n",
    "        prob = np.nan_to_num(prob, nan=-1.0, posinf=1.0, neginf=0.0)\n",
    "\n",
    "        for pid, p in zip(buf_pid, prob):\n",
    "            order = np.argsort(-p)\n",
    "\n",
    "            # 1) 기본 후보 뽑기\n",
    "            thr_keep = [i for i in order if p[i] >= 0.5][:MAX_LABS]\n",
    "            if len(thr_keep) >= MIN_LABS:\n",
    "                keep = thr_keep[:MAX_LABS]\n",
    "            else:\n",
    "                keep = order[:max(MIN_LABS, len(thr_keep))]\n",
    "                if len(keep) < MIN_LABS:\n",
    "                    keep = order[:MIN_LABS]\n",
    "\n",
    "            # 2) 부모 후보\n",
    "            parent_cands = []\n",
    "            for c in keep:\n",
    "                pars = ancestors_of(c, B)\n",
    "                for pa in pars:\n",
    "                    if pa not in keep and pa not in parent_cands:\n",
    "                        parent_cands.append(pa)\n",
    "\n",
    "            parent_cands.sort(key=lambda idx: p[idx], reverse=True)\n",
    "\n",
    "            # 3) 남는 슬롯 부모로 채우기\n",
    "            final_idxs = list(keep)\n",
    "            for pa in parent_cands:\n",
    "                if len(final_idxs) >= MAX_LABS:\n",
    "                    break\n",
    "                final_idxs.append(pa)\n",
    "\n",
    "            # 4) 그래도 모자라면 확률순\n",
    "            if len(final_idxs) < MIN_LABS:\n",
    "                for idx in order:\n",
    "                    if idx not in final_idxs:\n",
    "                        final_idxs.append(idx)\n",
    "                    if len(final_idxs) >= MIN_LABS:\n",
    "                        break\n",
    "\n",
    "            labels = sorted(int(lab_ids[i]) for i in final_idxs)\n",
    "            w.writerow([pid, \",\".join(map(str, labels))])\n",
    "\n",
    "        buf_x.clear()\n",
    "        buf_pid.clear()\n",
    "\n",
    "    # 여기서 바로 pids와 test_embeddings를 같이 순회\n",
    "    for pid, emb in zip(pids, test_embeddings):\n",
    "        x = emb\n",
    "        if x.dtype != np.float32:\n",
    "            x = x.astype(np.float32, copy=False)\n",
    "        buf_x.append(x)\n",
    "        buf_pid.append(pid)\n",
    "        if len(buf_x) >= BATCH:\n",
    "            flush()\n",
    "    flush()\n",
    "\n",
    "print(f\"Saved: {OUT_PATH} | samples={len(pids)} | min-max labels per sample={MIN_LABS}-{MAX_LABS} | missing_pids={missing}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e293bbf-8597-4c2c-91e1-b03ce7477217",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b72c5b-1d7a-4d59-a975-e7fa22dfbb32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a677853-591b-488e-835f-cf4b4c164c1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343dfe98-d91d-4c90-9325-ec5a6ff5c560",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49cebb9-e667-46aa-8c96-64a63c801ab3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d535af4-4d65-4127-b625-caac88456b68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b2e45b-844d-437a-9fc5-1d47ad4a9832",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a133dad-b306-4349-b9d3-073022f3eb43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba53595b-f997-468a-a4f9-6deee2dc1a08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb8152f-64ca-44fc-a37b-4ab46e4291d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7419c2a8-6739-4ed6-b9ee-3eeb108873de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-10T13:08:37.137152Z",
     "iopub.status.busy": "2025-11-10T13:08:37.136915Z",
     "iopub.status.idle": "2025-11-10T13:08:37.225118Z",
     "shell.execute_reply": "2025-11-10T13:08:37.224606Z",
     "shell.execute_reply.started": "2025-11-10T13:08:37.137137Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating dummy predictions: 100%|██████████| 19658/19658 [00:00<00:00, 419908.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy submission file saved to: submission.csv\n",
      "Total samples: 19658, Classes per sample: 1-3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# ------------------------\n",
    "# Dummy baseline for Kaggle submission\n",
    "# Generates random multi-label predictions\n",
    "# ------------------------\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Paths ---\n",
    "TEST_DIR = \"Amazon_products/test\"  # modify if needed\n",
    "TEST_CORPUS_PATH = os.path.join(TEST_DIR, \"test_corpus.txt\")  # product_id \\t text\n",
    "SUBMISSION_PATH = \"submission.csv\"  # output file\n",
    "\n",
    "# --- Constants ---\n",
    "NUM_CLASSES = 531  # total number of classes (0–530)\n",
    "MIN_LABELS = 1     # minimum number of labels per sample\n",
    "MAX_LABELS = 3     # maximum number of labels per sample\n",
    "\n",
    "# --- Load test corpus ---\n",
    "def load_corpus(path):\n",
    "    \"\"\"Load test corpus into {pid: text} dictionary.\"\"\"\n",
    "\"\"\"\n",
    "    pid2text = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\", 1)\n",
    "            if len(parts) == 2:\n",
    "                pid, text = parts\n",
    "                pid2text[pid] = text\n",
    "    return pid2text\n",
    "\n",
    "pid2text_test = load_corpus(TEST_CORPUS_PATH)\n",
    "pid_list_test = list(pid2text_test.keys())\n",
    "\n",
    "# --- Generate random predictions ---\n",
    "all_pids, all_labels = [], []\n",
    "for pid in tqdm(pid_list_test, desc=\"Generating dummy predictions\"):\n",
    "    n_labels = random.randint(MIN_LABELS, MAX_LABELS)\n",
    "    labels = random.sample(range(NUM_CLASSES), n_labels)\n",
    "    labels = sorted(labels)\n",
    "    all_pids.append(pid)\n",
    "    all_labels.append(labels)\n",
    "\n",
    "# --- Save submission file ---\n",
    "with open(SUBMISSION_PATH, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"id\", \"label\"])\n",
    "    for pid, labels in zip(all_pids, all_labels):\n",
    "        writer.writerow([pid, \",\".join(map(str, labels))])\n",
    "\n",
    "print(f\"Dummy submission file saved to: {SUBMISSION_PATH}\")\n",
    "print(f\"Total samples: {len(all_pids)}, Classes per sample: {MIN_LABELS}-{MAX_LABELS}\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5d29a7-c5b4-4a26-954b-b65a737f632c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
