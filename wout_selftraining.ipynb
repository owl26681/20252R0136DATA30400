{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUCtw9OESD4v"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krt4u0kiKEpp"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed_all(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "id": "PE8ceBET4BNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tBJPx97UPnm"
      },
      "outputs": [],
      "source": [
        "# Cell 1: imports & config\n",
        "\n",
        "import os\n",
        "import json\n",
        "import copy\n",
        "import math\n",
        "import queue\n",
        "\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity as cos\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "# 하이퍼파라미터 (원하면 튜닝)\n",
        "# -----------------------\n",
        "TOP_K = 3          # 문서당 최대 core class 개수\n",
        "MIN_PROB = 0.05    # softmax 확률 기준 최소값 (너무 애매한 라벨 컷)\n",
        "USE_MARGIN = True\n",
        "MARGIN_DELTA = 0.02  # 1등과 2등의 확률 차이가 이보다 크면 1등만 뽑도록 stop\n",
        "\n",
        "# -----------------------\n",
        "# 데이터 경로 설정 (여기만 네 상황에 맞게 수정)\n",
        "# -----------------------\n",
        "DATA_DIR = \"Amazon_products\"      # 예: \"/content/data\"\n",
        "DATASET = \"Amazon_products/train/train_corpus.txt\"          # document\n",
        "GPU = 0                         # cuda 디바이스 번호\n",
        "\n",
        "# output 파일 이름 (원본과 헷갈리지 않게)\n",
        "print(\"DATA_DIR:\", DATA_DIR)\n",
        "print(\"DATASET:\", DATASET)\n",
        "print(\"GPU:\", GPU)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhoTe5paZa8v"
      },
      "outputs": [],
      "source": [
        "# Cell: Node 클래스 + 그래프 생성 함수\n",
        "\n",
        "class Node:\n",
        "    def __init__(self, node_id, name):\n",
        "        self.node_id = str(node_id)       # \"0\", \"1\", ...\n",
        "        self.name = name                  # \"grocery_gourmet_food\"\n",
        "        self.childs = []                  # List[Node]\n",
        "        self.parents = []                 # List[Node]\n",
        "        self.path_score = 0.0\n",
        "        self.similarity_score = 0.0\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Node(id={self.node_id}, name={self.name})\"\n",
        "\n",
        "\n",
        "def build_graph_from_files(label_file, edge_file):\n",
        "    \"\"\"\n",
        "    label_file: \"id<tab>label_name\"\n",
        "    edge_file : \"parent_id<tab>child_id\"\n",
        "    \"\"\"\n",
        "    id2label = {}\n",
        "    label2id = {}\n",
        "    id2node = {}\n",
        "\n",
        "    # 1) 라벨 파일 읽기\n",
        "    with open(label_file, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            parts = line.split(\"\\t\")\n",
        "            if len(parts) != 2:\n",
        "                continue\n",
        "            idx, name = parts\n",
        "            idx = str(idx)\n",
        "            name = name.strip()\n",
        "            id2label[idx] = name\n",
        "            label2id[name] = idx\n",
        "            id2node[idx] = Node(idx, name)\n",
        "\n",
        "    # 2) 부모-자식 엣지 파일 읽기\n",
        "    with open(edge_file, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            parts = line.split(\"\\t\")\n",
        "            if len(parts) != 2:\n",
        "                continue\n",
        "            p, c = parts\n",
        "            p = str(p); c = str(c)\n",
        "            if p not in id2node or c not in id2node:\n",
        "                continue\n",
        "            parent = id2node[p]\n",
        "            child = id2node[c]\n",
        "            parent.childs.append(child)\n",
        "            child.parents.append(parent)\n",
        "\n",
        "    # 3) 부모가 없는 노드들 = top-level 루트들\n",
        "    roots = [n for n in id2node.values() if len(n.parents) == 0]\n",
        "\n",
        "    if len(roots) == 1:\n",
        "        root = roots[0]\n",
        "    else:\n",
        "        # top-level이 여러 개면 슈퍼루트 하나 만들어서 모두 연결\n",
        "        root = Node(\"-1\", \"ROOT\")\n",
        "        for r in roots:\n",
        "            root.childs.append(r)\n",
        "            r.parents.append(root)\n",
        "\n",
        "    print(f\"#labels: {len(id2label)}, #roots(before super-root): {len(roots)}\")\n",
        "    return root, id2label, label2id, id2node\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Licy3UDdZuRe"
      },
      "outputs": [],
      "source": [
        "import mmap\n",
        "def get_num_lines(file_path):\n",
        "    fp = open(file_path, \"r+\")\n",
        "    buf = mmap.mmap(fp.fileno(), 0)\n",
        "    lines = 0\n",
        "    while buf.readline():\n",
        "        lines += 1\n",
        "    return lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dk5N01RQUOnD"
      },
      "outputs": [],
      "source": [
        "# Cell 2: graph, corpus, embedding 준비\n",
        "\n",
        "# 1) 라벨 키워드 로드 (llm_enrichment.txt)\n",
        "enriched_file = os.path.join(DATA_DIR, \"class_related_keywords.txt\")\n",
        "label_keyterm_dict = {}\n",
        "\n",
        "with open(enriched_file, encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        components = line.split(\":\")\n",
        "        node = components[0]          # label_name (with underscore)\n",
        "        keywords = components[1]\n",
        "        keyword_list = [k for k in keywords.split(\",\") if k]\n",
        "        label_keyterm_dict[node] = keyword_list\n",
        "\n",
        "print(\"num labels with keywords:\", len(label_keyterm_dict))\n",
        "\n",
        "# 2) SentenceTransformer 모델 로드\n",
        "model_name = \"Alibaba-NLP/gte-base-en-v1.5\"\n",
        "model = SentenceTransformer(model_name, device=f\"cuda:{GPU}\",trust_remote_code=True)\n",
        "print(\"Loaded SentenceTransformer:\", model_name)\n",
        "\n",
        "# 3) taxonomy 그래프 로드\n",
        "LABEL_FILE = os.path.join(DATA_DIR, \"classes.txt\")      # 예시 이름\n",
        "EDGE_FILE  = os.path.join(DATA_DIR, \"class_hierarchy.txt\") # 예시 이름\n",
        "\n",
        "root, id2label, label2id, id2node = build_graph_from_files(LABEL_FILE, EDGE_FILE)\n",
        "\n",
        "num_class = len(id2label)\n",
        "print(\"num_class:\", num_class)\n",
        "# 4) corpus.txt 로드: \"doc_id \\t text\"\n",
        "corpus_path = os.path.join(DATASET)\n",
        "num_line = get_num_lines(corpus_path)\n",
        "\n",
        "all_docs = []\n",
        "all_docs_id = []\n",
        "\n",
        "with open(corpus_path, encoding=\"utf-8\") as f:\n",
        "    for i, line in tqdm(enumerate(f), total=num_line):\n",
        "        line = line.rstrip(\"\\n\")\n",
        "        if not line:\n",
        "            continue\n",
        "        doc_id, doc = line.split(\"\\t\", 1)\n",
        "        all_docs.append(doc)\n",
        "        all_docs_id.append(doc_id)\n",
        "\n",
        "print(\"num_docs:\", len(all_docs))\n",
        "\n",
        "# 5) 모든 문서 임베딩 계산\n",
        "with torch.no_grad():\n",
        "    total_doc_embedding = model.encode(\n",
        "        all_docs,\n",
        "        batch_size=128,\n",
        "        show_progress_bar=True,\n",
        "        convert_to_numpy=True,\n",
        "    )\n",
        "\n",
        "# 6) 각 라벨의 key-term 임베딩 계산\n",
        "# 6) 각 라벨의 key-term 임베딩 계산\n",
        "key_term_emb_dict = {}\n",
        "for i in tqdm(range(num_class)):\n",
        "    current_label = id2label[str(i)]           # 예: \"hair_care\"\n",
        "\n",
        "    # 임베딩에 쓸 label 텍스트에서만 _ → 공백\n",
        "    label_text = current_label.replace(\"_\", \" \")\n",
        "\n",
        "    # label 본인(공백 버전) + keyterms(원문 그대로, _ 유지)\n",
        "    current_key = [label_text] + label_keyterm_dict[current_label]\n",
        "\n",
        "    current_embed = model.encode(\n",
        "        current_key,\n",
        "        batch_size=128,\n",
        "        convert_to_numpy=True,\n",
        "    )  # (num_keyterms_for_label, dim)\n",
        "\n",
        "    # key_term_emb_dict의 키는 기존 label 그대로 (underscore 버전) 유지\n",
        "    key_term_emb_dict[current_label] = current_embed\n",
        "\n",
        "print(\"built key_term_emb_dict for all labels\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLrwx1VhaVS9"
      },
      "outputs": [],
      "source": [
        "# Cell 4: adjacency matrix + parents/siblings\n",
        "import json\n",
        "import numpy as np\n",
        "adj_upper = np.zeros((num_class, num_class), dtype=np.int32)\n",
        "\n",
        "with open(EDGE_FILE, encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        p_str, c_str = line.split(\"\\t\")\n",
        "        p = int(p_str)\n",
        "        c = int(c_str)\n",
        "        adj_upper[p, c] = 1\n",
        "\n",
        "\n",
        "print(\"adj_upper shape:\", adj_upper.shape)\n",
        "\n",
        "# ----- 위에서 이미 정의했던 함수들을 그대로 사용 -----\n",
        "import numpy as np\n",
        "\n",
        "def build_parents_children(adj: np.ndarray):\n",
        "    C = adj.shape[0]\n",
        "    parents = [np.flatnonzero(adj[:, j]).astype(np.int64) for j in range(C)]\n",
        "    children = [np.flatnonzero(adj[j]).astype(np.int64) for j in range(C)]\n",
        "    return parents, children\n",
        "\n",
        "def build_siblings(parents, children):\n",
        "    C = len(parents)\n",
        "    sibs = [set() for _ in range(C)]\n",
        "    for c in range(C):\n",
        "        for p in parents[c]:\n",
        "            for ch in children[p]:\n",
        "                if ch != c:\n",
        "                    sibs[c].add(int(ch))\n",
        "    sibs = [np.array(sorted(s), dtype=np.int64) for s in sibs]\n",
        "    return sibs\n",
        "\n",
        "parents, children = build_parents_children(adj_upper)\n",
        "siblings = build_siblings(parents, children)\n",
        "roots = [i for i, ps in enumerate(parents) if len(ps) == 0]\n",
        "\n",
        "print(\"built parents & siblings\")\n",
        "print(roots)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXxxJxx3JKgv"
      },
      "outputs": [],
      "source": [
        "# 2.99 시작  3.05끝  0.06\n",
        "# 257 call"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D29KWsue48hV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv(\"Amazon_products/run_20251218T131003Z/label_docs.csv\")\n",
        "\n",
        "# cid -> doc 맵\n",
        "cid2doc = df.set_index(\"cid\")[\"doc\"].to_dict()\n",
        "\n",
        "# num_labels(=C)가 있으면 0..C-1 순서로 정렬\n",
        "gpt_docs = [cid2doc[i] for i in range(0,531)]  # (C,)\n",
        "\n",
        "# 임베딩\n",
        "label_emb_gpt = model.encode(\n",
        "    gpt_docs,\n",
        "    batch_size=128,\n",
        "    convert_to_numpy=True,\n",
        "    normalize_embeddings=True,\n",
        "    show_progress_bar=True\n",
        ").astype(np.float32)\n",
        "\n",
        "print(\"label_emb_gpt:\", label_emb_gpt.shape)  # (C, d)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_kJ30sNZ0T9"
      },
      "outputs": [],
      "source": [
        "# Cell 3: label_emb 만들기 (라벨별 key-term 평균)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "dim = next(iter(key_term_emb_dict.values())).shape[1]\n",
        "label_emb = np.zeros((num_class, dim), dtype=np.float32)\n",
        "\n",
        "for i in range(num_class):\n",
        "    label_name = id2label[str(i)]  # 예: \"hair_care\"\n",
        "    term_emb = key_term_emb_dict[label_name]  # (num_terms, dim)\n",
        "    label_emb[i] = term_emb.mean(axis=0)\n",
        "\n",
        "print(\"label_emb shape:\", label_emb.shape)  # (num_class, dim)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeXUuOqRaGL7"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhNi4mRkpN16"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model_name = \"Alibaba-NLP/gte-base-en-v1.5\"\n",
        "st_model = SentenceTransformer(model_name, device=f\"cuda:{GPU}\",trust_remote_code=True)\n",
        "\n",
        "with torch.no_grad():\n",
        "    total_doc_emb = st_model.encode(\n",
        "        all_docs,\n",
        "        batch_size=128,\n",
        "        convert_to_tensor=True,\n",
        "        show_progress_bar=True,\n",
        "    ).cpu().numpy()   # [N_docs, d_doc]\n",
        "\n",
        "print(\"total_doc_emb:\", total_doc_emb.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80wo8BK-rapQ"
      },
      "outputs": [],
      "source": [
        "# 이미 이전 셀/파일에 있다고 가정하는 함수들\n",
        "# - hierarchical_beam_similarity_avg\n",
        "# - compute_conf_doc\n",
        "\n",
        "def compute_conf_doc(\n",
        "    sims: np.ndarray,\n",
        "    cand: np.ndarray,\n",
        "    parents,\n",
        "    siblings,\n",
        ") -> np.ndarray:\n",
        "    conf = np.empty_like(cand, dtype=np.float32)\n",
        "    for k, c in enumerate(cand):\n",
        "        par = parents[c]\n",
        "        sib = siblings[c]\n",
        "        neigh_vals = []\n",
        "\n",
        "        if len(par) > 0:\n",
        "            neigh_vals.append(sims[par].max())\n",
        "        if len(sib) > 0:\n",
        "            neigh_vals.append(sims[sib].max())\n",
        "\n",
        "        neigh_max = max(neigh_vals) if neigh_vals else 0.0\n",
        "        conf[k] = float(sims[c] - neigh_max)\n",
        "    return conf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opTG1VXEqOmp"
      },
      "outputs": [],
      "source": [
        "# ================================================\n",
        "# (사전 준비) label_term_emb / label_emb 만들기\n",
        "#   - label_term_emb: (C, 11, d)  [0]=label_name, [1:]=keywords(10)\n",
        "#   - label_emb: (C, d)  term 평균\n",
        "# ================================================\n",
        "import numpy as np\n",
        "\n",
        "K = 10\n",
        "T = 1 + K\n",
        "\n",
        "dim = next(iter(key_term_emb_dict.values())).shape[1]\n",
        "\n",
        "label_term_emb = np.zeros((num_class, T, dim), dtype=np.float32)\n",
        "label_emb      = np.zeros((num_class, dim), dtype=np.float32)\n",
        "\n",
        "stack = []\n",
        "for i in range(num_class):\n",
        "    label_name = id2label[str(i)]                  # underscore 버전\n",
        "    term_emb = key_term_emb_dict[label_name].astype(np.float32)  # (11, d) 기대\n",
        "\n",
        "    if term_emb.shape[0] != T:\n",
        "        stack.append(i)\n",
        "        continue\n",
        "        raise ValueError(f\"[{label_name}] term count = {term_emb.shape[0]} (expected {T}), {i}\")\n",
        "\n",
        "    label_term_emb[i] = term_emb\n",
        "    label_emb[i] = term_emb.mean(axis=0)\n",
        "\n",
        "print(\"label_term_emb:\", label_term_emb.shape)  # (C, 11, d)\n",
        "print(\"label_emb:\", label_emb.shape)            # (C, d)\n",
        "print(stack)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twX58TyxTgG4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYev1QeNp_tK"
      },
      "outputs": [],
      "source": [
        "# 5-1. node score similarity (alpha * label_name + beta * keywords_agg)\n",
        "\n",
        "EPS = 1e-12\n",
        "\n",
        "# ===== 하이퍼파라미터 =====\n",
        "ALPHA = 1.0\n",
        "BETA  = 1.0\n",
        "KW_AGG = \"topk\"   # \"topk\" or \"mean\"\n",
        "K_TOP  = 3        # KW_AGG=\"topk\"일 때만 사용\n",
        "LABEL_CHUNK = 256\n",
        "\n",
        "# ===== doc embedding =====\n",
        "doc_emb = total_doc_emb.astype(np.float32)  # (N_docs, d)\n",
        "doc_emb_cos = doc_emb / (np.linalg.norm(doc_emb, axis=1, keepdims=True) + EPS)\n",
        "\n",
        "# ===== label_term_emb 준비되어 있어야 함: (C, 11, d), [0]=label_name, [1:]=keywords(10)\n",
        "# label_term_emb가 아직 없으면, 미리 만들어둔 걸 사용하거나 생성해두면 됨.\n",
        "lab_term = label_term_emb.astype(np.float32)\n",
        "lab_term_cos = lab_term / (np.linalg.norm(lab_term, axis=2, keepdims=True) + EPS)\n",
        "\n",
        "N_docs = doc_emb_cos.shape[0]\n",
        "C = lab_term_cos.shape[0]\n",
        "K = lab_term_cos.shape[1] - 1  # 10\n",
        "\n",
        "sim_matrix = np.zeros((N_docs, C), dtype=np.float32)\n",
        "\n",
        "label_emb_gpt = label_emb_gpt.astype(np.float32)\n",
        "label_emb_gpt_cos = label_emb_gpt / (np.linalg.norm(label_emb_gpt, axis=1, keepdims=True) + EPS)\n",
        "\n",
        "for s in range(0, C, LABEL_CHUNK):\n",
        "    e = min(C, s + LABEL_CHUNK)\n",
        "    chunk = lab_term_cos[s:e]  # (Lc, 11, d)\n",
        "    Lc = e - s\n",
        "\n",
        "    # label-name cosine: (N, Lc)\n",
        "    sim_name = doc_emb_cos @ chunk[:, 0, :].T\n",
        "\n",
        "    # keyword cosine들: (N, Lc, K)\n",
        "    kw = chunk[:, 1:, :]  # (Lc, 10, d)\n",
        "    sim_kw_all = np.einsum(\"nd,lkd->nlk\", doc_emb_cos, kw)\n",
        "\n",
        "    # keyword aggregate: (N, Lc)\n",
        "    if KW_AGG == \"mean\":\n",
        "        sim_kw = sim_kw_all.mean(axis=2)\n",
        "    elif KW_AGG == \"topk\":\n",
        "        k = min(K_TOP, sim_kw_all.shape[2])\n",
        "        topk = np.partition(sim_kw_all, kth=sim_kw_all.shape[2]-k, axis=2)[:, :, -k:]\n",
        "        sim_kw = topk.mean(axis=2)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown KW_AGG={KW_AGG}\")\n",
        "\n",
        "    # ---- (기존) node score ----\n",
        "    sim_node = ALPHA * sim_name + BETA * sim_kw   # (N, Lc)\n",
        "\n",
        "    # ---- (추가) gpt-doc emb 내적(=cosine) 점수 ----\n",
        "    sim_gpt = doc_emb_cos @ label_emb_gpt_cos[s:e].T  # (N, Lc)\n",
        "\n",
        "    # ---- (변경) 두 점수 평균 ----\n",
        "    sim_matrix[:, s:e] = 0.7*sim_node + 0.3* sim_gpt\n",
        "\n",
        "print(\"sim_matrix(node+gpt avg):\", sim_matrix.shape,\n",
        "      \"range:\", float(sim_matrix.min()), \"~\", float(sim_matrix.max()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amthg3irIldD"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 5-2. path(c) = root → ... → c 경로 만들기 (parents 리스트 사용)\n",
        "def build_paths_from_parents(parents_list):\n",
        "    \"\"\"\n",
        "    parents_list[c]: np.array of parent indices for class c\n",
        "    return: paths[c] = [root, ..., c]\n",
        "    \"\"\"\n",
        "    C = len(parents_list)\n",
        "    paths = {}\n",
        "\n",
        "    for cid in range(C):\n",
        "        path = []\n",
        "        cur = cid\n",
        "        visited = set()\n",
        "\n",
        "        while True:\n",
        "            path.append(cur)\n",
        "            visited.add(cur)\n",
        "\n",
        "            # 부모 없으면 루트\n",
        "            if len(parents_list[cur]) == 0:\n",
        "                break\n",
        "\n",
        "            # 부모가 여러 개일 수 있지만, 일단 첫 번째만 사용 (tree 가정)\n",
        "            par = int(parents_list[cur][0])\n",
        "            if par in visited:   # 안전 장치 (cycle 방지)\n",
        "                break\n",
        "            cur = par\n",
        "\n",
        "        # root → ... → cid 순서로 뒤집기\n",
        "        paths[cid] = list(reversed(path))\n",
        "\n",
        "    return paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqyz_T9sE4Rd"
      },
      "outputs": [],
      "source": [
        "\n",
        "paths = build_paths_from_parents(parents)\n",
        "print(\"예시 path[0]:\", paths[0], \"->\", [id2label[str(i)] for i in paths[0]])\n",
        "\n",
        "\n",
        "# 5-3. path score 계산\n",
        "# path score(d, c) = 평균_{j ∈ path(c)} sim_matrix[d, j]\n",
        "N_docs, C = sim_matrix.shape\n",
        "path_scores = np.zeros_like(sim_matrix, dtype=np.float32)\n",
        "\n",
        "for cid, path in paths.items():\n",
        "    vals = sim_matrix[:, path]          # (N_docs, len(path))\n",
        "    path_scores[:, cid] = vals.mean(axis=1)\n",
        "\n",
        "print(\"path_scores:\", path_scores.shape)\n",
        "\n",
        "\n",
        "# 5-4. 각 문서마다 core class 선택 (Top-k + confidence 반영)\n",
        "TOP_K_CORE   = 2      # 문서당 core class 최대 개수 (원하면 조정)\n",
        "MIN_SCORE    = None   # 필요하면 path score threshold (예: 0.2)\n",
        "CAND_TOP     = 50     # confidence 계산에 쓸 후보 개수\n",
        "CONF_ALPHA   = 0.2    # path score에 confidence를 얼마나 섞을지\n",
        "CONF_MIN     = 0.0   # 예: 0.0 으로 두고 conf <= 0인 애들 컷하고 싶으면 사용\n",
        "\n",
        "core_classes_per_doc = []      # 문서별 core class id 리스트\n",
        "core_primary         = np.full(N_docs, -1, dtype=np.int32)   # 문서당 대표 core (1개)\n",
        "core_primary_score   = np.zeros(N_docs, dtype=np.float32)\n",
        "\n",
        "for i in range(N_docs):\n",
        "    path_row = path_scores[i]    # (C,) path score(d, c)\n",
        "    sim_row  = sim_matrix[i]     # (C,) local similarity(d, c) — confidence 계산용\n",
        "\n",
        "    # 1) path score 기준 1차 정렬\n",
        "    idx_sorted = np.argsort(-path_row)\n",
        "\n",
        "    # (옵션) path score 전역 threshold\n",
        "    if MIN_SCORE is not None:\n",
        "        idx_sorted = [cid for cid in idx_sorted if path_row[cid] >= MIN_SCORE]\n",
        "\n",
        "    if len(idx_sorted) == 0:\n",
        "        core_classes_per_doc.append([])\n",
        "        continue\n",
        "\n",
        "    # 2) 상위 일부만 candidate로 두고 confidence 계산\n",
        "    cand = np.array(idx_sorted[:CAND_TOP], dtype=np.int64)\n",
        "\n",
        "    conf_vals = compute_conf_doc(\n",
        "        sims=sim_row,      # local sim(d, :)\n",
        "        cand=cand,\n",
        "        parents=parents,\n",
        "        siblings=siblings,\n",
        "    )  # (len(cand),)\n",
        "\n",
        "    # (옵션) confidence threshold (부모/형제보다 안 좋은 애 걸러내기 등)\n",
        "    if CONF_MIN is not None:\n",
        "        mask = conf_vals >= CONF_MIN\n",
        "        cand = cand[mask]\n",
        "        conf_vals = conf_vals[mask]\n",
        "        if cand.size == 0:\n",
        "            core_classes_per_doc.append([])\n",
        "            continue\n",
        "\n",
        "    # 3) path score + confidence 를 합쳐서 최종 점수\n",
        "    #    → \"조금\"만 섞고 싶으니 작은 alpha\n",
        "    combined_scores = path_row[cand] + CONF_ALPHA * conf_vals\n",
        "\n",
        "    order = np.argsort(-combined_scores)\n",
        "    chosen = cand[order[:TOP_K_CORE]]\n",
        "\n",
        "    core_classes_per_doc.append(chosen.tolist())\n",
        "\n",
        "    # 대표 core 하나 (가장 점수 높은 것) — path score 기준 or combined 둘 중 택1\n",
        "    best_c = int(chosen[0])\n",
        "    core_primary[i] = best_c\n",
        "    core_primary_score[i] = float(path_row[best_c])  # or combined_scores[order[0]]\n",
        "\n",
        "# 예시 출력\n",
        "print(\"예시 문서 0의 core classes (id):\", core_classes_per_doc[0])\n",
        "print(\"예시 문서 0의 core classes (name):\",\n",
        "      [id2label[str(c)] for c in core_classes_per_doc[0]])\n",
        "print(\"예시 문서 0의 대표 core:\", core_primary[0],\n",
        "      id2label[str(core_primary[0])], \"score =\", core_primary_score[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXrgfoZDpN7g"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "def summarize_core_class_mapping_v2(\n",
        "    core_classes_per_doc,\n",
        "    core_primary,\n",
        "    core_primary_score,\n",
        "    id2label=None,\n",
        "    depths=None,\n",
        "    top_k_labels=20,\n",
        "):\n",
        "    \"\"\"\n",
        "    core_classes_per_doc : List[List[int]]  # 문서별 core class id 목록\n",
        "    core_primary         : np.ndarray (N_docs,)  # 대표 core class id (없으면 -1)\n",
        "    core_primary_score   : np.ndarray (N_docs,)  # 대표 core의 path / combined score\n",
        "    id2label             : dict[str,str] or None # label id(str) -> name\n",
        "    depths               : np.ndarray (C,) or None  # 각 label의 depth (root=0)\n",
        "    \"\"\"\n",
        "\n",
        "    N_docs = len(core_classes_per_doc)\n",
        "    core_counts = np.array([len(x) for x in core_classes_per_doc], dtype=int)\n",
        "\n",
        "    print(\"========== Core Class Mapping Summary (v2) ==========\")\n",
        "    print(f\"총 문서 수                     : {N_docs}\")\n",
        "    print(f\"core_primary 정의된 문서 수     : {(core_primary >= 0).sum()}\")\n",
        "\n",
        "    # 1) 문서당 core class 개수 분포 (평균 X, 분포만)\n",
        "    print(\"\\n[1] 문서당 core class 개수 분포\")\n",
        "    print(f\" - 최소 : {core_counts.min()}  / 최대 : {core_counts.max()}\")\n",
        "    for k in sorted(np.unique(core_counts)):\n",
        "        cnt = int((core_counts == k).sum())\n",
        "        print(f\"   · {k}개 core class 가진 문서 수 : {cnt} ({cnt / N_docs:.3%})\")\n",
        "\n",
        "    # 2) 대표 core score 분포 (평균 X, quantile + threshold 0.6/0.7/0.8/0.9)\n",
        "    mask_has_primary = core_primary >= 0\n",
        "    scores = core_primary_score[mask_has_primary]\n",
        "    if scores.size > 0:\n",
        "        print(\"\\n[2] 대표 core score 분포 (core_primary가 있는 문서 기준)\")\n",
        "        print(f\" - 문서 수 : {scores.size}\")\n",
        "        qs = np.percentile(scores, [0, 25, 50, 75, 100])\n",
        "        print(\n",
        "            \" - 최소 / 25% / 50% / 75% / 최대 : \"\n",
        "            f\"{qs[0]:.4f} / {qs[1]:.4f} / {qs[2]:.4f} / {qs[3]:.4f} / {qs[4]:.4f}\"\n",
        "        )\n",
        "\n",
        "        for t in [0.6, 0.7, 0.8, 0.9]:\n",
        "            cnt = int((scores >= t).sum())\n",
        "            print(f\"   · score ≥ {t:.1f} 인 문서 수 : {cnt} ({cnt / scores.size:.3%})\")\n",
        "    else:\n",
        "        print(\"\\n[2] 대표 core score 분포: core_primary가 정의된 문서가 없음\")\n",
        "\n",
        "    # 3) label별 core / primary 등장 횟수 (Top 20 + Worst 20)\n",
        "    print(\"\\n[3] label별 core / primary 등장 횟수 (문서 기준)\")\n",
        "\n",
        "    core_freq = Counter()\n",
        "    for core_list in core_classes_per_doc:\n",
        "        for c in core_list:\n",
        "            core_freq[int(c)] += 1\n",
        "\n",
        "    primary_freq = Counter()\n",
        "    for c in core_primary[mask_has_primary]:\n",
        "        primary_freq[int(c)] += 1\n",
        "\n",
        "    distinct_core_labels = len(core_freq)\n",
        "    distinct_primary_labels = len(primary_freq)\n",
        "    print(f\" - 한 번 이상 core로 등장한 label 수     : {distinct_core_labels}\")\n",
        "    print(f\" - 한 번 이상 primary로 등장한 label 수 : {distinct_primary_labels}\")\n",
        "\n",
        "    def _label_name(cid: int) -> str:\n",
        "        if id2label is None:\n",
        "            return str(cid)\n",
        "        return id2label.get(str(cid), str(cid))\n",
        "\n",
        "    # Top 20 (core 등장 많은 순)\n",
        "    print(f\"\\n   Top {top_k_labels} (core 등장 문서 수 기준)\")\n",
        "    for cid, cnt in core_freq.most_common(top_k_labels):\n",
        "        name = _label_name(cid)\n",
        "        p_cnt = primary_freq.get(cid, 0)\n",
        "        print(f\"   · [{cid:4d}] {name:30s}  core docs: {cnt:5d}  | primary docs: {p_cnt:5d}\")\n",
        "\n",
        "    # Worst 20 (core 등장 적은 순) – core로 한 번이라도 등장한 label 중에서\n",
        "    print(f\"\\n   Worst {top_k_labels} (core로 거의 안 뽑힌 label들)\")\n",
        "    # Counter에는 1번 이상 등장한 label만 있으므로, 그 안에서 오름차순 정렬\n",
        "    least_core = sorted(core_freq.items(), key=lambda x: (x[1], x[0]))[:top_k_labels]\n",
        "    for cid, cnt in least_core:\n",
        "        name = _label_name(cid)\n",
        "        p_cnt = primary_freq.get(cid, 0)\n",
        "        print(f\"   · [{cid:4d}] {name:30s}  core docs: {cnt:5d}  | primary docs: {p_cnt:5d}\")\n",
        "\n",
        "    # 4) depth 정보가 있으면 depth별 분포\n",
        "    if depths is not None:\n",
        "        print(\"\\n[4] depth별 분포 (depths가 주어진 경우)\")\n",
        "        depths = np.asarray(depths, dtype=int)\n",
        "\n",
        "        # core 등장 기준 depth 분포\n",
        "        depth_core_counts = Counter()\n",
        "        for core_list in core_classes_per_doc:\n",
        "            for c in core_list:\n",
        "                d = int(depths[c])\n",
        "                depth_core_counts[d] += 1\n",
        "\n",
        "        depth_primary_counts = Counter()\n",
        "        for c in core_primary[mask_has_primary]:\n",
        "            d = int(depths[int(c)])\n",
        "            depth_primary_counts[d] += 1\n",
        "\n",
        "        print(\"   · depth별 core label 등장 횟수:\")\n",
        "        for d in sorted(depth_core_counts.keys()):\n",
        "            cnt = depth_core_counts[d]\n",
        "            print(f\"     depth {d}: {cnt} 회\")\n",
        "\n",
        "        print(\"   · depth별 primary label 등장 문서 수:\")\n",
        "        for d in sorted(depth_primary_counts.keys()):\n",
        "            cnt = depth_primary_counts[d]\n",
        "            print(f\"     depth {d}: {cnt} 문서\")\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 예시 호출\n",
        "# -----------------------------\n",
        "summarize_core_class_mapping_v2(\n",
        "    core_classes_per_doc = core_classes_per_doc,\n",
        "    core_primary         = core_primary,\n",
        "    core_primary_score   = core_primary_score,\n",
        "    id2label             = id2label,   # 없으면 None\n",
        "    depths               = None,       # depth 있으면 np.array로 넣기\n",
        "    top_k_labels         = 60,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACIu5VDxpJrD"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ap8mCDBRF0Pm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "# ---------------------------------\n",
        "# 1. 경로 설정 (네 파일 이름에 맞게 바꿔줘)\n",
        "# ---------------------------------\n",
        "GPT_TEXT_PATH  = \"Amazon_products/train/generated_docs.txt\"    # 예: 0<TAB>문장\n",
        "GPT_LABEL_PATH = \"Amazon_products/train/generated_doc2label.json\"  # 예: {\"0\": {...}, \"1\": {...}, ...}\n",
        "\n",
        "\n",
        "# ---------------------------------\n",
        "# 2. GPT 텍스트 txt 불러오기\n",
        "#    각 줄에서 첫 토큰은 doc_id, 나머지는 text로 파싱\n",
        "# ---------------------------------\n",
        "rows = []\n",
        "with open(GPT_TEXT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line_no, line in enumerate(f, start=1):\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        parts = line.split(maxsplit=1)  # 첫 토큰과 나머지 전체를 분리\n",
        "        if len(parts) == 1:\n",
        "            print(f\"[경고] 텍스트 없음 (line {line_no}): {line!r}\")\n",
        "            continue\n",
        "        doc_id, text = parts[0], parts[1]\n",
        "        rows.append((doc_id, text))\n",
        "\n",
        "df_gpt = pd.DataFrame(rows, columns=[\"doc_id\", \"text\"])\n",
        "df_gpt[\"doc_id\"] = df_gpt[\"doc_id\"].astype(str)\n",
        "\n",
        "print(\"GPT 텍스트 로드:\", df_gpt.shape)\n",
        "print(df_gpt.head(2))\n",
        "\n",
        "# ---------------------------------\n",
        "# 3. GPT 라벨 JSON 불러오기\n",
        "# ---------------------------------\n",
        "with open(GPT_LABEL_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    gpt_label_info = json.load(f)\n",
        "\n",
        "print(\"GPT 라벨 키 수:\", len(gpt_label_info))\n",
        "example_key = list(gpt_label_info.keys())[0]\n",
        "print(\"라벨 예시:\", example_key, \"->\", gpt_label_info[example_key])\n",
        "\n",
        "# ---------------------------------\n",
        "# 4. doc_id 정렬 & 정합성 체크\n",
        "# ---------------------------------\n",
        "text_ids  = set(df_gpt[\"doc_id\"].tolist())\n",
        "label_ids = set(gpt_label_info.keys())\n",
        "\n",
        "print(\"\\n텍스트 id 개수:\", len(text_ids))\n",
        "print(\"라벨   id 개수:\", len(label_ids))\n",
        "print(\"교집합 개수  :\", len(text_ids & label_ids))\n",
        "\n",
        "missing_in_labels = sorted(text_ids - label_ids)\n",
        "missing_in_texts  = sorted(label_ids - text_ids)\n",
        "\n",
        "if missing_in_labels:\n",
        "    print(\"\\n[경고] 라벨이 없는 텍스트 id들:\", missing_in_labels[:10], \"...\")\n",
        "if missing_in_texts:\n",
        "    print(\"[경고] 텍스트가 없는 라벨 id들:\", missing_in_texts[:10], \"...\")\n",
        "\n",
        "# id 기준으로 정렬\n",
        "df_gpt = df_gpt.sort_values(\"doc_id\").reset_index(drop=True)\n",
        "print(\"\\n정렬 후 doc_id 상위 몇 개:\", df_gpt[\"doc_id\"].head().tolist())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9D2Yd6GEHKcu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "num_classes = 531 # 혹은 len(id2label)\n",
        "\n",
        "def build_multihot_from_info(info, num_classes: int):\n",
        "    \"\"\"\n",
        "    info 예:\n",
        "    {\n",
        "      \"core_classes\": \"2\",\n",
        "      \"with ancestors\": [\"0\", \"1\", \"2\"]\n",
        "    }\n",
        "    -> 길이 num_classes 멀티핫 벡터\n",
        "    \"\"\"\n",
        "    y = np.zeros(num_classes, dtype=np.float32)\n",
        "    for cid_str in info[\"with ancestors\"]:\n",
        "        cid = int(cid_str)\n",
        "        if 0 <= cid < num_classes:\n",
        "            y[cid] = 1.0\n",
        "    return y\n",
        "\n",
        "gpt_texts = []\n",
        "gpt_Y_all = []\n",
        "\n",
        "for _, row in df_gpt.iterrows():   # df_gpt: doc_id, text (앞에서 만든 것)\n",
        "    doc_id = row[\"doc_id\"]\n",
        "    text   = row[\"text\"]\n",
        "\n",
        "    if doc_id not in gpt_label_info:\n",
        "        continue\n",
        "\n",
        "    info = gpt_label_info[doc_id]\n",
        "    y = build_multihot_from_info(info, num_classes)\n",
        "\n",
        "    gpt_texts.append(text)\n",
        "    gpt_Y_all.append(y)\n",
        "\n",
        "gpt_Y_all = np.stack(gpt_Y_all, axis=0)  # [N_gpt, C]\n",
        "\n",
        "print(\"GPT 샘플 수:\", len(gpt_texts))\n",
        "print(\"gpt_Y_all shape:\", gpt_Y_all.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-RiT13AHrlF"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    gpt_emb = st_model.encode(\n",
        "        gpt_texts,\n",
        "        batch_size=128,\n",
        "        convert_to_tensor=True,\n",
        "        show_progress_bar=True,\n",
        "    ).cpu().numpy()   # [N_docs, d_doc]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzLkHpWDXql6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"device:\", device)\n",
        "\n",
        "# 1) 이미 위에서 만든 임베딩 그대로 사용\n",
        "doc_emb_np   = total_doc_emb        # [N_docs, d_doc]\n",
        "label_emb_np = label_emb            # [num_class, d_lab]\n",
        "\n",
        "N_docs, d_doc = doc_emb_np.shape\n",
        "num_classes, d_lab = label_emb_np.shape\n",
        "print(\"N_docs:\", N_docs, \"| num_classes:\", num_classes)\n",
        "\n",
        "# 2) GAT용 adjacency (adj_upper → 대칭 + self-loop)\n",
        "adj_np = adj_upper.astype(np.float32)\n",
        "adj_np = adj_np + adj_np.T          # parent-child 양방향으로\n",
        "np.fill_diagonal(adj_np, 1.0)       # self-loop\n",
        "\n",
        "adj = torch.from_numpy(adj_np).to(device)           # [C, C]\n",
        "init_class_emb = torch.from_numpy(label_emb_np).float().to(device)  # [C, d_lab]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffcF6N0ZiNdM"
      },
      "outputs": [],
      "source": [
        "# core_classes_per_doc: 길이 N_docs, 각 문서별 core class 리스트 (예: [3, 27, 81] …)\n",
        "N_docs     = total_doc_emb.shape[0]\n",
        "num_labels = num_class   # = label_emb.shape[0]\n",
        "\n",
        "Y_ALL = np.zeros((N_docs, num_labels), dtype=np.float32)\n",
        "for i, cores in enumerate(core_classes_per_doc):\n",
        "    for cid in cores:\n",
        "        Y_ALL[i, cid] = 1.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_NBkGz9xst0k"
      },
      "outputs": [],
      "source": [
        "# core_primary: (N_docs,)  각 문서당 대표 class id, 없으면 -1\n",
        "core_primary = np.array(core_primary, dtype=np.int64)\n",
        "\n",
        "# 유효한 라벨만 사용 (-1 제거)\n",
        "mask = core_primary >= 0\n",
        "doc_train = total_doc_emb[mask]          # [N_use, d_doc]\n",
        "y_idx     = core_primary[mask]           # [N_use]\n",
        "\n",
        "\n",
        "indices = np.arange(N_docs)\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "split = int(N_docs * 0.8)\n",
        "train_idx = indices[:split]\n",
        "val_idx   = indices[split:]\n",
        "\n",
        "X_train = total_doc_emb[train_idx]\n",
        "Y_train = Y_ALL[train_idx]\n",
        "X_val   = total_doc_emb[val_idx]\n",
        "Y_val   = Y_ALL[val_idx]\n",
        "\n",
        "print(\"train size:\", X_train.shape[0])\n",
        "print(\"val size  :\", X_val.shape[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMTdvfDSFm36"
      },
      "outputs": [],
      "source": [
        "# 1) 기존 train/val 유지\n",
        "print(\"원래 X_train:\", X_train.shape, \"Y_train:\", Y_train.shape)\n",
        "print(\"원래 X_val  :\", X_val.shape,   \"Y_val  :\", Y_val.shape)\n",
        "\n",
        "# 2) GPT를 train에만 concat\n",
        "X_train_aug = np.concatenate([X_train, gpt_emb], axis=0)      # [N_train + N_gpt, d_doc]\n",
        "Y_train_aug = np.concatenate([Y_train, gpt_Y_all], axis=0)    # [N_train + N_gpt, C]\n",
        "\n",
        "print(\"증강 X_train:\", X_train_aug.shape)\n",
        "print(\"증강 Y_train:\", Y_train_aug.shape)\n",
        "print(\"val은 그대로 :\", X_val.shape, Y_val.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8H0ljAYmhC7N"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class DocLabelDataset(Dataset):\n",
        "    def __init__(self, doc_np, y_np):\n",
        "        self.doc = torch.from_numpy(doc_np).float()\n",
        "        self.y   = torch.from_numpy(y_np).float()\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.doc.size(0)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.doc[idx], self.y[idx]\n",
        "\n",
        "batch_size = 256\n",
        "\n",
        "train_dataset = DocLabelDataset(X_train_aug, Y_train_aug)\n",
        "val_dataset   = DocLabelDataset(X_val,   Y_val)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, batch_size=batch_size, shuffle=True,\n",
        "    num_workers=2, pin_memory=True,\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset, batch_size=batch_size, shuffle=False,\n",
        "    num_workers=2, pin_memory=True,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4bcaMsNFxou"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3U6vvzVysvkY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LabelGATv2(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim, out_dim, adj, num_heads=4, dropout=0.1):\n",
        "        \"\"\"\n",
        "        in_dim     : 입력 라벨 임베딩 차원\n",
        "        hidden_dim : head당 hidden 차원\n",
        "        out_dim    : 출력 라벨 임베딩 차원\n",
        "        adj        : [C, C] adjacency (0/1 또는 양수, self-loop 포함)\n",
        "        num_heads  : multi-head 개수\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.register_buffer(\"adj\", adj)  # [C, C]\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = hidden_dim\n",
        "\n",
        "        # node feature projection: W\n",
        "        self.lin = nn.Linear(in_dim, num_heads * hidden_dim, bias=False)\n",
        "\n",
        "        # GATv2: head마다 하나의 attention 벡터 a_h\n",
        "        self.att = nn.Parameter(torch.empty(num_heads, hidden_dim))\n",
        "\n",
        "        self.leakyrelu = nn.LeakyReLU(0.2)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.fc_out = nn.Linear(num_heads * hidden_dim, out_dim)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.xavier_uniform_(self.lin.weight)\n",
        "        nn.init.xavier_uniform_(self.att)\n",
        "        nn.init.xavier_uniform_(self.fc_out.weight)\n",
        "        if self.fc_out.bias is not None:\n",
        "            nn.init.zeros_(self.fc_out.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x : [C, in_dim]  (현재 label embedding)\n",
        "        return : [C, out_dim]\n",
        "        \"\"\"\n",
        "        C = x.size(0)\n",
        "\n",
        "        # 1) 선형 변환 + head로 reshape\n",
        "        h = self.lin(x)                              # [C, H*D]\n",
        "        h = h.view(C, self.num_heads, self.head_dim) # [C, H, D]\n",
        "\n",
        "        # GATv2 핵심: feature에 비선형 먼저\n",
        "        h_act = F.elu(h)                             # [C, H, D]\n",
        "\n",
        "        # 2) attention score 계산\n",
        "        #   alpha[i,h] = a_h^T * h_act[i,h,:]\n",
        "        alpha = (h_act * self.att.unsqueeze(0)).sum(-1)  # [C, H]\n",
        "\n",
        "        # e_ij^h = LeakyReLU(alpha[i,h] + alpha[j,h])\n",
        "        e = alpha.unsqueeze(1) + alpha.unsqueeze(0)      # [C, C, H]\n",
        "        e = self.leakyrelu(e)\n",
        "\n",
        "        # adjacency로 mask (edge 없는 곳은 -inf)\n",
        "        mask = (self.adj > 0).unsqueeze(-1)              # [C, C, 1]\n",
        "        e = e.masked_fill(~mask, float('-inf'))\n",
        "\n",
        "        # 3) softmax로 attention coefficient\n",
        "        #   각 타겟 i에 대해 이웃 j 방향으로 softmax (dim=1)\n",
        "        attn = F.softmax(e, dim=1)      # [C, C, H]\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        # 4) 이웃 aggregation: out[i,h,:] = Σ_j attn[i,j,h] * h_act[j,h,:]\n",
        "        attn_h_first = attn.permute(2, 0, 1)   # [H, C, C]\n",
        "        h_h_first    = h_act.permute(1, 0, 2)  # [H, C, D]\n",
        "\n",
        "        out_heads = torch.bmm(attn_h_first, h_h_first)   # [H, C, D]\n",
        "        out = out_heads.permute(1, 0, 2).contiguous()    # [C, H, D]\n",
        "        out = out.view(C, self.num_heads * self.head_dim)  # [C, H*D]\n",
        "\n",
        "        out = self.fc_out(out)  # [C, out_dim]\n",
        "        out = F.elu(out)\n",
        "\n",
        "        # residual: 차원이 같으면 x 더해줌\n",
        "        if x.size(1) == out.size(1):\n",
        "            out = out + x\n",
        "\n",
        "        # label embedding은 보통 정규화해서 쓰는 게 안정적\n",
        "        return F.normalize(out, dim=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNp0b6U1ZR_K"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn import init\n",
        "\n",
        "# transformers.AutoModel 는 이제 안 써도 됨\n",
        "# from transformers import AutoModel\n",
        "\n",
        "\n",
        "class ClassModel(nn.Module):\n",
        "    def __init__(self, enc_dim, init_class_emb: torch.Tensor,\n",
        "                 adj_norm: torch.Tensor, gnn_hidden=256):\n",
        "        super().__init__()\n",
        "\n",
        "        self.doc_dim = enc_dim\n",
        "        self.num_classes, self.label_dim = init_class_emb.size()\n",
        "\n",
        "        self.label_embedding = nn.Parameter(\n",
        "            init_class_emb.clone().detach(), requires_grad=True\n",
        "        )\n",
        "\n",
        "\n",
        "        # 이제: GAT\n",
        "        self.label_gnn = LabelGATv2(\n",
        "            in_dim=self.label_dim,\n",
        "            hidden_dim=gnn_hidden,\n",
        "            out_dim=self.label_dim,\n",
        "            adj=adj_norm,          # 또는 adj_mask\n",
        "            num_heads=4,\n",
        "            dropout=0.1,\n",
        "        )\n",
        "\n",
        "        self.interaction = LBM(\n",
        "            self.doc_dim,\n",
        "            self.label_dim,\n",
        "            n_classes=self.num_classes,\n",
        "            bias=False,\n",
        "        )\n",
        "\n",
        "    def forward(self, doc_emb):\n",
        "        # doc_emb: [B, enc_dim]\n",
        "        label_emb = self.label_gnn(self.label_embedding)    # [C, label_dim]\n",
        "        scores = self.interaction(doc_emb, label_emb)       # [B, C]\n",
        "        return scores\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class LBM(nn.Module):\n",
        "    def __init__(self, l_dim, r_dim, n_classes=None, bias=True):\n",
        "        super(LBM, self).__init__()\n",
        "        self.weight = Parameter(torch.Tensor(l_dim, r_dim))\n",
        "        self.use_bias = bias\n",
        "        if self.use_bias:\n",
        "            self.bias = Parameter(torch.Tensor(n_classes))\n",
        "\n",
        "        bound = 1.0 / math.sqrt(l_dim)\n",
        "        init.uniform_(self.weight, -bound, bound)\n",
        "        if self.use_bias:\n",
        "            init.uniform_(self.bias, -bound, bound)\n",
        "\n",
        "    def forward(self, e1, e2):\n",
        "        \"\"\"\n",
        "        e1: tensor of size (batch_size, l_dim)  -> doc emb\n",
        "        e2: tensor of size (n_classes, r_dim)   -> label emb\n",
        "        return: tensor of size (batch_size, n_classes)\n",
        "        \"\"\"\n",
        "        scores = torch.matmul(torch.matmul(e1, self.weight), e2.T)\n",
        "        if self.use_bias:\n",
        "            scores = scores + self.bias\n",
        "        return scores\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXptiS-cZMSW"
      },
      "outputs": [],
      "source": [
        "model = ClassModel(\n",
        "    enc_dim=doc_train.shape[1],\n",
        "    init_class_emb=init_class_emb,\n",
        "    adj_norm=adj,\n",
        "    gnn_hidden=256,\n",
        ").to(device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GN5a2fHsaGXT"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def logits_to_pred_binary(logits, threshold=0.5, min_k=1, max_k=3):\n",
        "    \"\"\"\n",
        "    logits: [B, C] 텐서\n",
        "    return: [B, C] numpy (0/1)\n",
        "    \"\"\"\n",
        "    probs = torch.sigmoid(logits).detach().cpu().numpy()\n",
        "    B, C = probs.shape\n",
        "    pred_bin = np.zeros_like(probs, dtype=np.int32)\n",
        "\n",
        "    for i in range(B):\n",
        "        p = probs[i]\n",
        "        pos = np.where(p >= threshold)[0].tolist()\n",
        "\n",
        "        # 최소 min_k개는 선택\n",
        "        if len(pos) < min_k:\n",
        "            order = np.argsort(-p)\n",
        "            for k in order:\n",
        "                if k not in pos:\n",
        "                    pos.append(k)\n",
        "                if len(pos) >= min_k:\n",
        "                    break\n",
        "\n",
        "        # 최대 max_k개까지만\n",
        "        if max_k is not None and len(pos) > max_k:\n",
        "            pos = sorted(pos, key=lambda j: -p[j])[:max_k]\n",
        "\n",
        "        pred_bin[i, pos] = 1\n",
        "\n",
        "    return pred_bin\n",
        "\n",
        "\n",
        "def evaluate_f1(model, dev_loader, device,\n",
        "                threshold=0.5, min_k=1, max_k=3):\n",
        "    \"\"\"\n",
        "    dev_loader: TeleEmbDataset 기준\n",
        "        각 배치가 (doc_emb, labels) 형태라고 가정\n",
        "        - doc_emb : [B, enc_dim]\n",
        "        - labels  : [B, C] 멀티핫\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_true = []\n",
        "    all_pred = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for doc_emb_b, labels_b_t in dev_loader:\n",
        "            # doc_emb_b: [B, enc_dim] (CPU -> GPU)\n",
        "            doc_emb_b = doc_emb_b.to(device)\n",
        "            # labels_b: numpy [B, C]\n",
        "            labels_b  = labels_b_t.cpu().numpy()\n",
        "\n",
        "            # 모델은 이제 임베딩만 받음\n",
        "            logits = model(doc_emb_b)  # [B, C]\n",
        "\n",
        "            pred_b = logits_to_pred_binary(\n",
        "                logits,\n",
        "                threshold=threshold,\n",
        "                min_k=min_k,\n",
        "                max_k=max_k,\n",
        "            )\n",
        "\n",
        "            all_true.append(labels_b)\n",
        "            all_pred.append(pred_b)\n",
        "\n",
        "    y_true = np.concatenate(all_true, axis=0)\n",
        "    y_pred = np.concatenate(all_pred, axis=0)\n",
        "\n",
        "    micro = f1_score(y_true, y_pred, average=\"micro\", zero_division=0)\n",
        "    macro = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
        "    return micro, macro\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EL1AKRC7lIZ6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def build_pos_neg_masks_from_core(core_label_lists, num_labels, parents, children, device):\n",
        "    \"\"\"\n",
        "    core_label_lists : List[List[int]] (배치 안 각 문서의 core 라벨 인덱스 리스트)\n",
        "    return:\n",
        "        pos_mask : [B, C] float (pos 위치 1.0)\n",
        "        neg_mask : [B, C] float (neg 위치 1.0)\n",
        "    \"\"\"\n",
        "    B = len(core_label_lists)\n",
        "    pos_mask = torch.zeros(B, num_labels, dtype=torch.float32, device=device)\n",
        "    neg_mask = torch.zeros(B, num_labels, dtype=torch.float32, device=device)\n",
        "\n",
        "    all_classes = set(range(num_labels))\n",
        "\n",
        "    for i, labs in enumerate(core_label_lists):\n",
        "        if not labs:\n",
        "            continue  # core 라벨이 전혀 없는 문서는 그냥 skip (이 배치에는 거의 없겠지만)\n",
        "\n",
        "        ci = set(labs)\n",
        "\n",
        "        # C_i^pos = parents ∪ Ci\n",
        "        pos_set = set(ci)\n",
        "        for c in ci:\n",
        "            pos_set.update(parents[c])\n",
        "\n",
        "        # children union\n",
        "        chd_union = set()\n",
        "        for c in ci:\n",
        "            chd_union.update(children[c])\n",
        "\n",
        "        # C_i^neg = C - C_i^pos - children union\n",
        "        neg_set = all_classes - pos_set - chd_union\n",
        "\n",
        "        pos_idx = list(pos_set)\n",
        "        neg_idx = list(neg_set)\n",
        "\n",
        "        pos_mask[i, pos_idx] = 1.0\n",
        "        neg_mask[i, neg_idx] = 1.0\n",
        "\n",
        "    return pos_mask, neg_mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAKBEmq4lJCt"
      },
      "outputs": [],
      "source": [
        "def hierarchical_bce_loss(logits, core_label_lists, num_labels, parents, children,\n",
        "                          pos_alpha=3.0, neg_alpha=1.0,\n",
        "                          pos_target=0.9, neg_target=0.1):\n",
        "    device = logits.device\n",
        "    pos_mask, neg_mask = build_pos_neg_masks_from_core(\n",
        "        core_label_lists, num_labels, parents, children, device\n",
        "    )\n",
        "\n",
        "    log_p   = F.logsigmoid(logits)      # [B, C]\n",
        "    log_1m_p = F.logsigmoid(-logits)    # [B, C]\n",
        "\n",
        "    # 양성: 0.95, 음성: 0.05 로 label smoothing\n",
        "    loss_pos = -pos_alpha * pos_mask * (\n",
        "        pos_target * log_p + (1.0 - pos_target) * log_1m_p\n",
        "    )\n",
        "    loss_neg = -neg_alpha * neg_mask * (\n",
        "        neg_target * log_p + (1.0 - neg_target) * log_1m_p\n",
        "    )\n",
        "\n",
        "    per_doc_loss = (loss_pos + loss_neg).sum(dim=1)  # [B]\n",
        "\n",
        "    # 평균 낼 때는 마스크 개수 대신, 가중치 포함해서 나눠주는 게 깔끔\n",
        "    per_doc_denom = (\n",
        "        pos_alpha * pos_mask + neg_alpha * neg_mask\n",
        "    ).sum(dim=1) + 1e-8\n",
        "\n",
        "    per_doc_loss = per_doc_loss / per_doc_denom\n",
        "\n",
        "    valid = per_doc_denom > 1e-6\n",
        "    if valid.any():\n",
        "        loss = per_doc_loss[valid].mean()\n",
        "    else:\n",
        "        loss = torch.tensor(0.0, device=device)\n",
        "\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCBKsJr6auFj"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "\n",
        "class UnlabeledEmbDataset(Dataset):\n",
        "    def __init__(self, emb_tensor):\n",
        "        \"\"\"\n",
        "        emb_tensor : [N_unlab, enc_dim]  (SentenceTransformer 등으로 미리 뽑은 임베딩)\n",
        "        \"\"\"\n",
        "        self.emb_tensor = emb_tensor\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.emb_tensor.shape[0]  # 또는 len(self.emb_tensor)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # doc_emb: [enc_dim], idx: 원본 인덱스 (pseudo 중복 방지용)\n",
        "        return self.emb_tensor[idx], idx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUqmfPlaca4n"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ----- mpnet 기반 class_emb 셋업 -----\n",
        "num_labels, enc_dim = label_emb.shape\n",
        "class_emb = torch.from_numpy(label_emb).float()   # [C, d]\n",
        "class_emb = F.normalize(class_emb, dim=1)        # 선택사항이지만 보통 좋음\n",
        "class_emb = class_emb.to(device)\n",
        "\n",
        "\n",
        "\n",
        "# ----- 옵티마 / loss -----\n",
        "EPOCHS           = 140                 # 5 epoch 이후부터 pseudo 넣을 거니까 20 정도로\n",
        "PSEUDO_START_EP  = 5                  # 5 epoch 이후부터 pseudo 시작\n",
        "PSEUDO_THRESH    = 0.45               # pseudo label threshold\n",
        "lr               = 1e-3\n",
        "weight_decay     = 1e-4\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "best_macro = -1.0\n",
        "best_state = None\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgIWoknyKc1c"
      },
      "outputs": [],
      "source": [
        "print(len(train_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhDNloMWYrMK"
      },
      "outputs": [],
      "source": [
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    # ===== 1) train (labeled + 이미 추가된 pseudo 포함) =====\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for doc_emb, labels in train_loader:   # (input_ids, attn_mask, y) 대신 (doc_emb, y)\n",
        "        doc_emb = doc_emb.to(device)       # [B, enc_dim]\n",
        "        labels  = labels.to(device)        # [B, C]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logits = model(doc_emb)            # [B, C]\n",
        "        core_label_lists = []\n",
        "        for i in range(labels.size(0)):\n",
        "            labs = (labels[i] > 0.5).nonzero(as_tuple=False).squeeze(1).tolist()\n",
        "            core_label_lists.append(labs)\n",
        "\n",
        "\n",
        "        loss_sup = hierarchical_bce_loss(\n",
        "            logits,\n",
        "            core_label_lists,\n",
        "            num_labels,\n",
        "            parents,\n",
        "            children,\n",
        "        )\n",
        "        \"\"\"\n",
        "\n",
        "        loss_sup = hierarchical_bce_loss_v2(\n",
        "    logits,\n",
        "    core_label_lists,\n",
        "    num_labels,\n",
        "    anc_t,\n",
        "    desc_t,\n",
        "    parents,\n",
        "    # 추천 기본값 예시 (필요하면 조절)\n",
        "    core_alpha=4.0, anc_alpha=2.0, neg_alpha=1.0,\n",
        "    desc_alpha=0.0,            # 기존처럼 descendant는 무시\n",
        "    core_target=0.9, anc_target=0.8, neg_target=0.05,\n",
        "    lambda_hier=0.0,           # 원하면 0.02~0.1로 켜기\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "        loss = loss_sup\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "\n",
        "    # ===== 2) dev F1 =====\n",
        "    micro_f1, macro_f1 = evaluate_f1(model, val_loader, device)\n",
        "\n",
        "    print(f\"[Epoch {epoch}/{EPOCHS}] \"\n",
        "          f\"train_loss={avg_loss:.4f}  \"\n",
        "          f\"dev_micro={micro_f1:.4f}  dev_macro={macro_f1:.4f}\")\n",
        "\n",
        "    # best 모델 저장 (macro 기준 예시)\n",
        "    if macro_f1 > best_macro:\n",
        "        best_macro = macro_f1\n",
        "        best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
        "        print(\"  -> new best macro F1, saving state\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8OmKXdrcijz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if best_state is not None:\n",
        "    save_path = os.path.join(DATA_DIR, \"baba3_ablation_woutgnn_with_gpt.pt\")\n",
        "    torch.save(best_state, save_path)\n",
        "    print(\"saved best model to:\", save_path)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHPchkLViaZ0"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load(save_path, weights_only=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpzqdrRpblY5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "test_corpus_path = os.path.join(DATA_DIR, \"test\", \"test_corpus.txt\")\n",
        "\n",
        "test_doc_ids = []\n",
        "test_texts = []\n",
        "\n",
        "with open(test_corpus_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        line = line.rstrip(\"\\n\")\n",
        "        if not line:\n",
        "            continue\n",
        "        doc_id, text = line.split(\"\\t\", 1)\n",
        "        test_doc_ids.append(doc_id)\n",
        "        test_texts.append(text)\n",
        "\n",
        "print(f\"# test docs: {len(test_doc_ids)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jo6YbQ20iHEN"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    test_emb = st_model.encode(\n",
        "        test_texts,\n",
        "        batch_size=128,\n",
        "        convert_to_tensor=True,\n",
        "        show_progress_bar=True,\n",
        "    ).cpu()   # ★ CPU로 내려놓기 (DataLoader worker 문제 피하려고)\n",
        "\n",
        "print(\"test_emb shape:\", test_emb.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OThm0mf0iOkL"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TestEmbDataset(Dataset):\n",
        "    def __init__(self, emb_tensor):\n",
        "        # 혹시 리스트/넘파이여도 텐서로\n",
        "        if not isinstance(emb_tensor, torch.Tensor):\n",
        "            emb_tensor = torch.as_tensor(emb_tensor)\n",
        "        # GPU로 들어왔으면 CPU로\n",
        "        if emb_tensor.is_cuda:\n",
        "            emb_tensor = emb_tensor.cpu()\n",
        "\n",
        "        self.emb_tensor = emb_tensor  # [N_test, enc_dim]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.emb_tensor.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.emb_tensor[idx]   # [enc_dim]\n",
        "\n",
        "\n",
        "test_dataset = TestEmbDataset(test_emb)\n",
        "test_loader  = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=64,\n",
        "    shuffle=False,\n",
        "    num_workers=4,   # 에러 나면 0으로 줄이기\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VoQLFcZwu-EU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "CAND_TOP   = 50     # confidence 계산에 쓸 후보 수\n",
        "CONF_ALPHA = 0.2    # path score에 confidence를 섞는 비율\n",
        "CONF_MIN   = None   # 원하면 0.0 등으로 두고 conf < 0 컷\n",
        "\n",
        "def decode_core_from_logits(\n",
        "    logits: torch.Tensor,\n",
        "    parents,\n",
        "    siblings,\n",
        "    paths,\n",
        "    top_k_min: int = 2,\n",
        "    top_k_max: int = 3,\n",
        "    cand_top: int = CAND_TOP,\n",
        "    conf_alpha: float = CONF_ALPHA,\n",
        "    conf_min: float | None = CONF_MIN,\n",
        "):\n",
        "    \"\"\"\n",
        "    logits: [B, C]  (ClassModel 출력)\n",
        "    return: pred_bin: [B, C] bool\n",
        "    \"\"\"\n",
        "    probs = torch.sigmoid(logits).cpu().numpy()   # local sim(d,c) 역할\n",
        "    B, C = probs.shape\n",
        "\n",
        "    pred_bin = np.zeros((B, C), dtype=bool)\n",
        "\n",
        "    for i in range(B):\n",
        "        sim_row = probs[i]         # (C,)\n",
        "\n",
        "        # 1) path score(d, c) = 경로 상 노드들의 평균\n",
        "        path_row = np.zeros(C, dtype=np.float32)\n",
        "        for cid, path in paths.items():\n",
        "            path_row[cid] = float(sim_row[path].mean())\n",
        "\n",
        "        # 2) path score 기준 1차 정렬\n",
        "        idx_sorted = np.argsort(-path_row)\n",
        "        cand = np.array(idx_sorted[:cand_top], dtype=np.int64)\n",
        "\n",
        "        # 3) confidence 계산 (부모/형제 대비 여유)\n",
        "        conf_vals = compute_conf_doc(\n",
        "            sims=sim_row,\n",
        "            cand=cand,\n",
        "            parents=parents,\n",
        "            siblings=siblings,\n",
        "        )  # (len(cand),)\n",
        "\n",
        "        # (옵션) confidence threshold\n",
        "        if conf_min is not None:\n",
        "            mask = conf_vals >= conf_min\n",
        "            cand = cand[mask]\n",
        "            conf_vals = conf_vals[mask]\n",
        "            if cand.size == 0:\n",
        "                continue\n",
        "\n",
        "        # 4) 최종 점수: path + α * conf\n",
        "        combined = path_row[cand] + conf_alpha * conf_vals\n",
        "        order = np.argsort(-combined)\n",
        "\n",
        "        # 상한: top_k_max까지\n",
        "        k = min(top_k_max, len(order))\n",
        "        chosen = cand[order[:k]]\n",
        "\n",
        "        # 하한: top_k_min 보장 (부족하면 path_row 기준으로 추가)\n",
        "        if len(chosen) < top_k_min:\n",
        "            need = top_k_min - len(chosen)\n",
        "            extra = [c for c in idx_sorted if c not in chosen][:need]\n",
        "            if extra:\n",
        "                chosen = np.concatenate([chosen, np.array(extra, dtype=np.int64)])\n",
        "\n",
        "        pred_bin[i, chosen] = True\n",
        "\n",
        "    return pred_bin\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0gAm-zQdvY7L"
      },
      "outputs": [],
      "source": [
        "TOP_K_MIN = 2\n",
        "TOP_K_MAX = 3\n",
        "\n",
        "model.eval()\n",
        "predictions = {}\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "N = test_emb.shape[0]\n",
        "\n",
        "with torch.no_grad():\n",
        "    for start in range(0, N, BATCH_SIZE):\n",
        "        end = min(start + BATCH_SIZE, N)\n",
        "        doc_emb_b = test_emb[start:end].to(device)   # [B, enc_dim]\n",
        "\n",
        "        logits = model(doc_emb_b)  # [B, C]\n",
        "\n",
        "        # 기존: logits_to_pred_binary(...)\n",
        "        # 새로: core-class 스타일 디코딩\n",
        "        pred_bin = decode_core_from_logits(\n",
        "            logits,\n",
        "            parents=parents,\n",
        "            siblings=siblings,\n",
        "            paths=paths,\n",
        "            top_k_min=TOP_K_MIN,\n",
        "            top_k_max=TOP_K_MAX,\n",
        "        )  # [B, C] bool\n",
        "\n",
        "        for i in range(end - start):\n",
        "            row_idx = start + i\n",
        "            doc_id = test_doc_ids[row_idx]\n",
        "            pos = np.where(pred_bin[i])[0]           # 선택된 label index들\n",
        "            label_ids = [str(k) for k in pos]\n",
        "            predictions[doc_id] = label_ids\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDlRCEtvbtoG"
      },
      "outputs": [],
      "source": [
        "submit_path = os.path.join(DATA_DIR, \"baba3_ablation_woutgnn_with_gpt.csv\")\n",
        "with open(submit_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"id\", \"label\"])  # 포맷 맞게 조정\n",
        "\n",
        "    for doc_id in test_doc_ids:\n",
        "        label_ids = predictions[doc_id]  # [\"3\", \"40\", \"169\"] 같은 리스트\n",
        "        label_str = \",\".join(label_ids)\n",
        "        writer.writerow([doc_id, label_str])\n",
        "\n",
        "print(\"saved submission to:\", submit_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BK1ZpY3UbwrU",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "df = pd.read_csv(submit_path)\n",
        "\n",
        "print(\"=== 기본 정보 ===\")\n",
        "print(\"행 개수 (docs):\", len(df))\n",
        "print(\"컬럼:\", list(df.columns))\n",
        "\n",
        "# 1) 라벨 컬럼 이름 자동 선택 (label 또는 labels)\n",
        "if \"label\" in df.columns:\n",
        "    label_col = \"label\"\n",
        "elif \"labels\" in df.columns:\n",
        "    label_col = \"labels\"\n",
        "else:\n",
        "    raise ValueError(\"라벨 컬럼명을 찾을 수 없습니다. 'label' 또는 'labels'가 필요합니다.\")\n",
        "\n",
        "label_strs = df[label_col].fillna(\"\").astype(str)\n",
        "\n",
        "all_labels_flat  = []        # 개별 숫자 라벨\n",
        "combo_counter    = Counter() # (169,199) 같은 조합 통계용\n",
        "label_len_counter = Counter()# 문서당 라벨 개수 분포\n",
        "\n",
        "for s in label_strs:\n",
        "    s = s.strip(\",\")\n",
        "    if not s:\n",
        "        combo = ()\n",
        "    else:\n",
        "        # 콤마/공백 뭐가 와도 다 자르기: \"3 40,169\" -> [\"3\",\"40\",\"169\"]\n",
        "        parts = re.split(r\"[,\\s]+\", s)\n",
        "        parts = [x for x in parts if x != \"\"]\n",
        "        combo = tuple(int(x) for x in parts)\n",
        "\n",
        "    if combo:\n",
        "        combo_counter[combo] += 1\n",
        "        label_len_counter[len(combo)] += 1\n",
        "        all_labels_flat.extend(combo)\n",
        "    else:\n",
        "        label_len_counter[0] += 1\n",
        "\n",
        "# 1) 문서당 라벨 개수 분포\n",
        "print(\"\\n=== 문서당 라벨 개수 분포 ===\")\n",
        "for k in sorted(label_len_counter.keys()):\n",
        "    print(f\"{k}개 라벨: {label_len_counter[k]} docs\")\n",
        "\n",
        "print(\"min labels/doc:\", min(label_len_counter.keys()))\n",
        "print(\"max labels/doc:\", max(label_len_counter.keys()))\n",
        "\n",
        "# 2) 개별 라벨 기준 통계\n",
        "single_counter = Counter(all_labels_flat)\n",
        "unique_single_labels = sorted(single_counter.keys())\n",
        "print(\"\\n=== 개별 라벨 기준 ===\")\n",
        "print(\"고유 라벨 개수:\", len(unique_single_labels))\n",
        "\n",
        "TOTAL_CLASSES = 531\n",
        "print(\"coverage(single):\",\n",
        "      f\"{len(unique_single_labels)}/{TOTAL_CLASSES} = {len(unique_single_labels)/TOTAL_CLASSES:.3f}\")\n",
        "\n",
        "print(\"\\nTop 20 single labels:\")\n",
        "for lid, c in single_counter.most_common(20):\n",
        "    print(f\"{lid} : {c}\")\n",
        "\n",
        "# 3) 조합(세트) 기준 통계\n",
        "print(\"\\n=== 라벨 조합 기준 (tuple 그대로) ===\")\n",
        "print(\"고유 조합 개수:\", len(combo_counter))\n",
        "\n",
        "print(\"\\nTop 20 label combos:\")\n",
        "for combo, c in combo_counter.most_common(20):\n",
        "    print(f\"{','.join(map(str, combo))} : {c}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}