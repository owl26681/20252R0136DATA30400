{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26217918-5c27-43a7-a504-84e4cd5bba75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T01:28:50.444491Z",
     "iopub.status.busy": "2025-11-11T01:28:50.444253Z",
     "iopub.status.idle": "2025-11-11T01:28:51.758179Z",
     "shell.execute_reply": "2025-11-11T01:28:51.757596Z",
     "shell.execute_reply.started": "2025-11-11T01:28:50.444472Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25eba133-3361-42f0-8305-6df95935d14f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T01:28:51.759304Z",
     "iopub.status.busy": "2025-11-11T01:28:51.758963Z",
     "iopub.status.idle": "2025-11-11T01:28:59.945288Z",
     "shell.execute_reply": "2025-11-11T01:28:59.944602Z",
     "shell.execute_reply.started": "2025-11-11T01:28:51.759288Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-11 01:28:55.261564: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1762824535.273565    2050 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1762824535.277230    2050 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-11-11 01:28:55.289279: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, ConcatDataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "\n",
    "class LabelEncoder(nn.Module):\n",
    "    def __init__(self, model_name=\"bert-base-uncased\", alpha=0.7, l2norm=True):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.alpha = alpha\n",
    "        self.l2norm = l2norm\n",
    "\n",
    "    @torch.no_grad()  # 임베딩 추출만 할 경우 드롭아웃/그라드 off\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
    "        out = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            return_dict=True\n",
    "        )\n",
    "        last = out.last_hidden_state            # [B, T, H]\n",
    "        cls = last[:, 0, :]                     # [B, H]\n",
    "\n",
    "        # masked mean pooling (패딩 제외, CLS 제외)\n",
    "        mask = attention_mask.clone()           # [B, T]\n",
    "        mask[:, 0] = 0                          # CLS 제외\n",
    "        lengths = mask.sum(dim=1, keepdim=True).clamp(min=1)  # [B, 1]\n",
    "        mean = (last * mask.unsqueeze(-1)).sum(dim=1) / lengths  # [B, H]\n",
    "\n",
    "        h = self.alpha * cls + (1.0 - self.alpha) * mean         # [B, H]\n",
    "        if self.l2norm:\n",
    "            h = F.normalize(h, p=2, dim=-1)\n",
    "        return h\n",
    "\n",
    "# 사용 예시\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = LabelEncoder(model_name=\"bert-base-uncased\", alpha=0.7).to(device).eval()\n",
    "\n",
    "texts = [\n",
    "    'This is the class \"grocery_gourmet_food\". It is related to snacks, condiments, beverages, specialty foods, spices, cooking oils, baking ingredients, gourmet chocolates, artisanal cheeses, and organic foods.'\n",
    "]\n",
    "\n",
    "\n",
    "enc = tokenizer(texts, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    emb = model(input_ids=enc[\"input_ids\"].to(device),\n",
    "                attention_mask=enc[\"attention_mask\"].to(device))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e1bcacb-8765-49a0-b6d6-d18d6f5e5531",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T01:28:59.947460Z",
     "iopub.status.busy": "2025-11-11T01:28:59.947322Z",
     "iopub.status.idle": "2025-11-11T01:28:59.988985Z",
     "shell.execute_reply": "2025-11-11T01:28:59.988493Z",
     "shell.execute_reply.started": "2025-11-11T01:28:59.947446Z"
    }
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "\n",
    "# --------- 1) 파일 로더 ----------\n",
    "def load_class_mapping(path):\n",
    "    \"\"\"classes.txt: '<id> <key>' 형식을 (id오름차순)으로 반환\"\"\"\n",
    "    pairs = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            raw = line.strip()\n",
    "            if not raw or raw.startswith(\"#\"):\n",
    "                continue\n",
    "            parts = raw.split()\n",
    "            if len(parts) < 2:\n",
    "                continue\n",
    "            try:\n",
    "                cid = int(parts[0])\n",
    "            except ValueError:\n",
    "                continue\n",
    "            key = parts[1].strip()\n",
    "            pairs.append((cid, key))\n",
    "    # id 정렬 + 중복 id 처리\n",
    "    dedup = {}\n",
    "    for cid, key in sorted(pairs, key=lambda x: x[0]):\n",
    "        dedup[cid] = key\n",
    "    return [ (cid, dedup[cid]) for cid in sorted(dedup.keys()) ]\n",
    "\n",
    "def load_class_keywords(path):\n",
    "    \"\"\"'key: v1, v2, ...' → dict[key]=[values]\"\"\"\n",
    "    d = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            raw = line.strip()\n",
    "            if not raw or raw.startswith(\"#\"):\n",
    "                continue\n",
    "            if \":\" not in raw:\n",
    "                continue\n",
    "            key, rest = raw.split(\":\", 1)\n",
    "            key = key.strip()\n",
    "            vals = [v.strip() for v in rest.split(\",\") if v.strip()]\n",
    "            d[key] = vals\n",
    "    return d\n",
    "\n",
    "# --------- 2) 하드 프롬프트 ----------\n",
    "def _pretty(x): return x.replace(\"_\", \" \").strip()\n",
    "\n",
    "def make_prompt(key, values):\n",
    "    key_txt = _pretty(key)\n",
    "    vals = [_pretty(v) for v in values] if values else []\n",
    "    if not vals:\n",
    "        return f'This is the class \"{key_txt}\".'\n",
    "    if len(vals) == 1:\n",
    "        vals_txt = vals[0]\n",
    "    else:\n",
    "        vals_txt = \", \".join(vals[:-1]) + f\", and {vals[-1]}\"\n",
    "    return f'This is the class \"{key_txt}\". It is related to {vals_txt}.'\n",
    "\n",
    "# --------- 3) BERT 임베더 (CLS+mean) ----------\n",
    "class LabelEncoder(nn.Module):\n",
    "    def __init__(self, model_name=\"bert-base-uncased\", alpha=0.7, l2norm=True):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.alpha = alpha\n",
    "        self.l2norm = l2norm\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
    "        out = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            return_dict=True\n",
    "        )\n",
    "        last = out.last_hidden_state        # [B,T,H]\n",
    "        cls = last[:, 0, :]                 # [B,H]\n",
    "        # 패딩/CLS 제외 mean\n",
    "        mask = attention_mask.clone()       # [B,T]\n",
    "        mask[:, 0] = 0\n",
    "        lengths = mask.sum(1, keepdim=True).clamp(min=1)\n",
    "        mean = (last * mask.unsqueeze(-1)).sum(1) / lengths\n",
    "        h = self.alpha * cls + (1 - self.alpha) * mean\n",
    "        if self.l2norm:\n",
    "            h = F.normalize(h, p=2, dim=-1)\n",
    "        return h\n",
    "\n",
    "# --------- 4) 엔드투엔드: CSV(숫자만) 저장 ----------\n",
    "@torch.no_grad()\n",
    "def build_and_save_csv_with_headers(\n",
    "    classes_path=\"classes.txt\",\n",
    "    keywords_path=\"class_related_keywords.txt\",\n",
    "    model_name=\"bert-base-uncased\",\n",
    "    alpha=0.7,\n",
    "    batch_size=32,\n",
    "    max_length=128,\n",
    "    out_csv=\"class_embeddings_with_id.csv\",\n",
    "    pad_width=None,   # None이면 자동(특징 수에 맞춰 최소 자릿수), 정수로 강제 가능(예: 2 → feat00)\n",
    "):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    mapping = load_class_mapping(classes_path)           # [(id, key)]  id 오름차순\n",
    "    kw = load_class_keywords(keywords_path)              # key -> [values]\n",
    "\n",
    "    keys_in_order = [key for _, key in mapping]\n",
    "    prompts = [make_prompt(k, kw.get(k, [])) for k in keys_in_order]\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = LabelEncoder(model_name=model_name, alpha=alpha, l2norm=True).to(device).eval()\n",
    "\n",
    "    embs = []\n",
    "    for i in range(0, len(prompts), batch_size):\n",
    "        batch = prompts[i:i+batch_size]\n",
    "        enc = tokenizer(batch, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "        h = model(input_ids=enc[\"input_ids\"].to(device),\n",
    "                  attention_mask=enc[\"attention_mask\"].to(device),\n",
    "                  token_type_ids=enc.get(\"token_type_ids\").to(device) if enc.get(\"token_type_ids\") is not None else None)\n",
    "        embs.append(h.cpu())\n",
    "    E = torch.cat(embs, dim=0).numpy()   # [N, H]\n",
    "    \n",
    "    # E: [N, 768] (CSV 저장 직전의 임베딩 행렬)\n",
    "    # ids: classes.txt의 id 리스트\n",
    "    \n",
    "\n",
    "    # ---- 헤더 만들기: feat000, feat001, ... ----\n",
    "    H = E.shape[1]\n",
    "    auto_width = len(str(H-1)) if H > 1 else 1\n",
    "    width = max(2, auto_width) if pad_width is None else pad_width   # 최소 2자리는 보장(요청 반영)\n",
    "    feat_cols = [f\"feat{str(i).zfill(width)}\" for i in range(H)]\n",
    "\n",
    "\n",
    "    ids = [cid for cid, _ in mapping]    # classes.txt의 id 순서\n",
    "\n",
    "    df = pd.DataFrame(E, columns=feat_cols)\n",
    "    df.insert(0, \"id\", ids)\n",
    "    df.to_csv(out_csv, index=False,encoding=\"utf-8\")\n",
    "    print(f\"[OK] saved → {out_csv}  shape={df.shape}  (rows=id order from classes.txt)\")\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25be29e9-f98c-4e0a-b9b1-9b63dfbfa81f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T01:28:59.989805Z",
     "iopub.status.busy": "2025-11-11T01:28:59.989648Z",
     "iopub.status.idle": "2025-11-11T01:29:01.726935Z",
     "shell.execute_reply": "2025-11-11T01:29:01.726463Z",
     "shell.execute_reply.started": "2025-11-11T01:28:59.989790Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] saved → Amazon_products/class_embeddings_matrix.csv  shape=(531, 769)  (rows=id order from classes.txt)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>feat000</th>\n",
       "      <th>feat001</th>\n",
       "      <th>feat002</th>\n",
       "      <th>feat003</th>\n",
       "      <th>feat004</th>\n",
       "      <th>feat005</th>\n",
       "      <th>feat006</th>\n",
       "      <th>feat007</th>\n",
       "      <th>feat008</th>\n",
       "      <th>...</th>\n",
       "      <th>feat758</th>\n",
       "      <th>feat759</th>\n",
       "      <th>feat760</th>\n",
       "      <th>feat761</th>\n",
       "      <th>feat762</th>\n",
       "      <th>feat763</th>\n",
       "      <th>feat764</th>\n",
       "      <th>feat765</th>\n",
       "      <th>feat766</th>\n",
       "      <th>feat767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.010211</td>\n",
       "      <td>-0.011956</td>\n",
       "      <td>0.002603</td>\n",
       "      <td>-0.000184</td>\n",
       "      <td>0.000567</td>\n",
       "      <td>-0.032215</td>\n",
       "      <td>-0.016893</td>\n",
       "      <td>0.063920</td>\n",
       "      <td>-0.027164</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013105</td>\n",
       "      <td>-0.032628</td>\n",
       "      <td>-0.005374</td>\n",
       "      <td>-0.015925</td>\n",
       "      <td>0.006762</td>\n",
       "      <td>-0.012706</td>\n",
       "      <td>-0.005882</td>\n",
       "      <td>-0.013056</td>\n",
       "      <td>0.011159</td>\n",
       "      <td>0.058906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.017012</td>\n",
       "      <td>-0.016444</td>\n",
       "      <td>-0.000128</td>\n",
       "      <td>-0.011100</td>\n",
       "      <td>-0.025949</td>\n",
       "      <td>-0.002887</td>\n",
       "      <td>-0.025018</td>\n",
       "      <td>0.049013</td>\n",
       "      <td>-0.033802</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007400</td>\n",
       "      <td>-0.008585</td>\n",
       "      <td>-0.007387</td>\n",
       "      <td>-0.026812</td>\n",
       "      <td>0.019292</td>\n",
       "      <td>0.020590</td>\n",
       "      <td>0.003997</td>\n",
       "      <td>-0.000089</td>\n",
       "      <td>0.029783</td>\n",
       "      <td>0.046100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.034676</td>\n",
       "      <td>-0.019762</td>\n",
       "      <td>-0.008492</td>\n",
       "      <td>-0.011459</td>\n",
       "      <td>-0.022838</td>\n",
       "      <td>-0.006900</td>\n",
       "      <td>-0.010470</td>\n",
       "      <td>0.080544</td>\n",
       "      <td>-0.028093</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023415</td>\n",
       "      <td>-0.017251</td>\n",
       "      <td>-0.005550</td>\n",
       "      <td>-0.031397</td>\n",
       "      <td>0.019254</td>\n",
       "      <td>0.012294</td>\n",
       "      <td>0.000938</td>\n",
       "      <td>0.000992</td>\n",
       "      <td>0.038855</td>\n",
       "      <td>0.031516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.011635</td>\n",
       "      <td>-0.017314</td>\n",
       "      <td>0.028135</td>\n",
       "      <td>0.017923</td>\n",
       "      <td>-0.013000</td>\n",
       "      <td>-0.041281</td>\n",
       "      <td>-0.012311</td>\n",
       "      <td>0.041047</td>\n",
       "      <td>-0.049561</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011168</td>\n",
       "      <td>-0.017969</td>\n",
       "      <td>0.001752</td>\n",
       "      <td>-0.028430</td>\n",
       "      <td>0.036308</td>\n",
       "      <td>0.023512</td>\n",
       "      <td>-0.017785</td>\n",
       "      <td>-0.008129</td>\n",
       "      <td>-0.000691</td>\n",
       "      <td>0.092230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.007398</td>\n",
       "      <td>-0.023895</td>\n",
       "      <td>0.033761</td>\n",
       "      <td>0.002061</td>\n",
       "      <td>-0.029120</td>\n",
       "      <td>-0.016012</td>\n",
       "      <td>-0.012304</td>\n",
       "      <td>0.056903</td>\n",
       "      <td>-0.053605</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.032577</td>\n",
       "      <td>0.002364</td>\n",
       "      <td>-0.035561</td>\n",
       "      <td>0.031751</td>\n",
       "      <td>0.030951</td>\n",
       "      <td>-0.018557</td>\n",
       "      <td>-0.018734</td>\n",
       "      <td>-0.009401</td>\n",
       "      <td>0.093541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>526</td>\n",
       "      <td>-0.021016</td>\n",
       "      <td>-0.019151</td>\n",
       "      <td>0.041768</td>\n",
       "      <td>0.016444</td>\n",
       "      <td>-0.007131</td>\n",
       "      <td>-0.041068</td>\n",
       "      <td>-0.020283</td>\n",
       "      <td>0.000831</td>\n",
       "      <td>-0.017557</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040430</td>\n",
       "      <td>-0.030489</td>\n",
       "      <td>0.003122</td>\n",
       "      <td>-0.030808</td>\n",
       "      <td>-0.006206</td>\n",
       "      <td>0.015905</td>\n",
       "      <td>-0.015440</td>\n",
       "      <td>-0.029978</td>\n",
       "      <td>-0.015221</td>\n",
       "      <td>0.062001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>527</td>\n",
       "      <td>-0.032303</td>\n",
       "      <td>-0.000794</td>\n",
       "      <td>-0.006452</td>\n",
       "      <td>-0.038560</td>\n",
       "      <td>-0.058474</td>\n",
       "      <td>-0.003561</td>\n",
       "      <td>0.006136</td>\n",
       "      <td>0.049258</td>\n",
       "      <td>-0.041083</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018024</td>\n",
       "      <td>-0.014804</td>\n",
       "      <td>-0.005148</td>\n",
       "      <td>-0.028593</td>\n",
       "      <td>0.047571</td>\n",
       "      <td>0.013935</td>\n",
       "      <td>-0.003430</td>\n",
       "      <td>0.007667</td>\n",
       "      <td>0.037184</td>\n",
       "      <td>0.045884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>528</td>\n",
       "      <td>-0.044695</td>\n",
       "      <td>-0.023169</td>\n",
       "      <td>0.058748</td>\n",
       "      <td>0.001255</td>\n",
       "      <td>-0.017672</td>\n",
       "      <td>-0.035153</td>\n",
       "      <td>-0.003713</td>\n",
       "      <td>-0.009553</td>\n",
       "      <td>-0.018632</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016477</td>\n",
       "      <td>-0.039585</td>\n",
       "      <td>-0.008159</td>\n",
       "      <td>-0.049479</td>\n",
       "      <td>0.033699</td>\n",
       "      <td>0.001436</td>\n",
       "      <td>-0.002274</td>\n",
       "      <td>-0.023200</td>\n",
       "      <td>-0.037373</td>\n",
       "      <td>0.054765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>529</td>\n",
       "      <td>-0.023375</td>\n",
       "      <td>-0.003089</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>-0.008055</td>\n",
       "      <td>-0.025490</td>\n",
       "      <td>0.001360</td>\n",
       "      <td>-0.010454</td>\n",
       "      <td>0.081578</td>\n",
       "      <td>-0.038161</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001798</td>\n",
       "      <td>-0.038106</td>\n",
       "      <td>-0.004135</td>\n",
       "      <td>-0.045608</td>\n",
       "      <td>0.017642</td>\n",
       "      <td>-0.030496</td>\n",
       "      <td>-0.018465</td>\n",
       "      <td>0.013859</td>\n",
       "      <td>-0.008486</td>\n",
       "      <td>0.050243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>530</td>\n",
       "      <td>0.007190</td>\n",
       "      <td>-0.002431</td>\n",
       "      <td>0.006739</td>\n",
       "      <td>-0.010280</td>\n",
       "      <td>-0.001189</td>\n",
       "      <td>0.002834</td>\n",
       "      <td>-0.002730</td>\n",
       "      <td>0.031665</td>\n",
       "      <td>-0.021117</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012774</td>\n",
       "      <td>-0.010192</td>\n",
       "      <td>0.001767</td>\n",
       "      <td>-0.027629</td>\n",
       "      <td>0.026338</td>\n",
       "      <td>0.035504</td>\n",
       "      <td>0.002283</td>\n",
       "      <td>-0.027705</td>\n",
       "      <td>-0.016293</td>\n",
       "      <td>0.094173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>531 rows × 769 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id   feat000   feat001   feat002   feat003   feat004   feat005  \\\n",
       "0      0 -0.010211 -0.011956  0.002603 -0.000184  0.000567 -0.032215   \n",
       "1      1 -0.017012 -0.016444 -0.000128 -0.011100 -0.025949 -0.002887   \n",
       "2      2 -0.034676 -0.019762 -0.008492 -0.011459 -0.022838 -0.006900   \n",
       "3      3 -0.011635 -0.017314  0.028135  0.017923 -0.013000 -0.041281   \n",
       "4      4 -0.007398 -0.023895  0.033761  0.002061 -0.029120 -0.016012   \n",
       "..   ...       ...       ...       ...       ...       ...       ...   \n",
       "526  526 -0.021016 -0.019151  0.041768  0.016444 -0.007131 -0.041068   \n",
       "527  527 -0.032303 -0.000794 -0.006452 -0.038560 -0.058474 -0.003561   \n",
       "528  528 -0.044695 -0.023169  0.058748  0.001255 -0.017672 -0.035153   \n",
       "529  529 -0.023375 -0.003089  0.006700 -0.008055 -0.025490  0.001360   \n",
       "530  530  0.007190 -0.002431  0.006739 -0.010280 -0.001189  0.002834   \n",
       "\n",
       "      feat006   feat007   feat008  ...   feat758   feat759   feat760  \\\n",
       "0   -0.016893  0.063920 -0.027164  ... -0.013105 -0.032628 -0.005374   \n",
       "1   -0.025018  0.049013 -0.033802  ...  0.007400 -0.008585 -0.007387   \n",
       "2   -0.010470  0.080544 -0.028093  ...  0.023415 -0.017251 -0.005550   \n",
       "3   -0.012311  0.041047 -0.049561  ... -0.011168 -0.017969  0.001752   \n",
       "4   -0.012304  0.056903 -0.053605  ... -0.000004 -0.032577  0.002364   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "526 -0.020283  0.000831 -0.017557  ...  0.040430 -0.030489  0.003122   \n",
       "527  0.006136  0.049258 -0.041083  ...  0.018024 -0.014804 -0.005148   \n",
       "528 -0.003713 -0.009553 -0.018632  ...  0.016477 -0.039585 -0.008159   \n",
       "529 -0.010454  0.081578 -0.038161  ... -0.001798 -0.038106 -0.004135   \n",
       "530 -0.002730  0.031665 -0.021117  ...  0.012774 -0.010192  0.001767   \n",
       "\n",
       "      feat761   feat762   feat763   feat764   feat765   feat766   feat767  \n",
       "0   -0.015925  0.006762 -0.012706 -0.005882 -0.013056  0.011159  0.058906  \n",
       "1   -0.026812  0.019292  0.020590  0.003997 -0.000089  0.029783  0.046100  \n",
       "2   -0.031397  0.019254  0.012294  0.000938  0.000992  0.038855  0.031516  \n",
       "3   -0.028430  0.036308  0.023512 -0.017785 -0.008129 -0.000691  0.092230  \n",
       "4   -0.035561  0.031751  0.030951 -0.018557 -0.018734 -0.009401  0.093541  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "526 -0.030808 -0.006206  0.015905 -0.015440 -0.029978 -0.015221  0.062001  \n",
       "527 -0.028593  0.047571  0.013935 -0.003430  0.007667  0.037184  0.045884  \n",
       "528 -0.049479  0.033699  0.001436 -0.002274 -0.023200 -0.037373  0.054765  \n",
       "529 -0.045608  0.017642 -0.030496 -0.018465  0.013859 -0.008486  0.050243  \n",
       "530 -0.027629  0.026338  0.035504  0.002283 -0.027705 -0.016293  0.094173  \n",
       "\n",
       "[531 rows x 769 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "build_and_save_csv_with_headers(\n",
    "    classes_path=\"Amazon_products/classes.txt\",\n",
    "    keywords_path=\"Amazon_products/class_related_keywords.txt\",\n",
    "    model_name=\"bert-base-uncased\",\n",
    "    alpha=0.7,\n",
    "    batch_size=32,\n",
    "    max_length=128,\n",
    "    out_csv=\"Amazon_products/class_embeddings_matrix.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5efad2e-4006-4b90-920b-1e70cc6b7ca7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T01:29:01.727484Z",
     "iopub.status.busy": "2025-11-11T01:29:01.727342Z",
     "iopub.status.idle": "2025-11-11T01:29:01.732389Z",
     "shell.execute_reply": "2025-11-11T01:29:01.731973Z",
     "shell.execute_reply.started": "2025-11-11T01:29:01.727471Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roots: [0, 3, 10, 23, 40, 169]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_edges(path):\n",
    "    edges = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            raw = line.strip()\n",
    "            if not raw or raw.startswith(\"#\"):\n",
    "                continue\n",
    "            parts = raw.split()\n",
    "            if len(parts) < 2:\n",
    "                continue\n",
    "            try:\n",
    "                u, v = int(parts[0]), int(parts[1])\n",
    "            except ValueError:\n",
    "                continue\n",
    "            edges.append((u, v))\n",
    "    return edges\n",
    "\n",
    "def find_roots(edges):\n",
    "    parents = set()\n",
    "    children = set()\n",
    "    for u, v in edges:\n",
    "        parents.add(u)\n",
    "        children.add(v)\n",
    "    # 부모로만 나온 애들 = 루트들\n",
    "    roots = parents - children\n",
    "    return sorted(roots)\n",
    "\n",
    "# --- 사용 ---\n",
    "E = load_edges(\"Amazon_products/class_hierarchy.txt\")\n",
    "\n",
    "N = 531\n",
    "A = np.zeros((N, N), dtype=np.uint8)\n",
    "for u, v in E:\n",
    "    A[u, v] = 1\n",
    "    A[v, u] = 1   # 탐색용으로는 무방향 인접행렬 써도 됨\n",
    "\n",
    "roots = find_roots(E)\n",
    "print(\"roots:\", roots)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cde32d7f-3c5e-4810-ab28-07a7abd05c59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T01:29:01.902479Z",
     "iopub.status.busy": "2025-11-11T01:29:01.902282Z",
     "iopub.status.idle": "2025-11-11T01:29:01.909706Z",
     "shell.execute_reply": "2025-11-11T01:29:01.909247Z",
     "shell.execute_reply.started": "2025-11-11T01:29:01.902464Z"
    }
   },
   "outputs": [],
   "source": [
    "EMB_CSV  = \"Amazon_products/class_embeddings_matrix.csv\"   # id + featXX ... (초기 임베딩)\n",
    "OUT_CSV  = \"Amazon_products/label_embeddings_gat.csv\"\n",
    "\n",
    "def load_initial_embeddings(path):\n",
    "    df = pd.read_csv(path, encoding=\"utf-8-sig\")  # 먼저 시도\n",
    "    ids = df.iloc[:, 0].astype(int).tolist()\n",
    "    X = df.iloc[:, 1:].to_numpy(dtype=np.float32)  # [N, d]\n",
    "    return ids, X\n",
    "\n",
    "# ---------------------------\n",
    "# GAT \n",
    "# ---------------------------\n",
    "\n",
    "class SimpleGATLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, heads=4, concat=True, dropout=0.2, negative_slope=0.2, residual=True):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.out_dim = out_dim\n",
    "        self.concat = concat\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.leaky_relu = nn.LeakyReLU(negative_slope)\n",
    "        self.lin = nn.Linear(in_dim, heads * out_dim, bias=False)\n",
    "        self.a_src = nn.Parameter(torch.Tensor(heads, out_dim))\n",
    "        self.a_dst = nn.Parameter(torch.Tensor(heads, out_dim))\n",
    "        self.residual = residual\n",
    "        if residual and (in_dim == (heads * out_dim if concat else out_dim)):\n",
    "            self.res_proj = nn.Identity()\n",
    "        elif residual:\n",
    "            self.res_proj = nn.Linear(in_dim, heads * out_dim if concat else out_dim, bias=False)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.lin.weight)\n",
    "        nn.init.xavier_uniform_(self.a_src)\n",
    "        nn.init.xavier_uniform_(self.a_dst)\n",
    "        if self.residual and not isinstance(getattr(self, \"res_proj\", None), nn.Identity):\n",
    "            nn.init.xavier_uniform_(self.res_proj.weight)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        \"\"\"\n",
    "        x: [N, Fin]\n",
    "        adj: [N, N] (0/1; self-loop 없음)\n",
    "        \"\"\"\n",
    "        N = x.size(0)\n",
    "        Wh = self.lin(x).view(N, self.heads, self.out_dim)  # [N, H, F]\n",
    "\n",
    "        e_src = (Wh * self.a_src).sum(dim=-1)  # [N, H]\n",
    "        e_dst = (Wh * self.a_dst).sum(dim=-1)  # [N, H]\n",
    "        e = e_src.unsqueeze(1) + e_dst.unsqueeze(0)  # [N, N, H]\n",
    "        e = self.leaky_relu(e)\n",
    "        # --- 안전한 masked softmax ---\n",
    "        mask = (adj > 0).unsqueeze(-1)                    # [N, N, 1]\n",
    "        e = e.masked_fill(~mask, -1e9)                    # -inf 대신 -1e9로 NaN 방지\n",
    "        alpha = torch.softmax(e, dim=1)                   # 소프트맥스\n",
    "        alpha = alpha * mask.float()                      # 마스크로 0 처리\n",
    "        denom = alpha.sum(dim=1, keepdim=True).clamp(min=1e-12)  # 이웃 없을 때 0 분모 방지\n",
    "        alpha = alpha / denom                             # 이웃들로 정규화\n",
    "\n",
    "        out = torch.einsum(\"ijh,jhf->ihf\", alpha, Wh)     # [N, H, F]\n",
    "        out = out.reshape(N, self.heads * self.out_dim) if self.concat else out.mean(dim=1)\n",
    "        out = self.dropout(out)\n",
    "        if self.residual:\n",
    "            out = out + self.res_proj(x)                  # self-loop 없는 대신 residual로 자기정보 유지\n",
    "        return out\n",
    "\n",
    "class GATEncoder(nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim=64, out_dim=768, heads1=4, heads2=4, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.gat1 = SimpleGATLayer(in_dim, hid_dim, heads=heads1, concat=True,  dropout=dropout, residual=True)\n",
    "        self.gat2 = SimpleGATLayer(hid_dim*heads1, out_dim, heads=heads2, concat=False, dropout=dropout, residual=True)\n",
    "        self.act = nn.ELU(); self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x, adj):\n",
    "        h = self.gat1(x, adj); h = self.act(h); h = self.dropout(h)\n",
    "        z = self.gat2(h, adj)\n",
    "        return z  # [N, out_dim]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da4d3075-be19-4053-9b2f-827bcd31d952",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T01:29:04.830555Z",
     "iopub.status.busy": "2025-11-11T01:29:04.830248Z",
     "iopub.status.idle": "2025-11-11T01:29:14.443316Z",
     "shell.execute_reply": "2025-11-11T01:29:14.442801Z",
     "shell.execute_reply.started": "2025-11-11T01:29:04.830532Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001/700] loss=0.7186 | pos=0.661 neg=0.572 | val AUC=0.2631\n",
      "[002/700] loss=0.6633 | pos=0.399 neg=0.223 | val AUC=0.3791\n",
      "[003/700] loss=0.6425 | pos=0.450 neg=0.181 | val AUC=0.6173\n",
      "[004/700] loss=0.6169 | pos=0.489 neg=0.108 | val AUC=0.6999\n",
      "[005/700] loss=0.5937 | pos=0.559 neg=0.060 | val AUC=0.7334\n",
      "[006/700] loss=0.5713 | pos=0.653 neg=0.024 | val AUC=0.6256\n",
      "[007/700] loss=0.5601 | pos=0.712 neg=0.012 | val AUC=0.7302\n",
      "[008/700] loss=0.5584 | pos=0.755 neg=0.026 | val AUC=0.7490\n",
      "[009/700] loss=0.5484 | pos=0.785 neg=-0.000 | val AUC=0.7143\n",
      "[010/700] loss=0.5562 | pos=0.817 neg=0.042 | val AUC=0.7513\n",
      "[011/700] loss=0.5509 | pos=0.834 neg=0.027 | val AUC=0.7589\n",
      "[012/700] loss=0.5459 | pos=0.842 neg=0.011 | val AUC=0.7988\n",
      "[013/700] loss=0.5391 | pos=0.842 neg=-0.014 | val AUC=0.7506\n",
      "[014/700] loss=0.5331 | pos=0.853 neg=-0.035 | val AUC=0.8600\n",
      "[015/700] loss=0.5366 | pos=0.870 neg=-0.015 | val AUC=0.7618\n",
      "[016/700] loss=0.5396 | pos=0.885 neg=0.002 | val AUC=0.7844\n",
      "[017/700] loss=0.5432 | pos=0.898 neg=0.017 | val AUC=0.8208\n",
      "[018/700] loss=0.5268 | pos=0.903 neg=-0.038 | val AUC=0.7669\n",
      "[019/700] loss=0.5315 | pos=0.906 neg=-0.018 | val AUC=0.8115\n",
      "[020/700] loss=0.5333 | pos=0.908 neg=-0.013 | val AUC=0.8463\n",
      "[021/700] loss=0.5381 | pos=0.910 neg=0.012 | val AUC=0.8390\n",
      "[022/700] loss=0.5345 | pos=0.910 neg=-0.002 | val AUC=0.8122\n",
      "[023/700] loss=0.5307 | pos=0.913 neg=-0.018 | val AUC=0.7895\n",
      "[024/700] loss=0.5434 | pos=0.914 neg=0.030 | val AUC=0.8211\n",
      "[025/700] loss=0.5305 | pos=0.917 neg=-0.017 | val AUC=0.7828\n",
      "[026/700] loss=0.5531 | pos=0.919 neg=0.076 | val AUC=0.7462\n",
      "[027/700] loss=0.5257 | pos=0.923 neg=-0.031 | val AUC=0.8418\n",
      "[028/700] loss=0.5283 | pos=0.926 neg=-0.018 | val AUC=0.8909\n",
      "[029/700] loss=0.5330 | pos=0.925 neg=0.003 | val AUC=0.8278\n",
      "[030/700] loss=0.5378 | pos=0.926 neg=0.019 | val AUC=0.8520\n",
      "[031/700] loss=0.5301 | pos=0.926 neg=-0.013 | val AUC=0.8648\n",
      "[032/700] loss=0.5455 | pos=0.924 neg=0.045 | val AUC=0.8750\n",
      "[033/700] loss=0.5226 | pos=0.924 neg=-0.039 | val AUC=0.8406\n",
      "[034/700] loss=0.5272 | pos=0.923 neg=-0.019 | val AUC=0.8332\n",
      "[035/700] loss=0.5312 | pos=0.922 neg=-0.001 | val AUC=0.8645\n",
      "[036/700] loss=0.5448 | pos=0.925 neg=0.047 | val AUC=0.8329\n",
      "[037/700] loss=0.5341 | pos=0.926 neg=0.004 | val AUC=0.8339\n",
      "[038/700] loss=0.5435 | pos=0.927 neg=0.041 | val AUC=0.8237\n",
      "[039/700] loss=0.5321 | pos=0.927 neg=0.003 | val AUC=0.7975\n",
      "[040/700] loss=0.5373 | pos=0.925 neg=0.020 | val AUC=0.8221\n",
      "[041/700] loss=0.5331 | pos=0.923 neg=0.003 | val AUC=0.8584\n",
      "[042/700] loss=0.5394 | pos=0.922 neg=0.030 | val AUC=0.7828\n",
      "[043/700] loss=0.5379 | pos=0.924 neg=0.030 | val AUC=0.8112\n",
      "[044/700] loss=0.5258 | pos=0.926 neg=-0.027 | val AUC=0.8565\n",
      "[045/700] loss=0.5323 | pos=0.928 neg=-0.001 | val AUC=0.8093\n",
      "[046/700] loss=0.5328 | pos=0.929 neg=0.008 | val AUC=0.8689\n",
      "[047/700] loss=0.5377 | pos=0.929 neg=0.027 | val AUC=0.8485\n",
      "[048/700] loss=0.5285 | pos=0.928 neg=-0.011 | val AUC=0.7624\n",
      "[049/700] loss=0.5287 | pos=0.928 neg=-0.011 | val AUC=0.8849\n",
      "[050/700] loss=0.5176 | pos=0.927 neg=-0.047 | val AUC=0.8170\n",
      "[051/700] loss=0.5361 | pos=0.927 neg=0.016 | val AUC=0.8096\n",
      "[052/700] loss=0.5375 | pos=0.928 neg=0.029 | val AUC=0.8527\n",
      "[053/700] loss=0.5269 | pos=0.929 neg=-0.010 | val AUC=0.8527\n",
      "[054/700] loss=0.5211 | pos=0.930 neg=-0.027 | val AUC=0.8106\n",
      "[055/700] loss=0.5264 | pos=0.929 neg=-0.012 | val AUC=0.8409\n",
      "[056/700] loss=0.5347 | pos=0.929 neg=0.019 | val AUC=0.7781\n",
      "[057/700] loss=0.5302 | pos=0.930 neg=-0.000 | val AUC=0.8960\n",
      "[058/700] loss=0.5328 | pos=0.932 neg=0.014 | val AUC=0.8479\n",
      "[059/700] loss=0.5459 | pos=0.933 neg=0.065 | val AUC=0.8705\n",
      "[060/700] loss=0.5276 | pos=0.934 neg=-0.010 | val AUC=0.8689\n",
      "[061/700] loss=0.5402 | pos=0.933 neg=0.040 | val AUC=0.8367\n",
      "[062/700] loss=0.5359 | pos=0.932 neg=0.026 | val AUC=0.8693\n",
      "[063/700] loss=0.5200 | pos=0.934 neg=-0.034 | val AUC=0.7717\n",
      "[064/700] loss=0.5283 | pos=0.934 neg=-0.000 | val AUC=0.8460\n",
      "[065/700] loss=0.5331 | pos=0.935 neg=0.017 | val AUC=0.8096\n",
      "[066/700] loss=0.5323 | pos=0.938 neg=0.011 | val AUC=0.8144\n",
      "[067/700] loss=0.5251 | pos=0.936 neg=-0.017 | val AUC=0.7943\n",
      "[068/700] loss=0.5257 | pos=0.934 neg=-0.010 | val AUC=0.7739\n",
      "[069/700] loss=0.5325 | pos=0.933 neg=0.017 | val AUC=0.8355\n",
      "[070/700] loss=0.5377 | pos=0.933 neg=0.040 | val AUC=0.8249\n",
      "[071/700] loss=0.5365 | pos=0.935 neg=0.031 | val AUC=0.7997\n",
      "[072/700] loss=0.5185 | pos=0.938 neg=-0.035 | val AUC=0.7860\n",
      "[073/700] loss=0.5269 | pos=0.936 neg=-0.002 | val AUC=0.7860\n",
      "[074/700] loss=0.5225 | pos=0.934 neg=-0.014 | val AUC=0.7848\n",
      "[075/700] loss=0.5313 | pos=0.934 neg=0.020 | val AUC=0.8342\n",
      "[076/700] loss=0.5309 | pos=0.936 neg=0.014 | val AUC=0.8476\n",
      "[077/700] loss=0.5335 | pos=0.937 neg=0.027 | val AUC=0.8619\n",
      "[078/700] loss=0.5326 | pos=0.936 neg=0.025 | val AUC=0.7701\n",
      "[079/700] loss=0.5257 | pos=0.938 neg=0.000 | val AUC=0.7905\n",
      "[080/700] loss=0.5259 | pos=0.939 neg=-0.000 | val AUC=0.8310\n",
      "[081/700] loss=0.5280 | pos=0.938 neg=0.011 | val AUC=0.8265\n",
      "[082/700] loss=0.5325 | pos=0.936 neg=0.019 | val AUC=0.7392\n",
      "[083/700] loss=0.5372 | pos=0.935 neg=0.041 | val AUC=0.7937\n",
      "[084/700] loss=0.5267 | pos=0.935 neg=-0.000 | val AUC=0.7296\n",
      "[085/700] loss=0.5272 | pos=0.937 neg=0.006 | val AUC=0.7927\n",
      "[086/700] loss=0.5270 | pos=0.938 neg=-0.003 | val AUC=0.7975\n",
      "[087/700] loss=0.5305 | pos=0.939 neg=0.009 | val AUC=0.7545\n",
      "[088/700] loss=0.5193 | pos=0.940 neg=-0.029 | val AUC=0.7401\n",
      "[089/700] loss=0.5273 | pos=0.943 neg=0.007 | val AUC=0.7293\n",
      "[090/700] loss=0.5184 | pos=0.942 neg=-0.029 | val AUC=0.7554\n",
      "[091/700] loss=0.5293 | pos=0.943 neg=0.012 | val AUC=0.7334\n",
      "[092/700] loss=0.5150 | pos=0.945 neg=-0.046 | val AUC=0.6738\n",
      "[093/700] loss=0.5307 | pos=0.946 neg=0.014 | val AUC=0.7637\n",
      "[094/700] loss=0.5180 | pos=0.945 neg=-0.034 | val AUC=0.7213\n",
      "[095/700] loss=0.5428 | pos=0.947 neg=0.063 | val AUC=0.7691\n",
      "[096/700] loss=0.5248 | pos=0.947 neg=-0.002 | val AUC=0.6527\n",
      "[097/700] loss=0.5359 | pos=0.949 neg=0.036 | val AUC=0.7832\n",
      "[098/700] loss=0.5187 | pos=0.948 neg=-0.033 | val AUC=0.7701\n",
      "[099/700] loss=0.5259 | pos=0.947 neg=0.002 | val AUC=0.7714\n",
      "[100/700] loss=0.5290 | pos=0.947 neg=0.012 | val AUC=0.7806\n",
      "[101/700] loss=0.5315 | pos=0.947 neg=0.023 | val AUC=0.8313\n",
      "[102/700] loss=0.5230 | pos=0.946 neg=-0.008 | val AUC=0.7930\n",
      "[103/700] loss=0.5146 | pos=0.945 neg=-0.041 | val AUC=0.8036\n",
      "[104/700] loss=0.5194 | pos=0.946 neg=-0.024 | val AUC=0.8533\n",
      "[105/700] loss=0.5279 | pos=0.947 neg=0.008 | val AUC=0.8115\n",
      "[106/700] loss=0.5223 | pos=0.948 neg=-0.005 | val AUC=0.7937\n",
      "[107/700] loss=0.5215 | pos=0.948 neg=-0.007 | val AUC=0.7698\n",
      "[108/700] loss=0.5300 | pos=0.948 neg=0.020 | val AUC=0.7803\n",
      "[109/700] loss=0.5266 | pos=0.948 neg=0.009 | val AUC=0.8023\n",
      "[110/700] loss=0.5181 | pos=0.949 neg=-0.019 | val AUC=0.7124\n",
      "[111/700] loss=0.5155 | pos=0.949 neg=-0.035 | val AUC=0.7452\n",
      "[112/700] loss=0.5297 | pos=0.947 neg=0.019 | val AUC=0.7334\n",
      "[113/700] loss=0.5126 | pos=0.950 neg=-0.050 | val AUC=0.7098\n",
      "[114/700] loss=0.5226 | pos=0.951 neg=-0.006 | val AUC=0.7596\n",
      "[115/700] loss=0.5234 | pos=0.952 neg=-0.008 | val AUC=0.7427\n",
      "[116/700] loss=0.5235 | pos=0.948 neg=-0.005 | val AUC=0.7612\n",
      "[117/700] loss=0.5306 | pos=0.950 neg=0.021 | val AUC=0.7500\n",
      "[118/700] loss=0.5279 | pos=0.951 neg=0.008 | val AUC=0.7446\n",
      "[119/700] loss=0.5214 | pos=0.950 neg=-0.008 | val AUC=0.7337\n",
      "[120/700] loss=0.5223 | pos=0.949 neg=-0.009 | val AUC=0.7659\n",
      "[121/700] loss=0.5228 | pos=0.950 neg=-0.003 | val AUC=0.7098\n",
      "[122/700] loss=0.5243 | pos=0.949 neg=0.000 | val AUC=0.7647\n",
      "[123/700] loss=0.5306 | pos=0.953 neg=0.026 | val AUC=0.7682\n",
      "[124/700] loss=0.5328 | pos=0.952 neg=0.035 | val AUC=0.7299\n",
      "[125/700] loss=0.5304 | pos=0.952 neg=0.025 | val AUC=0.7057\n",
      "[126/700] loss=0.5250 | pos=0.952 neg=0.008 | val AUC=0.7098\n",
      "[127/700] loss=0.5332 | pos=0.950 neg=0.037 | val AUC=0.8144\n",
      "[128/700] loss=0.5221 | pos=0.953 neg=-0.008 | val AUC=0.7605\n",
      "[129/700] loss=0.5264 | pos=0.957 neg=0.009 | val AUC=0.7844\n",
      "[130/700] loss=0.5310 | pos=0.955 neg=0.031 | val AUC=0.8033\n",
      "[131/700] loss=0.5228 | pos=0.953 neg=-0.005 | val AUC=0.7679\n",
      "[132/700] loss=0.5166 | pos=0.953 neg=-0.027 | val AUC=0.7911\n",
      "[133/700] loss=0.5360 | pos=0.958 neg=0.042 | val AUC=0.7714\n",
      "[134/700] loss=0.5277 | pos=0.957 neg=0.011 | val AUC=0.7969\n",
      "[135/700] loss=0.5291 | pos=0.955 neg=0.021 | val AUC=0.7510\n",
      "[136/700] loss=0.5142 | pos=0.955 neg=-0.037 | val AUC=0.7417\n",
      "[137/700] loss=0.5218 | pos=0.956 neg=-0.011 | val AUC=0.7784\n",
      "[138/700] loss=0.5175 | pos=0.959 neg=-0.027 | val AUC=0.7707\n",
      "[139/700] loss=0.5424 | pos=0.960 neg=0.071 | val AUC=0.8202\n",
      "[140/700] loss=0.5193 | pos=0.957 neg=-0.022 | val AUC=0.7707\n",
      "[141/700] loss=0.5189 | pos=0.958 neg=-0.021 | val AUC=0.7015\n",
      "[142/700] loss=0.5258 | pos=0.958 neg=0.003 | val AUC=0.7111\n",
      "[143/700] loss=0.5298 | pos=0.959 neg=0.022 | val AUC=0.7730\n",
      "[144/700] loss=0.5265 | pos=0.958 neg=0.008 | val AUC=0.7790\n",
      "[145/700] loss=0.5223 | pos=0.958 neg=-0.006 | val AUC=0.7420\n",
      "[146/700] loss=0.5251 | pos=0.957 neg=0.004 | val AUC=0.7643\n",
      "[147/700] loss=0.5299 | pos=0.957 neg=0.023 | val AUC=0.7098\n",
      "[148/700] loss=0.5234 | pos=0.955 neg=-0.002 | val AUC=0.6958\n",
      "[149/700] loss=0.5343 | pos=0.954 neg=0.038 | val AUC=0.7452\n",
      "[150/700] loss=0.5136 | pos=0.953 neg=-0.039 | val AUC=0.7439\n",
      "[151/700] loss=0.5230 | pos=0.954 neg=-0.001 | val AUC=0.7586\n",
      "[152/700] loss=0.5374 | pos=0.955 neg=0.050 | val AUC=0.6349\n",
      "[153/700] loss=0.5326 | pos=0.956 neg=0.029 | val AUC=0.8243\n",
      "[154/700] loss=0.5204 | pos=0.955 neg=-0.013 | val AUC=0.7624\n",
      "[155/700] loss=0.5210 | pos=0.955 neg=-0.010 | val AUC=0.6888\n",
      "[156/700] loss=0.5175 | pos=0.956 neg=-0.023 | val AUC=0.7350\n",
      "[157/700] loss=0.5298 | pos=0.956 neg=0.025 | val AUC=0.6738\n",
      "[158/700] loss=0.5229 | pos=0.954 neg=-0.006 | val AUC=0.8664\n",
      "[159/700] loss=0.5200 | pos=0.955 neg=-0.016 | val AUC=0.8320\n",
      "[160/700] loss=0.5353 | pos=0.956 neg=0.045 | val AUC=0.7612\n",
      "[161/700] loss=0.5283 | pos=0.957 neg=0.024 | val AUC=0.7302\n",
      "[162/700] loss=0.5247 | pos=0.959 neg=0.001 | val AUC=0.7481\n",
      "[163/700] loss=0.5214 | pos=0.959 neg=-0.005 | val AUC=0.7822\n",
      "[164/700] loss=0.5126 | pos=0.960 neg=-0.041 | val AUC=0.7695\n",
      "[165/700] loss=0.5305 | pos=0.961 neg=0.028 | val AUC=0.7806\n",
      "[166/700] loss=0.5262 | pos=0.959 neg=0.012 | val AUC=0.7229\n",
      "[167/700] loss=0.5142 | pos=0.958 neg=-0.035 | val AUC=0.7363\n",
      "[168/700] loss=0.5305 | pos=0.958 neg=0.023 | val AUC=0.8827\n",
      "[169/700] loss=0.5211 | pos=0.959 neg=-0.015 | val AUC=0.7092\n",
      "[170/700] loss=0.5247 | pos=0.960 neg=0.002 | val AUC=0.7726\n",
      "[171/700] loss=0.5268 | pos=0.960 neg=0.009 | val AUC=0.8645\n",
      "[172/700] loss=0.5306 | pos=0.959 neg=0.023 | val AUC=0.8141\n",
      "[173/700] loss=0.5286 | pos=0.960 neg=0.020 | val AUC=0.7953\n",
      "[174/700] loss=0.5244 | pos=0.961 neg=0.008 | val AUC=0.7675\n",
      "[175/700] loss=0.5193 | pos=0.962 neg=-0.015 | val AUC=0.8045\n",
      "[176/700] loss=0.5219 | pos=0.962 neg=-0.006 | val AUC=0.7596\n",
      "[177/700] loss=0.5216 | pos=0.962 neg=-0.005 | val AUC=0.7379\n",
      "[178/700] loss=0.5245 | pos=0.960 neg=0.007 | val AUC=0.7535\n",
      "[179/700] loss=0.5310 | pos=0.962 neg=0.028 | val AUC=0.7966\n",
      "[180/700] loss=0.5229 | pos=0.959 neg=-0.005 | val AUC=0.7325\n",
      "[181/700] loss=0.5244 | pos=0.960 neg=0.005 | val AUC=0.6623\n",
      "[182/700] loss=0.5310 | pos=0.960 neg=0.026 | val AUC=0.7395\n",
      "[183/700] loss=0.5344 | pos=0.959 neg=0.039 | val AUC=0.7589\n",
      "[184/700] loss=0.5212 | pos=0.958 neg=-0.011 | val AUC=0.7899\n",
      "[185/700] loss=0.5331 | pos=0.961 neg=0.033 | val AUC=0.7133\n",
      "[186/700] loss=0.5298 | pos=0.962 neg=0.022 | val AUC=0.7650\n",
      "[187/700] loss=0.5269 | pos=0.961 neg=0.013 | val AUC=0.7720\n",
      "[188/700] loss=0.5282 | pos=0.960 neg=0.015 | val AUC=0.8109\n",
      "[189/700] loss=0.5315 | pos=0.962 neg=0.030 | val AUC=0.8052\n",
      "[190/700] loss=0.5215 | pos=0.963 neg=-0.010 | val AUC=0.8045\n",
      "[191/700] loss=0.5253 | pos=0.963 neg=0.007 | val AUC=0.7873\n",
      "[192/700] loss=0.5271 | pos=0.962 neg=0.015 | val AUC=0.7943\n",
      "[193/700] loss=0.5284 | pos=0.963 neg=0.018 | val AUC=0.7864\n",
      "[194/700] loss=0.5286 | pos=0.961 neg=0.024 | val AUC=0.7188\n",
      "[195/700] loss=0.5338 | pos=0.961 neg=0.032 | val AUC=0.7854\n",
      "[196/700] loss=0.5207 | pos=0.963 neg=-0.014 | val AUC=0.8434\n",
      "[197/700] loss=0.5163 | pos=0.962 neg=-0.028 | val AUC=0.7812\n",
      "[198/700] loss=0.5245 | pos=0.960 neg=0.007 | val AUC=0.7889\n",
      "[199/700] loss=0.5309 | pos=0.957 neg=0.031 | val AUC=0.7283\n",
      "[200/700] loss=0.5296 | pos=0.957 neg=0.026 | val AUC=0.7835\n",
      "[201/700] loss=0.5224 | pos=0.959 neg=0.001 | val AUC=0.7468\n",
      "[202/700] loss=0.5358 | pos=0.956 neg=0.053 | val AUC=0.7519\n",
      "[203/700] loss=0.5321 | pos=0.955 neg=0.037 | val AUC=0.7835\n",
      "[204/700] loss=0.5109 | pos=0.954 neg=-0.045 | val AUC=0.8253\n",
      "[205/700] loss=0.5200 | pos=0.955 neg=-0.012 | val AUC=0.6929\n",
      "[206/700] loss=0.5240 | pos=0.957 neg=0.005 | val AUC=0.8131\n",
      "[207/700] loss=0.5100 | pos=0.957 neg=-0.049 | val AUC=0.8457\n",
      "[208/700] loss=0.5305 | pos=0.956 neg=0.023 | val AUC=0.7270\n",
      "[209/700] loss=0.5194 | pos=0.957 neg=-0.011 | val AUC=0.7793\n",
      "[210/700] loss=0.5362 | pos=0.958 neg=0.051 | val AUC=0.6626\n",
      "[211/700] loss=0.5303 | pos=0.959 neg=0.030 | val AUC=0.7781\n",
      "[212/700] loss=0.5351 | pos=0.957 neg=0.041 | val AUC=0.7331\n",
      "[213/700] loss=0.5200 | pos=0.957 neg=-0.014 | val AUC=0.7200\n",
      "[214/700] loss=0.5238 | pos=0.959 neg=0.003 | val AUC=0.7315\n",
      "[215/700] loss=0.5286 | pos=0.959 neg=0.019 | val AUC=0.7848\n",
      "[216/700] loss=0.5252 | pos=0.959 neg=0.004 | val AUC=0.7589\n",
      "[217/700] loss=0.5213 | pos=0.958 neg=-0.010 | val AUC=0.7950\n",
      "[218/700] loss=0.5128 | pos=0.960 neg=-0.041 | val AUC=0.8112\n",
      "[219/700] loss=0.5310 | pos=0.960 neg=0.031 | val AUC=0.7755\n",
      "[220/700] loss=0.5420 | pos=0.958 neg=0.071 | val AUC=0.7637\n",
      "[221/700] loss=0.5278 | pos=0.959 neg=0.017 | val AUC=0.7423\n",
      "[222/700] loss=0.5274 | pos=0.961 neg=0.015 | val AUC=0.7742\n",
      "[223/700] loss=0.5164 | pos=0.961 neg=-0.027 | val AUC=0.8549\n",
      "[224/700] loss=0.5186 | pos=0.960 neg=-0.017 | val AUC=0.8001\n",
      "[225/700] loss=0.5204 | pos=0.958 neg=-0.012 | val AUC=0.7541\n",
      "[226/700] loss=0.5193 | pos=0.957 neg=-0.013 | val AUC=0.8399\n",
      "[227/700] loss=0.5174 | pos=0.959 neg=-0.025 | val AUC=0.7522\n",
      "[228/700] loss=0.5243 | pos=0.958 neg=0.005 | val AUC=0.7459\n",
      "[229/700] loss=0.5180 | pos=0.957 neg=-0.020 | val AUC=0.8332\n",
      "[230/700] loss=0.5224 | pos=0.956 neg=-0.003 | val AUC=0.6964\n",
      "[231/700] loss=0.5211 | pos=0.957 neg=-0.007 | val AUC=0.7124\n",
      "[232/700] loss=0.5181 | pos=0.957 neg=-0.023 | val AUC=0.6983\n",
      "[233/700] loss=0.5273 | pos=0.955 neg=0.017 | val AUC=0.7363\n",
      "[234/700] loss=0.5203 | pos=0.955 neg=-0.016 | val AUC=0.7235\n",
      "[235/700] loss=0.5285 | pos=0.958 neg=0.020 | val AUC=0.7369\n",
      "[236/700] loss=0.5231 | pos=0.957 neg=-0.003 | val AUC=0.8214\n",
      "[237/700] loss=0.5301 | pos=0.957 neg=0.025 | val AUC=0.7755\n",
      "[238/700] loss=0.5317 | pos=0.956 neg=0.028 | val AUC=0.7452\n",
      "[239/700] loss=0.5156 | pos=0.957 neg=-0.035 | val AUC=0.6553\n",
      "[240/700] loss=0.5327 | pos=0.957 neg=0.035 | val AUC=0.7908\n",
      "[241/700] loss=0.5301 | pos=0.957 neg=0.024 | val AUC=0.6961\n",
      "[242/700] loss=0.5276 | pos=0.959 neg=0.012 | val AUC=0.6594\n",
      "[243/700] loss=0.5351 | pos=0.959 neg=0.047 | val AUC=0.7162\n",
      "[244/700] loss=0.5123 | pos=0.957 neg=-0.042 | val AUC=0.7210\n",
      "[245/700] loss=0.5252 | pos=0.956 neg=0.010 | val AUC=0.7494\n",
      "[246/700] loss=0.5214 | pos=0.957 neg=-0.009 | val AUC=0.7902\n",
      "[247/700] loss=0.5195 | pos=0.958 neg=-0.012 | val AUC=0.7803\n",
      "[248/700] loss=0.5172 | pos=0.956 neg=-0.025 | val AUC=0.6783\n",
      "[249/700] loss=0.5227 | pos=0.954 neg=-0.005 | val AUC=0.6767\n",
      "[250/700] loss=0.5292 | pos=0.955 neg=0.019 | val AUC=0.7969\n",
      "[251/700] loss=0.5179 | pos=0.955 neg=-0.019 | val AUC=0.7446\n",
      "[252/700] loss=0.5334 | pos=0.959 neg=0.040 | val AUC=0.7293\n",
      "[253/700] loss=0.5218 | pos=0.957 neg=-0.005 | val AUC=0.7302\n",
      "[254/700] loss=0.5325 | pos=0.956 neg=0.034 | val AUC=0.6722\n",
      "[255/700] loss=0.5254 | pos=0.957 neg=0.004 | val AUC=0.7181\n",
      "[256/700] loss=0.5238 | pos=0.960 neg=0.003 | val AUC=0.7328\n",
      "[257/700] loss=0.5297 | pos=0.960 neg=0.022 | val AUC=0.7838\n",
      "[258/700] loss=0.5217 | pos=0.961 neg=-0.007 | val AUC=0.6508\n",
      "[259/700] loss=0.5280 | pos=0.960 neg=0.012 | val AUC=0.6776\n",
      "[260/700] loss=0.5287 | pos=0.962 neg=0.022 | val AUC=0.7226\n",
      "[261/700] loss=0.5262 | pos=0.961 neg=0.006 | val AUC=0.7672\n",
      "[262/700] loss=0.5156 | pos=0.961 neg=-0.029 | val AUC=0.7924\n",
      "[263/700] loss=0.5178 | pos=0.961 neg=-0.016 | val AUC=0.7057\n",
      "[264/700] loss=0.5190 | pos=0.959 neg=-0.013 | val AUC=0.7417\n",
      "[265/700] loss=0.5326 | pos=0.960 neg=0.038 | val AUC=0.7758\n",
      "[266/700] loss=0.5275 | pos=0.960 neg=0.019 | val AUC=0.6869\n",
      "[267/700] loss=0.5213 | pos=0.958 neg=-0.004 | val AUC=0.7344\n",
      "[268/700] loss=0.5243 | pos=0.957 neg=0.005 | val AUC=0.7296\n",
      "[269/700] loss=0.5248 | pos=0.955 neg=0.006 | val AUC=0.7184\n",
      "[270/700] loss=0.5321 | pos=0.955 neg=0.037 | val AUC=0.7350\n",
      "[271/700] loss=0.5306 | pos=0.954 neg=0.033 | val AUC=0.7136\n",
      "[272/700] loss=0.5243 | pos=0.955 neg=0.006 | val AUC=0.7073\n",
      "[273/700] loss=0.5216 | pos=0.954 neg=-0.005 | val AUC=0.7398\n",
      "[274/700] loss=0.5278 | pos=0.952 neg=0.021 | val AUC=0.7331\n",
      "[275/700] loss=0.5296 | pos=0.953 neg=0.025 | val AUC=0.8058\n",
      "[276/700] loss=0.5360 | pos=0.956 neg=0.048 | val AUC=0.7369\n",
      "[277/700] loss=0.5186 | pos=0.954 neg=-0.017 | val AUC=0.7685\n",
      "[278/700] loss=0.5218 | pos=0.955 neg=-0.004 | val AUC=0.7248\n",
      "[279/700] loss=0.5167 | pos=0.956 neg=-0.020 | val AUC=0.7573\n",
      "[280/700] loss=0.5205 | pos=0.956 neg=-0.008 | val AUC=0.7341\n",
      "[281/700] loss=0.5263 | pos=0.958 neg=0.016 | val AUC=0.6524\n",
      "[282/700] loss=0.5319 | pos=0.957 neg=0.033 | val AUC=0.7969\n",
      "[283/700] loss=0.5300 | pos=0.960 neg=0.030 | val AUC=0.7239\n",
      "[284/700] loss=0.5284 | pos=0.959 neg=0.019 | val AUC=0.6811\n",
      "[285/700] loss=0.5329 | pos=0.956 neg=0.034 | val AUC=0.6792\n",
      "[286/700] loss=0.5261 | pos=0.958 neg=0.008 | val AUC=0.7612\n",
      "[287/700] loss=0.5255 | pos=0.961 neg=0.005 | val AUC=0.6716\n",
      "[288/700] loss=0.5306 | pos=0.960 neg=0.029 | val AUC=0.7851\n",
      "[289/700] loss=0.5220 | pos=0.959 neg=-0.005 | val AUC=0.6327\n",
      "[290/700] loss=0.5193 | pos=0.958 neg=-0.014 | val AUC=0.7761\n",
      "[291/700] loss=0.5142 | pos=0.959 neg=-0.035 | val AUC=0.6381\n",
      "[292/700] loss=0.5353 | pos=0.960 neg=0.045 | val AUC=0.6629\n",
      "[293/700] loss=0.5243 | pos=0.961 neg=0.008 | val AUC=0.7012\n",
      "[294/700] loss=0.5291 | pos=0.959 neg=0.018 | val AUC=0.7704\n",
      "[295/700] loss=0.5308 | pos=0.959 neg=0.027 | val AUC=0.6467\n",
      "[296/700] loss=0.5200 | pos=0.961 neg=-0.010 | val AUC=0.6955\n",
      "[297/700] loss=0.5225 | pos=0.963 neg=-0.001 | val AUC=0.7561\n",
      "[298/700] loss=0.5208 | pos=0.963 neg=-0.010 | val AUC=0.6827\n",
      "[299/700] loss=0.5365 | pos=0.962 neg=0.051 | val AUC=0.8163\n",
      "[300/700] loss=0.5222 | pos=0.963 neg=-0.004 | val AUC=0.7628\n",
      "[301/700] loss=0.5276 | pos=0.962 neg=0.017 | val AUC=0.7357\n",
      "[302/700] loss=0.5259 | pos=0.960 neg=0.006 | val AUC=0.8173\n",
      "[303/700] loss=0.5419 | pos=0.960 neg=0.066 | val AUC=0.7956\n",
      "[304/700] loss=0.5219 | pos=0.961 neg=-0.006 | val AUC=0.7516\n",
      "[305/700] loss=0.5241 | pos=0.960 neg=-0.001 | val AUC=0.8135\n",
      "[306/700] loss=0.5246 | pos=0.959 neg=0.006 | val AUC=0.7398\n",
      "[307/700] loss=0.5140 | pos=0.958 neg=-0.032 | val AUC=0.7321\n",
      "[308/700] loss=0.5208 | pos=0.959 neg=-0.007 | val AUC=0.7545\n",
      "[309/700] loss=0.5339 | pos=0.959 neg=0.040 | val AUC=0.8036\n",
      "[310/700] loss=0.5261 | pos=0.960 neg=0.013 | val AUC=0.7698\n",
      "[311/700] loss=0.5333 | pos=0.960 neg=0.040 | val AUC=0.7357\n",
      "[312/700] loss=0.5183 | pos=0.960 neg=-0.016 | val AUC=0.7325\n",
      "[313/700] loss=0.5184 | pos=0.960 neg=-0.017 | val AUC=0.7819\n",
      "[314/700] loss=0.5267 | pos=0.958 neg=0.014 | val AUC=0.7172\n",
      "[315/700] loss=0.5279 | pos=0.959 neg=0.017 | val AUC=0.6958\n",
      "[316/700] loss=0.5177 | pos=0.958 neg=-0.023 | val AUC=0.8001\n",
      "[317/700] loss=0.5270 | pos=0.959 neg=0.017 | val AUC=0.6480\n",
      "[318/700] loss=0.5270 | pos=0.958 neg=0.015 | val AUC=0.7203\n",
      "[319/700] loss=0.5278 | pos=0.959 neg=0.018 | val AUC=0.7436\n",
      "[320/700] loss=0.5257 | pos=0.959 neg=0.012 | val AUC=0.7886\n",
      "[321/700] loss=0.5208 | pos=0.957 neg=-0.009 | val AUC=0.6885\n",
      "[322/700] loss=0.5235 | pos=0.958 neg=-0.000 | val AUC=0.7207\n",
      "[323/700] loss=0.5197 | pos=0.959 neg=-0.016 | val AUC=0.7114\n",
      "[324/700] loss=0.5259 | pos=0.961 neg=0.006 | val AUC=0.6709\n",
      "[325/700] loss=0.5166 | pos=0.961 neg=-0.031 | val AUC=0.6834\n",
      "[326/700] loss=0.5228 | pos=0.960 neg=-0.003 | val AUC=0.7439\n",
      "[327/700] loss=0.5253 | pos=0.961 neg=0.005 | val AUC=0.7226\n",
      "[328/700] loss=0.5251 | pos=0.962 neg=0.004 | val AUC=0.7321\n",
      "[329/700] loss=0.5352 | pos=0.963 neg=0.043 | val AUC=0.7261\n",
      "[330/700] loss=0.5257 | pos=0.963 neg=0.006 | val AUC=0.7286\n",
      "[331/700] loss=0.5106 | pos=0.962 neg=-0.052 | val AUC=0.7328\n",
      "[332/700] loss=0.5308 | pos=0.962 neg=0.029 | val AUC=0.8744\n",
      "[333/700] loss=0.5235 | pos=0.963 neg=-0.003 | val AUC=0.7781\n",
      "[334/700] loss=0.5200 | pos=0.964 neg=-0.014 | val AUC=0.7577\n",
      "[335/700] loss=0.5309 | pos=0.965 neg=0.025 | val AUC=0.7742\n",
      "[336/700] loss=0.5192 | pos=0.965 neg=-0.013 | val AUC=0.7516\n",
      "[337/700] loss=0.5221 | pos=0.965 neg=-0.007 | val AUC=0.7679\n",
      "[338/700] loss=0.5237 | pos=0.965 neg=0.004 | val AUC=0.7851\n",
      "[339/700] loss=0.5130 | pos=0.963 neg=-0.036 | val AUC=0.8045\n",
      "[340/700] loss=0.5165 | pos=0.964 neg=-0.021 | val AUC=0.6792\n",
      "[341/700] loss=0.5206 | pos=0.964 neg=-0.007 | val AUC=0.8141\n",
      "[342/700] loss=0.5187 | pos=0.964 neg=-0.018 | val AUC=0.7895\n",
      "[343/700] loss=0.5219 | pos=0.964 neg=0.003 | val AUC=0.6808\n",
      "[344/700] loss=0.5222 | pos=0.965 neg=0.001 | val AUC=0.7781\n",
      "[345/700] loss=0.5269 | pos=0.965 neg=0.015 | val AUC=0.7073\n",
      "[346/700] loss=0.5335 | pos=0.964 neg=0.041 | val AUC=0.7835\n",
      "[347/700] loss=0.5189 | pos=0.962 neg=-0.010 | val AUC=0.7494\n",
      "[348/700] loss=0.5231 | pos=0.961 neg=0.001 | val AUC=0.7341\n",
      "[349/700] loss=0.5129 | pos=0.962 neg=-0.035 | val AUC=0.7219\n",
      "[350/700] loss=0.5225 | pos=0.962 neg=0.002 | val AUC=0.7577\n",
      "[351/700] loss=0.5208 | pos=0.963 neg=-0.001 | val AUC=0.7615\n",
      "[352/700] loss=0.5262 | pos=0.962 neg=0.017 | val AUC=0.7156\n",
      "[353/700] loss=0.5297 | pos=0.962 neg=0.027 | val AUC=0.6763\n",
      "[354/700] loss=0.5191 | pos=0.961 neg=-0.011 | val AUC=0.6129\n",
      "[355/700] loss=0.5251 | pos=0.961 neg=0.014 | val AUC=0.7899\n",
      "[356/700] loss=0.5220 | pos=0.962 neg=-0.002 | val AUC=0.8619\n",
      "[357/700] loss=0.5215 | pos=0.962 neg=-0.008 | val AUC=0.7449\n",
      "[358/700] loss=0.5338 | pos=0.961 neg=0.038 | val AUC=0.6846\n",
      "[359/700] loss=0.5260 | pos=0.964 neg=0.010 | val AUC=0.7628\n",
      "[360/700] loss=0.5286 | pos=0.964 neg=0.022 | val AUC=0.6881\n",
      "[361/700] loss=0.5215 | pos=0.963 neg=-0.006 | val AUC=0.7420\n",
      "[362/700] loss=0.5197 | pos=0.964 neg=-0.012 | val AUC=0.7121\n",
      "[363/700] loss=0.5222 | pos=0.963 neg=-0.006 | val AUC=0.7618\n",
      "[364/700] loss=0.5297 | pos=0.963 neg=0.028 | val AUC=0.7605\n",
      "[365/700] loss=0.5205 | pos=0.964 neg=-0.010 | val AUC=0.6980\n",
      "[366/700] loss=0.5188 | pos=0.964 neg=-0.012 | val AUC=0.7242\n",
      "[367/700] loss=0.5286 | pos=0.965 neg=0.022 | val AUC=0.7717\n",
      "[368/700] loss=0.5246 | pos=0.964 neg=0.006 | val AUC=0.8820\n",
      "[369/700] loss=0.5275 | pos=0.962 neg=0.017 | val AUC=0.7034\n",
      "[370/700] loss=0.5230 | pos=0.965 neg=-0.002 | val AUC=0.7666\n",
      "[371/700] loss=0.5205 | pos=0.964 neg=-0.008 | val AUC=0.8112\n",
      "[372/700] loss=0.5139 | pos=0.962 neg=-0.036 | val AUC=0.7918\n",
      "[373/700] loss=0.5255 | pos=0.963 neg=0.011 | val AUC=0.7034\n",
      "[374/700] loss=0.5159 | pos=0.966 neg=-0.023 | val AUC=0.7350\n",
      "[375/700] loss=0.5332 | pos=0.965 neg=0.045 | val AUC=0.7634\n",
      "[376/700] loss=0.5266 | pos=0.964 neg=0.014 | val AUC=0.8141\n",
      "[377/700] loss=0.5225 | pos=0.961 neg=-0.004 | val AUC=0.7899\n",
      "[378/700] loss=0.5280 | pos=0.964 neg=0.022 | val AUC=0.7736\n",
      "[379/700] loss=0.5281 | pos=0.966 neg=0.020 | val AUC=0.7446\n",
      "[380/700] loss=0.5274 | pos=0.966 neg=0.019 | val AUC=0.7200\n",
      "[381/700] loss=0.5248 | pos=0.964 neg=0.007 | val AUC=0.7628\n",
      "[382/700] loss=0.5105 | pos=0.964 neg=-0.047 | val AUC=0.7117\n",
      "[383/700] loss=0.5238 | pos=0.964 neg=0.004 | val AUC=0.5941\n",
      "[384/700] loss=0.5209 | pos=0.963 neg=-0.013 | val AUC=0.7577\n",
      "[385/700] loss=0.5266 | pos=0.963 neg=0.012 | val AUC=0.7213\n",
      "[386/700] loss=0.5220 | pos=0.963 neg=-0.006 | val AUC=0.7481\n",
      "[387/700] loss=0.5263 | pos=0.964 neg=0.009 | val AUC=0.7516\n",
      "[388/700] loss=0.5337 | pos=0.962 neg=0.039 | val AUC=0.6932\n",
      "[389/700] loss=0.5251 | pos=0.962 neg=0.010 | val AUC=0.7535\n",
      "[390/700] loss=0.5252 | pos=0.963 neg=0.007 | val AUC=0.7216\n",
      "[391/700] loss=0.5201 | pos=0.964 neg=-0.009 | val AUC=0.6674\n",
      "[392/700] loss=0.5234 | pos=0.963 neg=0.002 | val AUC=0.7516\n",
      "[393/700] loss=0.5347 | pos=0.962 neg=0.046 | val AUC=0.7267\n",
      "[394/700] loss=0.5273 | pos=0.963 neg=0.016 | val AUC=0.7427\n",
      "[395/700] loss=0.5384 | pos=0.962 neg=0.062 | val AUC=0.7264\n",
      "[396/700] loss=0.5243 | pos=0.963 neg=0.006 | val AUC=0.7427\n",
      "[397/700] loss=0.5292 | pos=0.962 neg=0.023 | val AUC=0.7449\n",
      "[398/700] loss=0.5167 | pos=0.963 neg=-0.021 | val AUC=0.7353\n",
      "[399/700] loss=0.5299 | pos=0.960 neg=0.024 | val AUC=0.7184\n",
      "[400/700] loss=0.5220 | pos=0.963 neg=-0.002 | val AUC=0.7066\n",
      "[401/700] loss=0.5280 | pos=0.963 neg=0.022 | val AUC=0.7828\n",
      "[402/700] loss=0.5275 | pos=0.961 neg=0.019 | val AUC=0.7216\n",
      "[403/700] loss=0.5106 | pos=0.959 neg=-0.043 | val AUC=0.7280\n",
      "[404/700] loss=0.5178 | pos=0.961 neg=-0.014 | val AUC=0.6598\n",
      "[405/700] loss=0.5312 | pos=0.960 neg=0.036 | val AUC=0.6869\n",
      "[406/700] loss=0.5246 | pos=0.960 neg=0.005 | val AUC=0.7656\n",
      "[407/700] loss=0.5155 | pos=0.961 neg=-0.030 | val AUC=0.8112\n",
      "[408/700] loss=0.5260 | pos=0.962 neg=0.011 | val AUC=0.7191\n",
      "[409/700] loss=0.5230 | pos=0.961 neg=0.003 | val AUC=0.7714\n",
      "[410/700] loss=0.5261 | pos=0.962 neg=0.015 | val AUC=0.7513\n",
      "[411/700] loss=0.5265 | pos=0.961 neg=0.017 | val AUC=0.7012\n",
      "[412/700] loss=0.5217 | pos=0.963 neg=-0.006 | val AUC=0.7296\n",
      "[413/700] loss=0.5352 | pos=0.960 neg=0.047 | val AUC=0.6853\n",
      "[414/700] loss=0.5222 | pos=0.961 neg=-0.002 | val AUC=0.7631\n",
      "[415/700] loss=0.5281 | pos=0.960 neg=0.017 | val AUC=0.8297\n",
      "[416/700] loss=0.5298 | pos=0.960 neg=0.023 | val AUC=0.7561\n",
      "[417/700] loss=0.5287 | pos=0.961 neg=0.017 | val AUC=0.6384\n",
      "[418/700] loss=0.5180 | pos=0.962 neg=-0.019 | val AUC=0.7261\n",
      "[419/700] loss=0.5307 | pos=0.961 neg=0.027 | val AUC=0.7921\n",
      "[420/700] loss=0.5315 | pos=0.962 neg=0.031 | val AUC=0.7143\n",
      "[421/700] loss=0.5287 | pos=0.963 neg=0.020 | val AUC=0.7188\n",
      "[422/700] loss=0.5167 | pos=0.963 neg=-0.025 | val AUC=0.7114\n",
      "[423/700] loss=0.5223 | pos=0.966 neg=-0.001 | val AUC=0.7459\n",
      "[424/700] loss=0.5152 | pos=0.966 neg=-0.026 | val AUC=0.7605\n",
      "[425/700] loss=0.5117 | pos=0.967 neg=-0.042 | val AUC=0.7337\n",
      "[426/700] loss=0.5272 | pos=0.965 neg=0.016 | val AUC=0.7299\n",
      "[427/700] loss=0.5272 | pos=0.964 neg=0.017 | val AUC=0.7982\n",
      "[428/700] loss=0.5272 | pos=0.965 neg=0.016 | val AUC=0.7832\n",
      "[429/700] loss=0.5233 | pos=0.966 neg=0.002 | val AUC=0.7567\n",
      "[430/700] loss=0.5222 | pos=0.964 neg=-0.003 | val AUC=0.8020\n",
      "[431/700] loss=0.5183 | pos=0.963 neg=-0.018 | val AUC=0.7554\n",
      "[432/700] loss=0.5222 | pos=0.963 neg=-0.001 | val AUC=0.7503\n",
      "[433/700] loss=0.5224 | pos=0.963 neg=0.002 | val AUC=0.7643\n",
      "[434/700] loss=0.5233 | pos=0.962 neg=0.005 | val AUC=0.7219\n",
      "[435/700] loss=0.5214 | pos=0.962 neg=-0.010 | val AUC=0.7806\n",
      "[436/700] loss=0.5346 | pos=0.963 neg=0.038 | val AUC=0.6818\n",
      "[437/700] loss=0.5269 | pos=0.962 neg=0.015 | val AUC=0.7293\n",
      "[438/700] loss=0.5257 | pos=0.963 neg=0.008 | val AUC=0.6929\n",
      "[439/700] loss=0.5290 | pos=0.962 neg=0.024 | val AUC=0.7793\n",
      "[440/700] loss=0.5032 | pos=0.963 neg=-0.070 | val AUC=0.6853\n",
      "[441/700] loss=0.5156 | pos=0.961 neg=-0.027 | val AUC=0.7127\n",
      "[442/700] loss=0.5160 | pos=0.965 neg=-0.025 | val AUC=0.7752\n",
      "[443/700] loss=0.5246 | pos=0.963 neg=0.011 | val AUC=0.7181\n",
      "[444/700] loss=0.5219 | pos=0.963 neg=0.002 | val AUC=0.6802\n",
      "[445/700] loss=0.5162 | pos=0.963 neg=-0.022 | val AUC=0.6980\n",
      "[446/700] loss=0.5265 | pos=0.962 neg=0.018 | val AUC=0.7860\n",
      "[447/700] loss=0.5251 | pos=0.962 neg=0.012 | val AUC=0.6849\n",
      "[448/700] loss=0.5171 | pos=0.961 neg=-0.021 | val AUC=0.7478\n",
      "[449/700] loss=0.5245 | pos=0.965 neg=0.010 | val AUC=0.7411\n",
      "[450/700] loss=0.5244 | pos=0.962 neg=0.008 | val AUC=0.7465\n",
      "[451/700] loss=0.5307 | pos=0.963 neg=0.025 | val AUC=0.8514\n",
      "[452/700] loss=0.5295 | pos=0.964 neg=0.022 | val AUC=0.6071\n",
      "[453/700] loss=0.5341 | pos=0.965 neg=0.042 | val AUC=0.7899\n",
      "[454/700] loss=0.5139 | pos=0.964 neg=-0.031 | val AUC=0.7219\n",
      "[455/700] loss=0.5246 | pos=0.964 neg=0.010 | val AUC=0.7746\n",
      "[456/700] loss=0.5211 | pos=0.964 neg=-0.010 | val AUC=0.8106\n",
      "[457/700] loss=0.5292 | pos=0.966 neg=0.022 | val AUC=0.7812\n",
      "[458/700] loss=0.5175 | pos=0.966 neg=-0.020 | val AUC=0.7057\n",
      "[459/700] loss=0.5236 | pos=0.963 neg=0.004 | val AUC=0.7254\n",
      "[460/700] loss=0.5212 | pos=0.963 neg=-0.006 | val AUC=0.7621\n",
      "[461/700] loss=0.5262 | pos=0.964 neg=0.018 | val AUC=0.7857\n",
      "[462/700] loss=0.5235 | pos=0.964 neg=-0.002 | val AUC=0.7073\n",
      "[463/700] loss=0.5185 | pos=0.963 neg=-0.015 | val AUC=0.7551\n",
      "[464/700] loss=0.5176 | pos=0.962 neg=-0.018 | val AUC=0.6744\n",
      "[465/700] loss=0.5191 | pos=0.963 neg=-0.011 | val AUC=0.6999\n",
      "[466/700] loss=0.5294 | pos=0.961 neg=0.022 | val AUC=0.6913\n",
      "[467/700] loss=0.5121 | pos=0.963 neg=-0.034 | val AUC=0.6425\n",
      "[468/700] loss=0.5272 | pos=0.961 neg=0.017 | val AUC=0.7101\n",
      "[469/700] loss=0.5353 | pos=0.960 neg=0.045 | val AUC=0.8106\n",
      "[470/700] loss=0.5201 | pos=0.961 neg=-0.009 | val AUC=0.7140\n",
      "[471/700] loss=0.5119 | pos=0.960 neg=-0.043 | val AUC=0.7242\n",
      "[472/700] loss=0.5259 | pos=0.961 neg=0.013 | val AUC=0.8004\n",
      "[473/700] loss=0.5157 | pos=0.960 neg=-0.029 | val AUC=0.7672\n",
      "[474/700] loss=0.5242 | pos=0.961 neg=0.001 | val AUC=0.7360\n",
      "[475/700] loss=0.5307 | pos=0.960 neg=0.030 | val AUC=0.7848\n",
      "[476/700] loss=0.5287 | pos=0.961 neg=0.026 | val AUC=0.8004\n",
      "[477/700] loss=0.5396 | pos=0.962 neg=0.068 | val AUC=0.7293\n",
      "[478/700] loss=0.5220 | pos=0.962 neg=-0.001 | val AUC=0.7487\n",
      "[479/700] loss=0.5216 | pos=0.961 neg=-0.004 | val AUC=0.8288\n",
      "[480/700] loss=0.5167 | pos=0.959 neg=-0.018 | val AUC=0.8709\n",
      "[481/700] loss=0.5262 | pos=0.960 neg=0.016 | val AUC=0.8080\n",
      "[482/700] loss=0.5155 | pos=0.962 neg=-0.031 | val AUC=0.8575\n",
      "[483/700] loss=0.5313 | pos=0.963 neg=0.030 | val AUC=0.8036\n",
      "[484/700] loss=0.5227 | pos=0.962 neg=0.003 | val AUC=0.8004\n",
      "[485/700] loss=0.5366 | pos=0.962 neg=0.050 | val AUC=0.8208\n",
      "[486/700] loss=0.5177 | pos=0.961 neg=-0.021 | val AUC=0.7959\n",
      "[487/700] loss=0.5286 | pos=0.964 neg=0.020 | val AUC=0.8355\n",
      "[488/700] loss=0.5206 | pos=0.962 neg=-0.007 | val AUC=0.7835\n",
      "[489/700] loss=0.5158 | pos=0.962 neg=-0.027 | val AUC=0.7892\n",
      "[490/700] loss=0.5250 | pos=0.962 neg=0.005 | val AUC=0.7580\n",
      "[491/700] loss=0.5178 | pos=0.963 neg=-0.016 | val AUC=0.8284\n",
      "[492/700] loss=0.5206 | pos=0.963 neg=-0.009 | val AUC=0.7175\n",
      "[493/700] loss=0.5256 | pos=0.963 neg=0.011 | val AUC=0.7516\n",
      "[494/700] loss=0.5226 | pos=0.963 neg=-0.001 | val AUC=0.7513\n",
      "[495/700] loss=0.5311 | pos=0.963 neg=0.027 | val AUC=0.7165\n",
      "[496/700] loss=0.5219 | pos=0.964 neg=-0.004 | val AUC=0.7331\n",
      "[497/700] loss=0.5215 | pos=0.963 neg=-0.006 | val AUC=0.7889\n",
      "[498/700] loss=0.5314 | pos=0.963 neg=0.030 | val AUC=0.7226\n",
      "[499/700] loss=0.5198 | pos=0.962 neg=-0.013 | val AUC=0.7057\n",
      "[500/700] loss=0.5204 | pos=0.961 neg=-0.014 | val AUC=0.7564\n",
      "[501/700] loss=0.5232 | pos=0.960 neg=0.001 | val AUC=0.7838\n",
      "[502/700] loss=0.5349 | pos=0.959 neg=0.046 | val AUC=0.7784\n",
      "[503/700] loss=0.5225 | pos=0.958 neg=-0.003 | val AUC=0.7663\n",
      "[504/700] loss=0.5272 | pos=0.958 neg=0.015 | val AUC=0.7637\n",
      "[505/700] loss=0.5201 | pos=0.957 neg=-0.012 | val AUC=0.7577\n",
      "[506/700] loss=0.5205 | pos=0.957 neg=-0.008 | val AUC=0.7816\n",
      "[507/700] loss=0.5182 | pos=0.957 neg=-0.016 | val AUC=0.6620\n",
      "[508/700] loss=0.5258 | pos=0.958 neg=0.014 | val AUC=0.7522\n",
      "[509/700] loss=0.5237 | pos=0.958 neg=0.006 | val AUC=0.7554\n",
      "[510/700] loss=0.5321 | pos=0.959 neg=0.042 | val AUC=0.7044\n",
      "[511/700] loss=0.5269 | pos=0.961 neg=0.014 | val AUC=0.7168\n",
      "[512/700] loss=0.5266 | pos=0.961 neg=0.011 | val AUC=0.7701\n",
      "[513/700] loss=0.5246 | pos=0.962 neg=0.008 | val AUC=0.7372\n",
      "[514/700] loss=0.5229 | pos=0.960 neg=-0.001 | val AUC=0.6920\n",
      "[515/700] loss=0.5298 | pos=0.961 neg=0.028 | val AUC=0.7143\n",
      "[516/700] loss=0.5261 | pos=0.963 neg=0.017 | val AUC=0.7290\n",
      "[517/700] loss=0.5184 | pos=0.962 neg=-0.022 | val AUC=0.6652\n",
      "[518/700] loss=0.5341 | pos=0.963 neg=0.037 | val AUC=0.7433\n",
      "[519/700] loss=0.5270 | pos=0.963 neg=0.010 | val AUC=0.6792\n",
      "[520/700] loss=0.5225 | pos=0.961 neg=-0.004 | val AUC=0.7768\n",
      "[521/700] loss=0.5242 | pos=0.962 neg=0.003 | val AUC=0.6897\n",
      "[522/700] loss=0.5137 | pos=0.963 neg=-0.035 | val AUC=0.7172\n",
      "[523/700] loss=0.5234 | pos=0.962 neg=0.006 | val AUC=0.7669\n",
      "[524/700] loss=0.5187 | pos=0.961 neg=-0.011 | val AUC=0.7497\n",
      "[525/700] loss=0.5358 | pos=0.962 neg=0.053 | val AUC=0.6894\n",
      "[526/700] loss=0.5253 | pos=0.963 neg=0.013 | val AUC=0.7203\n",
      "[527/700] loss=0.5171 | pos=0.962 neg=-0.020 | val AUC=0.7293\n",
      "[528/700] loss=0.5245 | pos=0.963 neg=0.005 | val AUC=0.7156\n",
      "[529/700] loss=0.5172 | pos=0.961 neg=-0.018 | val AUC=0.6719\n",
      "[530/700] loss=0.5188 | pos=0.960 neg=-0.013 | val AUC=0.7478\n",
      "[531/700] loss=0.5231 | pos=0.964 neg=0.006 | val AUC=0.6786\n",
      "[532/700] loss=0.5180 | pos=0.964 neg=-0.017 | val AUC=0.6591\n",
      "[533/700] loss=0.5125 | pos=0.963 neg=-0.039 | val AUC=0.6773\n",
      "[534/700] loss=0.5107 | pos=0.961 neg=-0.046 | val AUC=0.7392\n",
      "[535/700] loss=0.5250 | pos=0.963 neg=0.010 | val AUC=0.7357\n",
      "[536/700] loss=0.5274 | pos=0.964 neg=0.019 | val AUC=0.6617\n",
      "[537/700] loss=0.5290 | pos=0.962 neg=0.025 | val AUC=0.7296\n",
      "[538/700] loss=0.5212 | pos=0.963 neg=-0.007 | val AUC=0.7334\n",
      "[539/700] loss=0.5291 | pos=0.964 neg=0.026 | val AUC=0.7082\n",
      "[540/700] loss=0.5227 | pos=0.961 neg=0.000 | val AUC=0.7133\n",
      "[541/700] loss=0.5227 | pos=0.964 neg=0.002 | val AUC=0.7726\n",
      "[542/700] loss=0.5305 | pos=0.964 neg=0.033 | val AUC=0.6894\n",
      "[543/700] loss=0.5245 | pos=0.962 neg=0.009 | val AUC=0.7966\n",
      "[544/700] loss=0.5067 | pos=0.963 neg=-0.062 | val AUC=0.7592\n",
      "[545/700] loss=0.5266 | pos=0.963 neg=0.014 | val AUC=0.7264\n",
      "[546/700] loss=0.5174 | pos=0.962 neg=-0.017 | val AUC=0.7184\n",
      "[547/700] loss=0.5260 | pos=0.964 neg=0.017 | val AUC=0.7580\n",
      "[548/700] loss=0.5271 | pos=0.964 neg=0.019 | val AUC=0.6993\n",
      "[549/700] loss=0.5264 | pos=0.963 neg=0.016 | val AUC=0.7873\n",
      "[550/700] loss=0.5150 | pos=0.962 neg=-0.027 | val AUC=0.7768\n",
      "[551/700] loss=0.5276 | pos=0.962 neg=0.019 | val AUC=0.7121\n",
      "[552/700] loss=0.5282 | pos=0.962 neg=0.026 | val AUC=0.6971\n",
      "[553/700] loss=0.5232 | pos=0.964 neg=0.004 | val AUC=0.7066\n",
      "[554/700] loss=0.5239 | pos=0.964 neg=0.006 | val AUC=0.7825\n",
      "[555/700] loss=0.5283 | pos=0.962 neg=0.018 | val AUC=0.7159\n",
      "[556/700] loss=0.5258 | pos=0.962 neg=0.014 | val AUC=0.6709\n",
      "[557/700] loss=0.5217 | pos=0.961 neg=-0.001 | val AUC=0.7156\n",
      "[558/700] loss=0.5249 | pos=0.963 neg=0.013 | val AUC=0.7420\n",
      "[559/700] loss=0.5239 | pos=0.961 neg=0.003 | val AUC=0.7006\n",
      "[560/700] loss=0.5314 | pos=0.962 neg=0.032 | val AUC=0.7404\n",
      "[561/700] loss=0.5277 | pos=0.961 neg=0.019 | val AUC=0.6601\n",
      "[562/700] loss=0.5361 | pos=0.960 neg=0.050 | val AUC=0.7669\n",
      "[563/700] loss=0.5218 | pos=0.961 neg=-0.001 | val AUC=0.8004\n",
      "[564/700] loss=0.5242 | pos=0.963 neg=0.009 | val AUC=0.7101\n",
      "[565/700] loss=0.5291 | pos=0.962 neg=0.024 | val AUC=0.7653\n",
      "[566/700] loss=0.5263 | pos=0.960 neg=0.015 | val AUC=0.7385\n",
      "[567/700] loss=0.5297 | pos=0.962 neg=0.032 | val AUC=0.7015\n",
      "[568/700] loss=0.5319 | pos=0.961 neg=0.039 | val AUC=0.7545\n",
      "[569/700] loss=0.5112 | pos=0.962 neg=-0.045 | val AUC=0.7997\n",
      "[570/700] loss=0.5192 | pos=0.961 neg=-0.014 | val AUC=0.7592\n",
      "[571/700] loss=0.5261 | pos=0.964 neg=0.015 | val AUC=0.7717\n",
      "[572/700] loss=0.5263 | pos=0.964 neg=0.018 | val AUC=0.6834\n",
      "[573/700] loss=0.5260 | pos=0.964 neg=0.016 | val AUC=0.7337\n",
      "[574/700] loss=0.5298 | pos=0.964 neg=0.030 | val AUC=0.6932\n",
      "[575/700] loss=0.5203 | pos=0.963 neg=-0.007 | val AUC=0.8335\n",
      "[576/700] loss=0.5189 | pos=0.964 neg=-0.011 | val AUC=0.7436\n",
      "[577/700] loss=0.5269 | pos=0.965 neg=0.021 | val AUC=0.7245\n",
      "[578/700] loss=0.5306 | pos=0.967 neg=0.033 | val AUC=0.7577\n",
      "[579/700] loss=0.5216 | pos=0.965 neg=0.001 | val AUC=0.8020\n",
      "[580/700] loss=0.5273 | pos=0.965 neg=0.019 | val AUC=0.7305\n",
      "[581/700] loss=0.5095 | pos=0.965 neg=-0.051 | val AUC=0.7414\n",
      "[582/700] loss=0.5265 | pos=0.965 neg=0.018 | val AUC=0.7318\n",
      "[583/700] loss=0.5276 | pos=0.966 neg=0.020 | val AUC=0.7860\n",
      "[584/700] loss=0.5224 | pos=0.967 neg=0.001 | val AUC=0.7216\n",
      "[585/700] loss=0.5265 | pos=0.967 neg=0.012 | val AUC=0.7612\n",
      "[586/700] loss=0.5232 | pos=0.967 neg=0.005 | val AUC=0.7513\n",
      "[587/700] loss=0.5117 | pos=0.963 neg=-0.044 | val AUC=0.7302\n",
      "[588/700] loss=0.5287 | pos=0.963 neg=0.025 | val AUC=0.7414\n",
      "[589/700] loss=0.5137 | pos=0.961 neg=-0.032 | val AUC=0.7787\n",
      "[590/700] loss=0.5281 | pos=0.962 neg=0.019 | val AUC=0.7267\n",
      "[591/700] loss=0.5197 | pos=0.964 neg=-0.009 | val AUC=0.7596\n",
      "[592/700] loss=0.5128 | pos=0.960 neg=-0.037 | val AUC=0.8001\n",
      "[593/700] loss=0.5373 | pos=0.960 neg=0.057 | val AUC=0.8055\n",
      "[594/700] loss=0.5116 | pos=0.962 neg=-0.043 | val AUC=0.7494\n",
      "[595/700] loss=0.5222 | pos=0.961 neg=-0.002 | val AUC=0.7717\n",
      "[596/700] loss=0.5202 | pos=0.964 neg=-0.009 | val AUC=0.7548\n",
      "[597/700] loss=0.5082 | pos=0.963 neg=-0.054 | val AUC=0.7446\n",
      "[598/700] loss=0.5282 | pos=0.964 neg=0.026 | val AUC=0.7809\n",
      "[599/700] loss=0.5236 | pos=0.964 neg=0.008 | val AUC=0.7328\n",
      "[600/700] loss=0.5218 | pos=0.965 neg=0.002 | val AUC=0.8434\n",
      "[601/700] loss=0.5122 | pos=0.965 neg=-0.035 | val AUC=0.7124\n",
      "[602/700] loss=0.5228 | pos=0.964 neg=0.004 | val AUC=0.8112\n",
      "[603/700] loss=0.5232 | pos=0.964 neg=0.009 | val AUC=0.7825\n",
      "[604/700] loss=0.5302 | pos=0.964 neg=0.027 | val AUC=0.7707\n",
      "[605/700] loss=0.5295 | pos=0.965 neg=0.026 | val AUC=0.6916\n",
      "[606/700] loss=0.5295 | pos=0.965 neg=0.028 | val AUC=0.7605\n",
      "[607/700] loss=0.5236 | pos=0.963 neg=0.005 | val AUC=0.7235\n",
      "[608/700] loss=0.5205 | pos=0.962 neg=-0.008 | val AUC=0.7047\n",
      "[609/700] loss=0.5261 | pos=0.963 neg=0.017 | val AUC=0.7621\n",
      "[610/700] loss=0.5189 | pos=0.963 neg=-0.008 | val AUC=0.7382\n",
      "[611/700] loss=0.5206 | pos=0.963 neg=-0.006 | val AUC=0.7749\n",
      "[612/700] loss=0.5207 | pos=0.961 neg=-0.008 | val AUC=0.7656\n",
      "[613/700] loss=0.5221 | pos=0.960 neg=-0.004 | val AUC=0.6473\n",
      "[614/700] loss=0.5300 | pos=0.960 neg=0.023 | val AUC=0.7350\n",
      "[615/700] loss=0.5280 | pos=0.961 neg=0.020 | val AUC=0.6920\n",
      "[616/700] loss=0.5245 | pos=0.963 neg=0.010 | val AUC=0.6901\n",
      "[617/700] loss=0.5226 | pos=0.960 neg=-0.004 | val AUC=0.7777\n",
      "[618/700] loss=0.5231 | pos=0.959 neg=-0.007 | val AUC=0.7940\n",
      "[619/700] loss=0.5206 | pos=0.960 neg=-0.008 | val AUC=0.7490\n",
      "[620/700] loss=0.5171 | pos=0.961 neg=-0.019 | val AUC=0.7535\n",
      "[621/700] loss=0.5324 | pos=0.961 neg=0.033 | val AUC=0.7411\n",
      "[622/700] loss=0.5263 | pos=0.961 neg=0.019 | val AUC=0.8017\n",
      "[623/700] loss=0.5173 | pos=0.962 neg=-0.015 | val AUC=0.7685\n",
      "[624/700] loss=0.5183 | pos=0.963 neg=-0.013 | val AUC=0.7290\n",
      "[625/700] loss=0.5202 | pos=0.963 neg=-0.008 | val AUC=0.7207\n",
      "[626/700] loss=0.5235 | pos=0.962 neg=0.005 | val AUC=0.7685\n",
      "[627/700] loss=0.5280 | pos=0.963 neg=0.022 | val AUC=0.7561\n",
      "[628/700] loss=0.5287 | pos=0.963 neg=0.024 | val AUC=0.7570\n",
      "[629/700] loss=0.5141 | pos=0.964 neg=-0.031 | val AUC=0.8106\n",
      "[630/700] loss=0.5112 | pos=0.962 neg=-0.038 | val AUC=0.7790\n",
      "[631/700] loss=0.5298 | pos=0.962 neg=0.026 | val AUC=0.7679\n",
      "[632/700] loss=0.5316 | pos=0.962 neg=0.038 | val AUC=0.7573\n",
      "[633/700] loss=0.5216 | pos=0.965 neg=0.002 | val AUC=0.7838\n",
      "[634/700] loss=0.5199 | pos=0.964 neg=-0.009 | val AUC=0.7302\n",
      "[635/700] loss=0.5134 | pos=0.965 neg=-0.034 | val AUC=0.7624\n",
      "[636/700] loss=0.5297 | pos=0.962 neg=0.032 | val AUC=0.7994\n",
      "[637/700] loss=0.5247 | pos=0.963 neg=0.006 | val AUC=0.7060\n",
      "[638/700] loss=0.5223 | pos=0.963 neg=-0.002 | val AUC=0.8163\n",
      "[639/700] loss=0.5152 | pos=0.963 neg=-0.030 | val AUC=0.6649\n",
      "[640/700] loss=0.5235 | pos=0.963 neg=0.005 | val AUC=0.7481\n",
      "[641/700] loss=0.5317 | pos=0.963 neg=0.034 | val AUC=0.7487\n",
      "[642/700] loss=0.5182 | pos=0.962 neg=-0.013 | val AUC=0.6977\n",
      "[643/700] loss=0.5302 | pos=0.961 neg=0.031 | val AUC=0.7232\n",
      "[644/700] loss=0.5199 | pos=0.963 neg=-0.012 | val AUC=0.8386\n",
      "[645/700] loss=0.5202 | pos=0.964 neg=-0.012 | val AUC=0.7573\n",
      "[646/700] loss=0.5295 | pos=0.964 neg=0.024 | val AUC=0.6502\n",
      "[647/700] loss=0.5071 | pos=0.964 neg=-0.054 | val AUC=0.7050\n",
      "[648/700] loss=0.5240 | pos=0.964 neg=0.006 | val AUC=0.6716\n",
      "[649/700] loss=0.5274 | pos=0.965 neg=0.022 | val AUC=0.8045\n",
      "[650/700] loss=0.5261 | pos=0.963 neg=0.015 | val AUC=0.8444\n",
      "[651/700] loss=0.5236 | pos=0.963 neg=0.005 | val AUC=0.8036\n",
      "[652/700] loss=0.5233 | pos=0.965 neg=0.000 | val AUC=0.7848\n",
      "[653/700] loss=0.5228 | pos=0.965 neg=-0.000 | val AUC=0.7752\n",
      "[654/700] loss=0.5218 | pos=0.964 neg=-0.005 | val AUC=0.6604\n",
      "[655/700] loss=0.5185 | pos=0.965 neg=-0.012 | val AUC=0.6320\n",
      "[656/700] loss=0.5207 | pos=0.965 neg=-0.006 | val AUC=0.7577\n",
      "[657/700] loss=0.5260 | pos=0.966 neg=0.013 | val AUC=0.7439\n",
      "[658/700] loss=0.5346 | pos=0.966 neg=0.044 | val AUC=0.8651\n",
      "[659/700] loss=0.5132 | pos=0.966 neg=-0.040 | val AUC=0.7124\n",
      "[660/700] loss=0.5270 | pos=0.964 neg=0.012 | val AUC=0.7573\n",
      "[661/700] loss=0.5238 | pos=0.966 neg=0.004 | val AUC=0.7398\n",
      "[662/700] loss=0.5186 | pos=0.966 neg=-0.016 | val AUC=0.7245\n",
      "[663/700] loss=0.5315 | pos=0.967 neg=0.032 | val AUC=0.6996\n",
      "[664/700] loss=0.5218 | pos=0.964 neg=-0.003 | val AUC=0.7953\n",
      "[665/700] loss=0.5280 | pos=0.964 neg=0.017 | val AUC=0.7666\n",
      "[666/700] loss=0.5281 | pos=0.965 neg=0.024 | val AUC=0.7812\n",
      "[667/700] loss=0.5259 | pos=0.966 neg=0.012 | val AUC=0.7376\n",
      "[668/700] loss=0.5319 | pos=0.966 neg=0.036 | val AUC=0.8406\n",
      "[669/700] loss=0.5176 | pos=0.964 neg=-0.016 | val AUC=0.6942\n",
      "[670/700] loss=0.5275 | pos=0.963 neg=0.019 | val AUC=0.7443\n",
      "[671/700] loss=0.5281 | pos=0.963 neg=0.020 | val AUC=0.8377\n",
      "[672/700] loss=0.5279 | pos=0.964 neg=0.026 | val AUC=0.7136\n",
      "[673/700] loss=0.5273 | pos=0.965 neg=0.017 | val AUC=0.7414\n",
      "[674/700] loss=0.5182 | pos=0.964 neg=-0.012 | val AUC=0.7207\n",
      "[675/700] loss=0.5327 | pos=0.964 neg=0.038 | val AUC=0.7178\n",
      "[676/700] loss=0.5270 | pos=0.966 neg=0.015 | val AUC=0.7854\n",
      "[677/700] loss=0.5236 | pos=0.966 neg=0.000 | val AUC=0.7267\n",
      "[678/700] loss=0.5264 | pos=0.967 neg=0.014 | val AUC=0.7637\n",
      "[679/700] loss=0.5265 | pos=0.966 neg=0.017 | val AUC=0.6757\n",
      "[680/700] loss=0.5248 | pos=0.964 neg=0.006 | val AUC=0.7357\n",
      "[681/700] loss=0.5074 | pos=0.965 neg=-0.059 | val AUC=0.8007\n",
      "[682/700] loss=0.5282 | pos=0.964 neg=0.021 | val AUC=0.6677\n",
      "[683/700] loss=0.5188 | pos=0.965 neg=-0.016 | val AUC=0.7114\n",
      "[684/700] loss=0.5206 | pos=0.964 neg=-0.006 | val AUC=0.7357\n",
      "[685/700] loss=0.5291 | pos=0.963 neg=0.024 | val AUC=0.7261\n",
      "[686/700] loss=0.5153 | pos=0.962 neg=-0.024 | val AUC=0.7232\n",
      "[687/700] loss=0.5282 | pos=0.964 neg=0.022 | val AUC=0.7350\n",
      "[688/700] loss=0.5174 | pos=0.962 neg=-0.018 | val AUC=0.6980\n",
      "[689/700] loss=0.5261 | pos=0.962 neg=0.017 | val AUC=0.7825\n",
      "[690/700] loss=0.5289 | pos=0.960 neg=0.020 | val AUC=0.6760\n",
      "[691/700] loss=0.5147 | pos=0.961 neg=-0.029 | val AUC=0.7522\n",
      "[692/700] loss=0.5194 | pos=0.961 neg=-0.008 | val AUC=0.6770\n",
      "[693/700] loss=0.5193 | pos=0.961 neg=-0.010 | val AUC=0.6897\n",
      "[694/700] loss=0.5230 | pos=0.960 neg=-0.001 | val AUC=0.7883\n",
      "[695/700] loss=0.5153 | pos=0.961 neg=-0.030 | val AUC=0.7277\n",
      "[696/700] loss=0.5262 | pos=0.960 neg=0.010 | val AUC=0.7497\n",
      "[697/700] loss=0.5259 | pos=0.962 neg=0.018 | val AUC=0.7395\n",
      "[698/700] loss=0.5211 | pos=0.963 neg=-0.003 | val AUC=0.7251\n",
      "[699/700] loss=0.5288 | pos=0.964 neg=0.025 | val AUC=0.7293\n",
      "[700/700] loss=0.5213 | pos=0.962 neg=-0.005 | val AUC=0.7698\n",
      "[OK] saved GAT label embeddings → Amazon_products/label_embeddings_gat.csv  shape=(531, 769)\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 학습 유틸: 음성 엣지 샘플/로스\n",
    "# ---------------------------\n",
    "def to_upper_pos_edges(A):\n",
    "    pos = []\n",
    "    N = A.shape[0]\n",
    "    for i in range(N):\n",
    "        for j in range(i+1, N):\n",
    "            if A[i, j] == 1:\n",
    "                pos.append((i, j))\n",
    "    return pos\n",
    "\n",
    "def sample_neg(A, k):\n",
    "    N = A.shape[0]\n",
    "    neg = set()\n",
    "    while len(neg) < k:\n",
    "        u = np.random.randint(0, N); v = np.random.randint(0, N)\n",
    "        if u == v: continue\n",
    "        a, b = (u, v) if u < v else (v, u)\n",
    "        if A[a, b] == 0:\n",
    "            neg.add((a, b))\n",
    "    return list(neg)\n",
    "\n",
    "def sample_neg_excluding(A, k, exclude_edges):\n",
    "    \"\"\"\n",
    "    A: np.array [N,N]  (0/1)\n",
    "    k: 뽑을 음성 개수\n",
    "    exclude_edges: {(u,v), ...}  무조건 빼야 하는 양성(또는 금지) 엣지들 (u<v 형태로 넣기)\n",
    "    \"\"\"\n",
    "    N = A.shape[0]\n",
    "    neg = set()\n",
    "    while len(neg) < k:\n",
    "        u = np.random.randint(0, N); v = np.random.randint(0, N)\n",
    "        if u == v:\n",
    "            continue\n",
    "        a, b = (u, v) if u < v else (v, u)\n",
    "        if A[a, b] == 0 and (a, b) not in exclude_edges:\n",
    "            neg.add((a, b))\n",
    "    return list(neg)\n",
    "\n",
    "\n",
    "def edge_score(z, edges):\n",
    "    u = torch.tensor([a for a, _ in edges], device=z.device, dtype=torch.long)\n",
    "    v = torch.tensor([b for _, b in edges], device=z.device, dtype=torch.long)\n",
    "    return (z[u] * z[v]).sum(dim=1)  # 내적 디코더\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "def eval_auc(z, pos_edges, A_full, k_factor=1.0):\n",
    "    z = F.normalize(z, p=2, dim=1)\n",
    "    neg_edges = sample_neg(A_full, int(len(pos_edges) * k_factor))\n",
    "    s = torch.cat([edge_score(z, pos_edges), edge_score(z, neg_edges)]).detach().cpu().numpy()\n",
    "    y = np.concatenate([np.ones(len(pos_edges)), np.zeros(len(neg_edges))])\n",
    "    return roc_auc_score(y, s)\n",
    "\n",
    "hidden_dim=64\n",
    "out_dim=768\n",
    "heads1=8\n",
    "heads2=8\n",
    "dropout=0.2\n",
    "epochs=700\n",
    "lr=1e-3\n",
    "weight_decay=5e-4\n",
    "neg_ratio=1.0\n",
    "eval_every=20\n",
    "use_full_graph_for_final=True\n",
    "pad_width=2\n",
    "normalize_out = True\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "ids, X = load_initial_embeddings(EMB_CSV)      # ids: [0..530] 순서, X: [N, d0]\n",
    "N, d0 = X.shape\n",
    "pos_edges = to_upper_pos_edges(A)\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "idx = rng.permutation(len(pos_edges))\n",
    "n_val = max(1, int(0.1 * len(pos_edges)))          # 10% val\n",
    "pos_val = [pos_edges[i] for i in idx[:n_val]]\n",
    "pos_train = [pos_edges[i] for i in idx[n_val:]]\n",
    "\n",
    "# train 그래프만으로 학습(누출 방지)\n",
    "A_train = np.zeros_like(A)\n",
    "for u, v in pos_train:\n",
    "    A_train[u, v] = 1; A_train[v, u] = 1\n",
    "\n",
    "adj_train = torch.tensor(A_train, dtype=torch.float32, device=device)\n",
    "# 텐서\n",
    "x = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "adj = torch.tensor(A, dtype=torch.float32, device=device)  # softmax 마스크용\n",
    "\n",
    "model = GATEncoder(in_dim=d0, hid_dim=hidden_dim, out_dim=out_dim, heads1=heads1, heads2=heads2, dropout=dropout).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "bce = nn.BCEWithLogitsLoss()\n",
    "\n",
    "maxauc = 0\n",
    "best_ckpt = \"Amazon_products/best_gat.ckpt\"\n",
    "# 금지 엣지 집합 (train+val 모두)\n",
    "forbidden = set()\n",
    "for u, v in pos_edges:        # pos_edges = train+val 전체\n",
    "    a, b = (u, v) if u < v else (v, u)\n",
    "    forbidden.add((a, b))\n",
    "\n",
    "for ep in range(1, epochs+1):\n",
    "    model.train()\n",
    "    # 🔴 여기서 전체 adj 말고 train용 adj만 본다\n",
    "    z = model(x, adj_train)                          # [N, out_dim]\n",
    "    if normalize_out:\n",
    "        z = F.normalize(z, p=2, dim=1)\n",
    "\n",
    "    # 🔴 실제로 학습에 쓰는 양성 수 기준으로 음성 수 결정\n",
    "    num_pos = len(pos_train)\n",
    "    num_neg = int(num_pos * neg_ratio)\n",
    "    # 🔴 train 그래프 기준으로 뽑되, train+val 양성은 무조건 제외\n",
    "    neg_edges = sample_neg_excluding(A_train, num_neg, forbidden)\n",
    "\n",
    "    score_pos = edge_score(z, pos_train)\n",
    "    score_neg = edge_score(z, neg_edges)\n",
    "    scores = torch.cat([score_pos, score_neg], dim=0)\n",
    "    labels = torch.cat([torch.ones_like(score_pos), torch.zeros_like(score_neg)], dim=0)\n",
    "\n",
    "    loss = bce(scores, labels)\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    # 평가 부분은 거의 그대로\n",
    "    if ep % 1 == 0 or ep == 1:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # val은 여전히 train 그래프로 임베딩\n",
    "            z_val = F.normalize(model(x, adj_train), p=2, dim=1)\n",
    "            auc_val = eval_auc(z_val, pos_val, A, k_factor=1.0)\n",
    "        print(f\"[{ep:03d}/{epochs}] loss={loss.item():.4f} | \"\n",
    "              f\"pos={score_pos.mean().item():.3f} neg={score_neg.mean().item():.3f} | \"\n",
    "              f\"val AUC={auc_val:.4f}\")\n",
    "        if maxauc < auc_val:\n",
    "            maxauc = auc_val\n",
    "            torch.save(model.state_dict(), best_ckpt)\n",
    "\n",
    "model.load_state_dict(torch.load(best_ckpt, weights_only=True))\n",
    "\n",
    "# 최종 임베딩 추출\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    z = model(x, adj)\n",
    "    if normalize_out:\n",
    "        z = F.normalize(z, p=2, dim=1)\n",
    "    Z = z.detach().cpu().numpy()  # [N, out_dim]\n",
    "\n",
    "# CSV 저장 (id + feat00..)\n",
    "pad = max(2, len(str(out_dim-1)))\n",
    "feat_cols = [f\"feat{str(i).zfill(pad)}\" for i in range(out_dim)]\n",
    "df = pd.DataFrame(Z, columns=feat_cols)\n",
    "df.insert(0, \"id\", ids)\n",
    "df.to_csv(OUT_CSV, index=False)\n",
    "print(f\"[OK] saved GAT label embeddings → {OUT_CSV}  shape={df.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2be9c212-349a-4635-80ed-67c309e618ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T01:29:14.444135Z",
     "iopub.status.busy": "2025-11-11T01:29:14.443986Z",
     "iopub.status.idle": "2025-11-11T01:29:14.447763Z",
     "shell.execute_reply": "2025-11-11T01:29:14.447386Z",
     "shell.execute_reply.started": "2025-11-11T01:29:14.444121Z"
    }
   },
   "outputs": [],
   "source": [
    "class DocumentEncoder(nn.Module):\n",
    "    def __init__(self, model_name=\"bert-base-uncased\", alpha=0.6, l2norm=True):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.alpha = alpha\n",
    "        self.l2norm = l2norm\n",
    "\n",
    "    @torch.no_grad()  # 임베딩 추출만 할 경우 드롭아웃/그라드 off\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
    "        out = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            return_dict=True\n",
    "        )\n",
    "        last = out.last_hidden_state            # [B, T, H]\n",
    "        cls = last[:, 0, :]                     # [B, H]\n",
    "\n",
    "        # masked mean pooling (패딩 제외, CLS 제외)\n",
    "        mask = attention_mask.clone()           # [B, T]\n",
    "        mask[:, 0] = 0                          # CLS 제외\n",
    "        lengths = mask.sum(dim=1, keepdim=True).clamp(min=1)  # [B, 1]\n",
    "        mean = (last * mask.unsqueeze(-1)).sum(dim=1) / lengths  # [B, H]\n",
    "\n",
    "        h = self.alpha * cls + (1.0 - self.alpha) * mean         # [B, H]\n",
    "        if self.l2norm:\n",
    "            h = F.normalize(h, p=2, dim=-1)\n",
    "\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da0c19da-b9ba-4c24-a70b-d5f59ceb7a82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T01:29:14.448272Z",
     "iopub.status.busy": "2025-11-11T01:29:14.448123Z",
     "iopub.status.idle": "2025-11-11T01:29:14.463163Z",
     "shell.execute_reply": "2025-11-11T01:29:14.462728Z",
     "shell.execute_reply.started": "2025-11-11T01:29:14.448260Z"
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "from typing import List, Dict, Any\n",
    "def read_lines_robust(path):\n",
    "    trials = (\"utf-8\", \"utf-8-sig\", \"cp949\", \"euc-kr\", \"latin1\")\n",
    "    last = None\n",
    "    for enc in trials:\n",
    "        try:\n",
    "            with io.open(path, \"r\", encoding=enc) as f:\n",
    "                return f.readlines()\n",
    "        except Exception as e:\n",
    "            last = e\n",
    "    raise RuntimeError(f\"디코딩 실패: {path}\") from last\n",
    "\n",
    "def parse_id_text(line):\n",
    "    s = line.strip()\n",
    "    if not s or s.startswith(\"#\"):\n",
    "        return None\n",
    "    if \"\\t\" in s:\n",
    "        left, right = s.split(\"\\t\", 1)\n",
    "    else:\n",
    "        parts = s.split(maxsplit=1)\n",
    "        if len(parts) < 2:\n",
    "            return None\n",
    "        left, right = parts[0], parts[1]\n",
    "    try:\n",
    "        doc_id = int(left)\n",
    "    except ValueError:\n",
    "        return None\n",
    "    return doc_id, right.strip()\n",
    "\n",
    "# ========= 2) Dataset / Collate =========\n",
    "class TextCorpusDataset(Dataset):\n",
    "    def __init__(self, corpus_path):\n",
    "        lines = read_lines_robust(corpus_path)\n",
    "        pairs = [parse_id_text(ln) for ln in lines]\n",
    "        pairs = [p for p in pairs if p is not None]\n",
    "        # 필요하면 정렬: pairs.sort(key=lambda x: x[0])\n",
    "        self.ids = [p[0] for p in pairs]\n",
    "        self.texts = [p[1] for p in pairs]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\"id\": self.ids[idx], \"text\": self.texts[idx]}\n",
    "\n",
    "def make_collate_fn(tokenizer, max_length=256):\n",
    "    def collate(batch):\n",
    "        ids = [b[\"id\"] for b in batch]\n",
    "        texts = [b[\"text\"] for b in batch]\n",
    "        enc = tokenizer(\n",
    "            texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        out = {\n",
    "            \"doc_ids\": torch.tensor(ids, dtype=torch.long),\n",
    "            \"input_ids\": enc[\"input_ids\"],\n",
    "            \"attention_mask\": enc[\"attention_mask\"],\n",
    "        }\n",
    "        if \"token_type_ids\" in enc:\n",
    "            out[\"token_type_ids\"] = enc[\"token_type_ids\"]\n",
    "        else:\n",
    "            out[\"token_type_ids\"] = None\n",
    "        return out\n",
    "    return collate\n",
    "\n",
    "# ========= 3) 임베딩 생성 & CSV 저장 =========\n",
    "@torch.no_grad()\n",
    "def build_and_save_document_embeddings(\n",
    "    corpus_path=\"text_corpus.txt\",\n",
    "    model_name=\"bert-base-uncased\",\n",
    "    alpha=0.6,\n",
    "    max_length=256,\n",
    "    batch_size=32,\n",
    "    out_csv=\"document_embeddings_768d.csv\",\n",
    "    pad_width=2,     # 'feat00' 스타일(두 자리)\n",
    "):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # data\n",
    "    ds = TextCorpusDataset(corpus_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    collate_fn = make_collate_fn(tokenizer, max_length=max_length)\n",
    "    dl = DataLoader(ds, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    # model\n",
    "    model = DocumentEncoder(model_name=model_name, alpha=alpha, l2norm=True).to(device).eval()\n",
    "\n",
    "    all_ids, all_embs = [], []\n",
    "    for batch in dl:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attn = batch[\"attention_mask\"].to(device)\n",
    "        tt = batch[\"token_type_ids\"]\n",
    "        if tt is not None:\n",
    "            tt = tt.to(device)\n",
    "\n",
    "        emb = model(input_ids=input_ids, attention_mask=attn, token_type_ids=tt)  # [B,out_dim]\n",
    "        all_embs.append(emb.cpu())\n",
    "        all_ids.extend(batch[\"doc_ids\"].tolist())\n",
    "\n",
    "    E = torch.cat(all_embs, dim=0).numpy()  # [N, out_dim]\n",
    "\n",
    "    # CSV: id + featXX...\n",
    "    feat_cols = [f\"feat{str(i).zfill(pad_width)}\" for i in range(E.shape[1])]\n",
    "    df = pd.DataFrame(E, columns=feat_cols)\n",
    "    df.insert(0, \"id\", all_ids)\n",
    "    df.to_csv(out_csv, index=False, encoding=\"utf-8\")\n",
    "    print(f\"[OK] saved document embeddings → {out_csv}  shape={df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a4a7bdf-957a-4d3f-94d0-b140e9805b54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T00:23:47.281503Z",
     "iopub.status.busy": "2025-11-11T00:23:47.281371Z",
     "iopub.status.idle": "2025-11-11T00:27:03.521294Z",
     "shell.execute_reply": "2025-11-11T00:27:03.520780Z",
     "shell.execute_reply.started": "2025-11-11T00:23:47.281491Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] saved document embeddings → Amazon_products/document_embeddings_768d.csv  shape=(29487, 769)\n"
     ]
    }
   ],
   "source": [
    "build_and_save_document_embeddings(\n",
    "    corpus_path=\"Amazon_products/train/train_corpus.txt\",\n",
    "    model_name=\"bert-base-uncased\",\n",
    "    alpha=0.7,\n",
    "    max_length=256,    # 문서 길이에 맞게 조절\n",
    "    batch_size=256,\n",
    "    out_csv=\"Amazon_products/document_embeddings_768d.csv\",\n",
    "    pad_width=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2df2fb6-6a10-4b74-a909-c9e60bef1f42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T01:29:15.806928Z",
     "iopub.status.busy": "2025-11-11T01:29:15.806691Z",
     "iopub.status.idle": "2025-11-11T01:29:15.810038Z",
     "shell.execute_reply": "2025-11-11T01:29:15.809578Z",
     "shell.execute_reply.started": "2025-11-11T01:29:15.806915Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "[0, 3, 10, 23, 40, 169]\n"
     ]
    }
   ],
   "source": [
    "N = 531 \n",
    "B = np.zeros((N, N), dtype=np.uint8)\n",
    "\n",
    "for u, v in E:\n",
    "    B[u, v] = 1\n",
    "print(B)\n",
    "print(roots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cd05771-6906-4acf-8372-ba0bfd7d4a19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T01:29:18.071007Z",
     "iopub.status.busy": "2025-11-11T01:29:18.070776Z",
     "iopub.status.idle": "2025-11-11T01:29:18.077709Z",
     "shell.execute_reply": "2025-11-11T01:29:18.077329Z",
     "shell.execute_reply.started": "2025-11-11T01:29:18.070993Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy as np\n",
    "\n",
    "def hierarchical_beam_similarity_avg(\n",
    "    doc_vec: np.ndarray,\n",
    "    label_emb: np.ndarray,\n",
    "    adj_upper: np.ndarray,\n",
    "    roots: list[int] = [0],       # 여러 루트\n",
    "    beam: int = 5,\n",
    "    per_parent: str | int = \"l+2\",\n",
    "    tau: float = 0.35,\n",
    "    eps: float = 1e-9,\n",
    "    max_depth: int | None = None,\n",
    "    normalize: bool = False,      # 필요하면 True로\n",
    "):\n",
    "    doc = np.asarray(doc_vec, dtype=np.float32)\n",
    "    L = np.asarray(label_emb, dtype=np.float32)\n",
    "    A = np.asarray(adj_upper).astype(bool)\n",
    "    N, d = L.shape\n",
    "\n",
    "    if normalize:\n",
    "        doc = doc / (np.linalg.norm(doc) + eps)\n",
    "        L = L / (np.linalg.norm(L, axis=1, keepdims=True) + eps)\n",
    "\n",
    "    # 로컬 점수\n",
    "    sims = L @ doc\n",
    "    p = 1.0 / (1.0 + np.exp(-sims / max(tau, 1e-6)))\n",
    "\n",
    "    children = [np.flatnonzero(A[i]) for i in range(N)]\n",
    "\n",
    "    S = np.full(N, -np.inf, dtype=np.float32)\n",
    "    K = np.full(N, -np.inf, dtype=np.float32)\n",
    "    Llen = np.zeros(N, dtype=np.int32)\n",
    "\n",
    "    roots = list(roots)\n",
    "    for r in roots:\n",
    "        S[r] = 0.0\n",
    "        Llen[r] = 0\n",
    "        K[r] = -np.inf\n",
    "\n",
    "    levels = [roots[:]]\n",
    "    cur = roots[:]\n",
    "    level_id = 0\n",
    "\n",
    "    while True:\n",
    "        cand_best = {}\n",
    "        k_parent = (level_id + 2) if (per_parent == \"l+2\") else int(per_parent)\n",
    "\n",
    "        for par in cur:\n",
    "            ch = children[par]\n",
    "            if ch.size == 0:\n",
    "                continue\n",
    "            if ch.size > k_parent:\n",
    "                idx = np.argpartition(-sims[ch], k_parent - 1)[:k_parent]\n",
    "                ch = ch[idx]\n",
    "            for c in ch:\n",
    "                S_c = S[par] + float(p[c])\n",
    "                L_c = Llen[par] + 1\n",
    "                K_c = S_c / (L_c + eps)\n",
    "                if (c not in cand_best) or (K_c > cand_best[c][2]):\n",
    "                    cand_best[c] = (S_c, L_c, K_c)\n",
    "\n",
    "        if not cand_best:\n",
    "            break\n",
    "\n",
    "        kept = sorted(cand_best.items(), key=lambda x: x[1][2], reverse=True)[:min(beam, len(cand_best))]\n",
    "        next_level = [i for i, _ in kept]\n",
    "        for i, (Si, Li, Ki) in kept:\n",
    "            S[i], Llen[i], K[i] = Si, Li, Ki\n",
    "\n",
    "        levels.append(next_level)\n",
    "        cur = next_level\n",
    "        level_id += 1\n",
    "        if max_depth is not None and level_id >= max_depth:\n",
    "            break\n",
    "\n",
    "    return K, levels, sims, p\n",
    "\n",
    "\n",
    "\n",
    "def topk_labels_by_avg(\n",
    "    doc_vec, label_emb, adj_upper, rootㄴ=(0,), beam=5, per_parent=\"l+2\", k=5, **kw\n",
    "):\n",
    "    \"\"\"평균 점수 기반 최종 상위 k 라벨(루트 제외).\"\"\"\n",
    "    K, levels, sims, p = hierarchical_beam_similarity_avg(\n",
    "        doc_vec, label_emb, adj_upper, root=list(roots), beam=beam, per_parent=per_parent, **kw\n",
    "    )\n",
    "    root_set = set(roots)\n",
    "    order = np.argsort(-K)\n",
    "    order = [i for i in order if i not in root_set and np.isfinite(K[i])]\n",
    "    top = order[:k]\n",
    "    return top, K[top]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb5f7b9f-1627-4a14-aca0-769891ef07db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T01:29:20.697931Z",
     "iopub.status.busy": "2025-11-11T01:29:20.697537Z",
     "iopub.status.idle": "2025-11-11T01:29:20.705785Z",
     "shell.execute_reply": "2025-11-11T01:29:20.705328Z",
     "shell.execute_reply.started": "2025-11-11T01:29:20.697898Z"
    }
   },
   "outputs": [],
   "source": [
    "# silver label\n",
    "# documnet bert 마지막층 unfrozen하고\n",
    "# 학습 돌리면서 label 추가하기\n",
    "\n",
    "from __future__ import annotations\n",
    "import argparse\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------- IO Utils -----------------------------\n",
    "\n",
    "def l2_normalize(x: np.ndarray, axis: int = -1, eps: float = 1e-12) -> np.ndarray:\n",
    "    n = np.linalg.norm(x, axis=axis, keepdims=True)\n",
    "    return x / (n + eps)\n",
    "\n",
    "\n",
    "def load_embeddings_csv(path: str | Path, id_col: str = \"id\") -> Tuple[List[int], np.ndarray]:\n",
    "    \"\"\"Load embeddings from CSV where the first column is an id and the rest are feature columns.\n",
    "    Returns (ids, float32 matrix).\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    cols = list(df.columns)\n",
    "    if id_col in df.columns:\n",
    "        id_series = df[id_col]\n",
    "        X = df.drop(columns=[id_col])\n",
    "    else:\n",
    "        # Fallback: use the first column as id\n",
    "        id_series = df.iloc[:, 0]\n",
    "        X = df.iloc[:, 1:]\n",
    "    ids = id_series.astype(int).tolist()\n",
    "    X = X.to_numpy(dtype=np.float32)\n",
    "    return ids, X\n",
    "\n",
    "\n",
    "\n",
    "# ---------------- Hierarchical beam search (average score) ----------------\n",
    "\n",
    "\n",
    "def all_label_similarity(\n",
    "    doc_vec: np.ndarray,\n",
    "    label_emb: np.ndarray,\n",
    "    tau: float = 0.35,\n",
    "    normalize: bool = True,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"문서 임베딩 vs 모든 라벨 임베딩 점수 한 번에 계산.\n",
    "\n",
    "    반환:\n",
    "        sims: (N,) 원시 유사도 (cosine 기반)\n",
    "        p:    (N,) sigmoid 점수\n",
    "    \"\"\"\n",
    "    doc = np.asarray(doc_vec, dtype=np.float32)\n",
    "    L = np.asarray(label_emb, dtype=np.float32)\n",
    "\n",
    "    # 모든 라벨에 대해 한 번에\n",
    "    sims = L @ doc  # (N,)\n",
    "    p = 1.0 / (1.0 + np.exp(-sims / max(tau, 1e-6)))\n",
    "    return sims, p\n",
    "\n",
    "\n",
    "def silver_labeling(\n",
    "    doc_ids: List[int],\n",
    "    docs: np.ndarray,\n",
    "    label_ids: List[int],\n",
    "    label_emb: np.ndarray,\n",
    "    threshold: float = 0.8,\n",
    "    top_k: int = 3,\n",
    "    tau: float = 0.35,\n",
    "    root_id: int = 0,\n",
    "    normalize: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"문서마다 '모든' 라벨 임베딩을 비교해서 점수 높은 라벨을 뽑는 버전.\n",
    "\n",
    "    - 트리(adj) 안 탐.\n",
    "    - root_id는 결과에서 제외.\n",
    "    - p >= threshold 인 애들 중에서 top_k만 고름.\n",
    "    \"\"\"\n",
    "    # 기본 정합성 체크\n",
    "    label_ids = list(label_ids)\n",
    "    N = label_emb.shape[0]\n",
    "    if len(label_ids) != N:\n",
    "        raise ValueError(f\"label_ids 길이({len(label_ids)})와 label_emb 행({N})이 다릅니다.\")\n",
    "\n",
    "    rows = []\n",
    "    for idx, (doc_id, d) in enumerate(zip(doc_ids, docs)):\n",
    "        sims, p = all_label_similarity(d, label_emb, tau=tau, normalize=normalize)\n",
    "\n",
    "        # root는 제외하고, threshold 이상만 후보로\n",
    "        candidates = [\n",
    "            (i, float(p[i]))\n",
    "            for i in range(N)\n",
    "            if i != root_id and np.isfinite(p[i]) and p[i] >= threshold\n",
    "        ]\n",
    "\n",
    "        # 점수 내림차순 정렬\n",
    "        candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        selected = candidates[:top_k]\n",
    "\n",
    "        row = {\"doc_id\": int(doc_id)}\n",
    "        for j in range(top_k):\n",
    "            if j < len(selected):\n",
    "                li, sc = selected[j]\n",
    "                row[f\"label_id_{j+1}\"] = int(label_ids[li])\n",
    "                row[f\"score_{j+1}\"] = float(sc)\n",
    "            else:\n",
    "                row[f\"label_id_{j+1}\"] = np.nan\n",
    "                row[f\"score_{j+1}\"] = np.nan\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "        if (idx + 1) % 100 == 0:\n",
    "            print(f\"Processed {idx+1} / {len(doc_ids)} docs...\")\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e4244ac-1ed8-470c-bc63-1470f460ea33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T01:29:24.925335Z",
     "iopub.status.busy": "2025-11-11T01:29:24.925114Z",
     "iopub.status.idle": "2025-11-11T01:29:24.940228Z",
     "shell.execute_reply": "2025-11-11T01:29:24.939769Z",
     "shell.execute_reply.started": "2025-11-11T01:29:24.925320Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Self-training pipeline with hierarchical silver labeling and dynamic dataloaders.\n",
    "\n",
    "- Reads document/label embeddings CSVs (first column \"id\", rest feat000..feat127)\n",
    "- Reads upper-triangular adjacency (A[i,j]=1 means i->j)\n",
    "- Makes initial silver labels via hierarchical beam search (average score)\n",
    "- Splits into train/val on silver set; keeps the rest as unlabeled pool\n",
    "- Trains a multi-label classifier (Linear/MLP) with BCEWithLogitsLoss\n",
    "- Each epoch, pseudo-labels unlabeled docs whose predicted probs exceed a threshold\n",
    "- Adds them to the training set (up to top_k per doc), with patience-based early stopping\n",
    "\n",
    "Run example\n",
    "-----------\n",
    "python self_training_pipeline.py \\\n",
    "  --doc_csv docs.csv \\\n",
    "  --label_csv labels.csv \\\n",
    "  --adj adj.npy \\\n",
    "  --val_ratio 0.2 --epochs 50 --patience 5 \\\n",
    "  --silver_threshold 0.60 --silver_topk 3 --beam 5 --tau 0.35 --root_id 0 \\\n",
    "  --pseudo_threshold 0.70 --pseudo_topk 3 --batch_size 256 --lr 1e-3\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import argparse\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "def load_embeddings_csv(path: str | Path, id_col: str = \"id\") -> Tuple[List[int], np.ndarray]:\n",
    "    \"\"\"Load embeddings from CSV where the first column is an id and the rest are feature columns.\n",
    "    Returns (ids, float32 matrix).\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    cols = list(df.columns)\n",
    "    if id_col in df.columns:\n",
    "        id_series = df[id_col]\n",
    "        X = df.drop(columns=[id_col])\n",
    "    else:\n",
    "        # Fallback: use the first column as id\n",
    "        id_series = df.iloc[:, 0]\n",
    "        X = df.iloc[:, 1:]\n",
    "    ids = id_series.astype(int).tolist()\n",
    "    X = X.to_numpy(dtype=np.float32)\n",
    "    return ids, X\n",
    "\n",
    "\n",
    "# ----------------------------- Datasets -----------------------------\n",
    "\n",
    "class MultiLabelDataset(Dataset):\n",
    "    def __init__(self, X: np.ndarray, Y: np.ndarray, indices: List[int] | None = None):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.indices = np.array(indices if indices is not None else np.arange(X.shape[0]), dtype=np.int64)\n",
    "    def __len__(self):\n",
    "        return self.indices.shape[0]\n",
    "    def __getitem__(self, idx: int):\n",
    "        i = int(self.indices[idx])\n",
    "        x = torch.from_numpy(self.X[i])\n",
    "        y = torch.from_numpy(self.Y[i])\n",
    "        return x, y\n",
    "\n",
    "class UnlabeledDataset(Dataset):\n",
    "    def __init__(self, X: np.ndarray, indices: List[int]):\n",
    "        self.X = X\n",
    "        self.indices = np.array(indices, dtype=np.int64)\n",
    "    def __len__(self):\n",
    "        return self.indices.shape[0]\n",
    "    def __getitem__(self, idx: int):\n",
    "        i = int(self.indices[idx])\n",
    "        x = torch.from_numpy(self.X[i])\n",
    "        return x, i\n",
    "\n",
    "# ----------------------------- Model -----------------------------\n",
    "\n",
    "class MLPHead(nn.Module):\n",
    "    def __init__(self, in_dim: int, out_dim: int, hidden: int | None = 256, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        if hidden is None or hidden <= 0:\n",
    "            self.net = nn.Sequential(\n",
    "                nn.LayerNorm(in_dim),\n",
    "                nn.Linear(in_dim, out_dim),\n",
    "            )\n",
    "        else:\n",
    "            self.net = nn.Sequential(\n",
    "                nn.LayerNorm(in_dim),\n",
    "                nn.Linear(in_dim, hidden),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(hidden, out_dim),\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# ----------------------------- Utils -----------------------------\n",
    "\n",
    "def to_device(batch, device):\n",
    "    if isinstance(batch, (tuple, list)):\n",
    "        return [b.to(device) if torch.is_tensor(b) else b for b in batch]\n",
    "    return batch.to(device)\n",
    "\n",
    "\n",
    "def micro_f1(y_true: np.ndarray, y_prob: np.ndarray, thr: float = 0.5, eps: float = 1e-9) -> float:\n",
    "    y_pred = (y_prob >= thr).astype(np.float32)\n",
    "    tp = (y_true * y_pred).sum()\n",
    "    fp = ((1 - y_true) * y_pred).sum()\n",
    "    fn = (y_true * (1 - y_pred)).sum()\n",
    "    prec = tp / (tp + fp + eps)\n",
    "    rec = tp / (tp + fn + eps)\n",
    "    f1 = 2 * prec * rec / (prec + rec + eps)\n",
    "    return float(f1)\n",
    "\n",
    "# -------- Initial silver labeling (no CSV save; in-memory) --------\n",
    "def make_initial_silver_hier(\n",
    "    docs: np.ndarray,\n",
    "    labels: np.ndarray,\n",
    "    adj: np.ndarray,\n",
    "    roots: list[int] = [0],\n",
    "    silver_threshold: float = 0.6,    # 이건 avg(K) 기준\n",
    "    silver_topk: int = 3,\n",
    "    beam: int = 5,\n",
    "    per_parent: str | int = \"l+2\",\n",
    "    tau: float = 0.35,\n",
    ") -> list[list[int]]:\n",
    "    \"\"\"\n",
    "    계층 빔 서치로 각 문서의 라벨 후보를 뽑는다.\n",
    "    - 계층 밖 라벨은 애초에 안 들어옴\n",
    "    - 루트들은 결과에서 제외\n",
    "    - K(경로 평균) >= silver_threshold 인 애들 중 top-k\n",
    "    \"\"\"\n",
    "    N = labels.shape[0]\n",
    "    silver: list[list[int]] = []\n",
    "    root_set = set(roots)\n",
    "\n",
    "    for d in docs:\n",
    "        K, levels, sims, p = hierarchical_beam_similarity_avg(\n",
    "            d, labels, adj,\n",
    "            roots=roots,\n",
    "            beam=beam,\n",
    "            per_parent=per_parent,\n",
    "            tau=tau,\n",
    "            normalize=False,   # 너 임베딩이 이미 L2라면 False\n",
    "        )\n",
    "        # 평균 점수로 정렬\n",
    "        order = np.argsort(-K)\n",
    "        # 루트는 제외, 유한한 것만\n",
    "        order = [i for i in order if (i not in root_set) and np.isfinite(K[i])]\n",
    "        # threshold 통과한 것만\n",
    "        cand = [i for i in order if K[i] >= silver_threshold]\n",
    "        selected = cand[:silver_topk]\n",
    "        silver.append(selected)\n",
    "\n",
    "    return silver\n",
    "\n",
    "def make_initial_silver(\n",
    "    docs: np.ndarray,\n",
    "    labels: np.ndarray,\n",
    "    adj: np.ndarray,              # 이제 안 씀 (호환용으로만 둠)\n",
    "    silver_threshold: float = 0.9,\n",
    "    silver_topk: int = 3,\n",
    "    beam: int = 5,                # 이제 안 씀\n",
    "    tau: float = 0.35,\n",
    "    root_id: int = 0,\n",
    ") -> List[List[int]]:\n",
    "    \"\"\"\n",
    "    문서마다 전 라벨 임베딩과의 유사도를 보고 초기 silver label을 만든다.\n",
    "    - 트리/경로 탐색 안 함\n",
    "    - root_id는 결과에서 제외\n",
    "    - p >= silver_threshold인 라벨 중에서 상위 silver_topk만 남김\n",
    "    \"\"\"\n",
    "    N = labels.shape[0]\n",
    "    silver: List[List[int]] = []\n",
    "\n",
    "    for d in docs:\n",
    "        # 문서 vs 모든 라벨 점수\n",
    "        sims, p = all_label_similarity(d, labels, tau=tau, normalize=True)\n",
    "\n",
    "        # threshold 통과 + root 제외\n",
    "        cand = [\n",
    "            (i, float(p[i]))\n",
    "            for i in range(N)\n",
    "            if i != root_id and np.isfinite(p[i]) and p[i] >= silver_threshold\n",
    "        ]\n",
    "\n",
    "        # 점수 높은 순\n",
    "        cand.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # label index만 추출\n",
    "        selected = [i for i, _ in cand[:silver_topk]]\n",
    "        silver.append(selected)\n",
    "\n",
    "    return silver\n",
    "# ------------------------ Train / Self-Training ------------------------\n",
    "\n",
    "def train_epoch(model, loader, optim, device, criterion):\n",
    "    model.train()\n",
    "    total = 0.0\n",
    "    for x, y in loader:\n",
    "        x, y = to_device(x, device), to_device(y, device)\n",
    "        optim.zero_grad(set_to_none=True)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        total += float(loss.detach().cpu().item()) * x.size(0)\n",
    "    return total / max(1, len(loader.dataset))\n",
    "\n",
    "\n",
    "def eval_epoch(model, loader, device, criterion, thr=0.5):\n",
    "    model.eval()\n",
    "    total = 0.0\n",
    "    ys = []\n",
    "    ps = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = to_device(x, device), to_device(y, device)\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            total += float(loss.detach().cpu().item()) * x.size(0)\n",
    "            prob = torch.sigmoid(logits).detach().cpu().numpy()\n",
    "            ys.append(y.detach().cpu().numpy())\n",
    "            ps.append(prob)\n",
    "    y_true = np.concatenate(ys, axis=0)\n",
    "    y_prob = np.concatenate(ps, axis=0)\n",
    "    f1 = micro_f1(y_true, y_prob, thr=thr)\n",
    "    return total / max(1, len(loader.dataset)), f1, y_prob\n",
    "\n",
    "\n",
    "def pseudo_label_and_grow(model, unl_ds: UnlabeledDataset,\n",
    "                          num_labels: int,\n",
    "                          pseudo_threshold: float = 0.9, pseudo_topk: int = 3,\n",
    "                          device: str = \"cpu\", batch_size: int = 512):\n",
    "    \"\"\"Infer on unlabeled, select labels with prob>=threshold (top-k), and return new_indices and Y matrix.\"\"\"\n",
    "    if len(unl_ds) == 0:\n",
    "        return [], np.zeros((0, num_labels), dtype=np.float32)\n",
    "    loader = DataLoader(unl_ds, batch_size=batch_size, shuffle=False)\n",
    "    model.eval()\n",
    "    all_idx: List[int] = []\n",
    "    all_y: List[np.ndarray] = []\n",
    "    with torch.no_grad():\n",
    "        for xb, idxs in loader:\n",
    "            xb = xb.to(device)\n",
    "            logits = model(xb)\n",
    "            prob = torch.sigmoid(logits).detach().cpu().numpy()\n",
    "            for p, i in zip(prob, idxs.numpy().tolist()):\n",
    "                sel = np.flatnonzero(p >= pseudo_threshold)\n",
    "                if sel.size > 0:\n",
    "                    # keep at most top-k by prob\n",
    "                    if sel.size > pseudo_topk:\n",
    "                        top = np.argpartition(-p[sel], pseudo_topk - 1)[:pseudo_topk]\n",
    "                        sel = sel[top]\n",
    "                    y = np.zeros(num_labels, dtype=np.float32)\n",
    "                    y[sel] = 1.0\n",
    "                    all_idx.append(int(i))\n",
    "                    all_y.append(y)\n",
    "    if len(all_idx) == 0:\n",
    "        return [], np.zeros((0, num_labels), dtype=np.float32)\n",
    "    Y_new = np.stack(all_y, axis=0)\n",
    "    return all_idx, Y_new\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a6c93d9-087d-4037-a7f2-ed637b3a2344",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T01:29:30.238638Z",
     "iopub.status.busy": "2025-11-11T01:29:30.238416Z",
     "iopub.status.idle": "2025-11-11T01:30:19.050901Z",
     "shell.execute_reply": "2025-11-11T01:30:19.050457Z",
     "shell.execute_reply.started": "2025-11-11T01:29:30.238621Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5650\n",
      "1412\n",
      "22425\n"
     ]
    }
   ],
   "source": [
    "\n",
    "doc_ids, X = load_embeddings_csv(\"Amazon_products/document_embeddings_768d.csv\")\n",
    "label_ids, L = load_embeddings_csv(\"Amazon_products/label_embeddings_gat.csv\")\n",
    "B\n",
    "\n",
    "# Ensure label order matches adjacency\n",
    "order = np.argsort(label_ids)\n",
    "label_ids_sorted = [label_ids[i] for i in order]\n",
    "L = L[order]\n",
    "assert B.shape == (L.shape[0], L.shape[0]), \"Adjacency/label size mismatch\"\n",
    "\n",
    "# Initial silver labeling\n",
    "# silver = make_initial_silver(X, L, B, silver_threshold=0.9, silver_topk=3, tau=0.35, root_id=0)\n",
    "silver = make_initial_silver_hier(\n",
    "    X,          # docs\n",
    "    L,          # label_emb\n",
    "    B,          # upper adj\n",
    "    roots=roots,  # 여러 개면 [0, 10, 25] 이런 식\n",
    "    silver_threshold=0.55,\n",
    "    silver_topk=3,\n",
    "    beam=5,\n",
    "    per_parent=\"l+2\",\n",
    "    tau=0.35,\n",
    ")\n",
    "\n",
    "val_ratio  = 0.2\n",
    "epochs = 50\n",
    "patience = 5\n",
    "batch_size = 256\n",
    "lr = 1e-3\n",
    "hidden = 256\n",
    "dropout = 0.1\n",
    "pseudo_threshold = 0.6\n",
    "pseudo_topk = 3\n",
    "seed = 42\n",
    "# Build multi-hot targets for silver docs; split into train/val; keep unlabeled indices\n",
    "N_labels = L.shape[0]\n",
    "Y = np.zeros((X.shape[0], N_labels), dtype=np.float32)\n",
    "silver_mask = np.zeros(X.shape[0], dtype=bool)\n",
    "for i, lab_list in enumerate(silver):\n",
    "    if len(lab_list) > 0:\n",
    "        Y[i, lab_list] = 1.0\n",
    "        silver_mask[i] = True\n",
    "\n",
    "idx_silver = np.flatnonzero(silver_mask)\n",
    "idx_unl = np.flatnonzero(~silver_mask)\n",
    "\n",
    "# Train/val split\n",
    "rng = np.random.default_rng(seed)\n",
    "rng.shuffle(idx_silver)\n",
    "n_val = max(1, int(len(idx_silver) * val_ratio)) if len(idx_silver) > 0 else 0\n",
    "idx_val = idx_silver[:n_val]\n",
    "idx_train = idx_silver[n_val:]\n",
    "\n",
    "train_ds = MultiLabelDataset(X, Y, indices=idx_train)\n",
    "val_ds = MultiLabelDataset(X, Y, indices=idx_val) if n_val > 0 else None\n",
    "unl_ds = UnlabeledDataset(X, idx_unl.tolist())\n",
    "\n",
    "print(len(train_ds))\n",
    "print(len(val_ds))\n",
    "print(len(unl_ds))\n",
    "# Model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = MLPHead(in_dim=X.shape[1], out_dim=N_labels, hidden=hidden, dropout=dropout).to(device)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "crit = nn.BCEWithLogitsLoss()\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False) if val_ds is not None else None\n",
    "\n",
    "best_val = float(\"inf\")\n",
    "best_epoch = -1\n",
    "no_improve = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0795c01b-891d-48d1-a90f-e252f47da5f8",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-11T00:27:03.901999Z",
     "iopub.status.idle": "2025-11-11T00:27:03.902173Z",
     "shell.execute_reply": "2025-11-11T00:27:03.902095Z",
     "shell.execute_reply.started": "2025-11-11T00:27:03.902087Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(train_ds))\n",
    "print(len(val_ds))\n",
    "\n",
    "print(len(unl_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a44d818e-888e-487c-a9fe-c65d524e0b20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T00:44:04.756635Z",
     "iopub.status.busy": "2025-11-11T00:44:04.756408Z",
     "iopub.status.idle": "2025-11-11T00:44:14.986667Z",
     "shell.execute_reply": "2025-11-11T00:44:14.985429Z",
     "shell.execute_reply.started": "2025-11-11T00:44:04.756619Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train_loss=0.183  val_loss=0.021  val_f1=0.000\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 002 | train_loss=0.020  val_loss=0.018  val_f1=0.001\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 003 | train_loss=0.017  val_loss=0.015  val_f1=0.203\n",
      "  + Added 827 pseudo-labeled docs (unl pool → 21598 left)\n",
      "Epoch 004 | train_loss=0.014  val_loss=0.014  val_f1=0.257\n",
      "  + Added 1096 pseudo-labeled docs (unl pool → 20502 left)\n",
      "Epoch 005 | train_loss=0.012  val_loss=0.013  val_f1=0.277\n",
      "  + Added 2385 pseudo-labeled docs (unl pool → 18117 left)\n",
      "Epoch 006 | train_loss=0.009  val_loss=0.013  val_f1=0.329\n",
      "  + Added 1878 pseudo-labeled docs (unl pool → 16239 left)\n",
      "Epoch 007 | train_loss=0.008  val_loss=0.013  val_f1=0.342\n",
      "  + Added 2058 pseudo-labeled docs (unl pool → 14181 left)\n",
      "Epoch 008 | train_loss=0.006  val_loss=0.013  val_f1=0.345\n",
      "  + Added 2849 pseudo-labeled docs (unl pool → 11332 left)\n",
      "Epoch 009 | train_loss=0.005  val_loss=0.012  val_f1=0.369\n",
      "  + Added 2377 pseudo-labeled docs (unl pool → 8955 left)\n",
      "Epoch 010 | train_loss=0.004  val_loss=0.012  val_f1=0.432\n",
      "  + Added 2174 pseudo-labeled docs (unl pool → 6781 left)\n",
      "Epoch 011 | train_loss=0.004  val_loss=0.012  val_f1=0.438\n",
      "  + Added 1681 pseudo-labeled docs (unl pool → 5100 left)\n",
      "Epoch 012 | train_loss=0.004  val_loss=0.011  val_f1=0.447\n",
      "  + Added 1077 pseudo-labeled docs (unl pool → 4023 left)\n",
      "Epoch 013 | train_loss=0.003  val_loss=0.011  val_f1=0.461\n",
      "  + Added 624 pseudo-labeled docs (unl pool → 3399 left)\n",
      "Epoch 014 | train_loss=0.003  val_loss=0.011  val_f1=0.477\n",
      "  + Added 893 pseudo-labeled docs (unl pool → 2506 left)\n",
      "Epoch 015 | train_loss=0.003  val_loss=0.011  val_f1=0.486\n",
      "  + Added 119 pseudo-labeled docs (unl pool → 2387 left)\n",
      "Epoch 016 | train_loss=0.003  val_loss=0.011  val_f1=0.500\n",
      "  + Added 329 pseudo-labeled docs (unl pool → 2058 left)\n",
      "Epoch 017 | train_loss=0.003  val_loss=0.010  val_f1=0.515\n",
      "  + Added 131 pseudo-labeled docs (unl pool → 1927 left)\n",
      "Epoch 018 | train_loss=0.003  val_loss=0.010  val_f1=0.528\n",
      "  + Added 433 pseudo-labeled docs (unl pool → 1494 left)\n",
      "Epoch 019 | train_loss=0.003  val_loss=0.010  val_f1=0.525\n",
      "  + Added 172 pseudo-labeled docs (unl pool → 1322 left)\n",
      "Epoch 020 | train_loss=0.003  val_loss=0.010  val_f1=0.560\n",
      "  + Added 87 pseudo-labeled docs (unl pool → 1235 left)\n",
      "Epoch 021 | train_loss=0.002  val_loss=0.010  val_f1=0.544\n",
      "  + Added 49 pseudo-labeled docs (unl pool → 1186 left)\n",
      "Epoch 022 | train_loss=0.002  val_loss=0.010  val_f1=0.504\n",
      "  + Added 108 pseudo-labeled docs (unl pool → 1078 left)\n",
      "Epoch 023 | train_loss=0.002  val_loss=0.010  val_f1=0.555\n",
      "  + Added 98 pseudo-labeled docs (unl pool → 980 left)\n",
      "Epoch 024 | train_loss=0.002  val_loss=0.010  val_f1=0.544\n",
      "  + Added 125 pseudo-labeled docs (unl pool → 855 left)\n",
      "Epoch 025 | train_loss=0.002  val_loss=0.010  val_f1=0.559\n",
      "  + Added 75 pseudo-labeled docs (unl pool → 780 left)\n",
      "Epoch 026 | train_loss=0.002  val_loss=0.010  val_f1=0.558\n",
      "  + Added 32 pseudo-labeled docs (unl pool → 748 left)\n",
      "Epoch 027 | train_loss=0.002  val_loss=0.010  val_f1=0.566\n",
      "  + Added 183 pseudo-labeled docs (unl pool → 565 left)\n",
      "Epoch 028 | train_loss=0.002  val_loss=0.010  val_f1=0.573\n",
      "Early stopping at epoch 28 (best@23)\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    tr_loss = train_epoch(model, train_loader, opt, device, crit)\n",
    "    if val_loader is not None and len(val_ds) > 0:\n",
    "        va_loss, va_f1, _ = eval_epoch(model, val_loader, device, crit, thr=0.5)\n",
    "        print(f\"Epoch {epoch:03d} | train_loss={tr_loss:.3f}  val_loss={va_loss:.3f}  val_f1={va_f1:.3f}\")\n",
    "        # Early stopping on val_loss\n",
    "        if va_loss + 1e-6 < best_val:\n",
    "            best_val = va_loss\n",
    "            best_epoch = epoch\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch} (best@{best_epoch})\")\n",
    "                break\n",
    "    else:\n",
    "        print(f\"Epoch {epoch:03d} | train_loss={tr_loss:.3f}\")\n",
    "\n",
    "    # Pseudo-labeling step (after each epoch)\n",
    "    new_idx, Y_new = pseudo_label_and_grow(\n",
    "        model, unl_ds, N_labels,\n",
    "        pseudo_threshold=pseudo_threshold,\n",
    "        pseudo_topk=pseudo_topk,\n",
    "        device=device,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    if len(new_idx) > 0:\n",
    "        # Extend training set\n",
    "        # Update Y with new labels\n",
    "        for i, y in zip(new_idx, Y_new):\n",
    "            Y[i] = np.maximum(Y[i], y)  # merge if any\n",
    "        # Move indices from unlabeled to train\n",
    "        keep_mask = ~np.isin(unl_ds.indices, np.array(new_idx, dtype=np.int64))\n",
    "        unl_ds.indices = unl_ds.indices[keep_mask]\n",
    "        train_ds.indices = np.concatenate([train_ds.indices, np.array(new_idx, dtype=np.int64)])\n",
    "        # Rebuild train loader to reflect length change\n",
    "        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "        print(f\"  + Added {len(new_idx)} pseudo-labeled docs (unl pool → {len(unl_ds)} left)\")\n",
    "    else:\n",
    "        print(\"  + No pseudo-labeled docs added this epoch\")\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cccc039-f93f-49de-8843-19c7b5e4338d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d15867a-f278-41ab-88a0-e751c62e5873",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0b4cfc-c593-4841-b847-2759ad663984",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4c63ecfd-663d-4ed8-88b2-e8ecbc14de5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T02:07:20.508003Z",
     "iopub.status.busy": "2025-11-11T02:07:20.507851Z",
     "iopub.status.idle": "2025-11-11T02:09:39.812489Z",
     "shell.execute_reply": "2025-11-11T02:09:39.811938Z",
     "shell.execute_reply.started": "2025-11-11T02:07:20.507988Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 29487\n",
      "with silver: 7062\n",
      "unlabeled : 22425\n",
      "5650 1412 22425\n",
      "Epoch 001 | train_loss=0.607  val_loss=0.444  val_f1=0.000\n",
      "  + (skip pseudo-labeling on warmup epoch)\n",
      "Epoch 002 | train_loss=0.269  val_loss=0.110  val_f1=0.072\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 003 | train_loss=0.076  val_loss=0.058  val_f1=0.063\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 004 | train_loss=0.055  val_loss=0.051  val_f1=0.005\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 005 | train_loss=0.049  val_loss=0.047  val_f1=0.018\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 006 | train_loss=0.045  val_loss=0.043  val_f1=0.100\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 007 | train_loss=0.043  val_loss=0.041  val_f1=0.192\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 008 | train_loss=0.041  val_loss=0.039  val_f1=0.203\n",
      "  + Added 1 pseudo-labeled docs (unl pool → 22424 left)\n",
      "Epoch 009 | train_loss=0.039  val_loss=0.037  val_f1=0.209\n",
      "  + Added 2 pseudo-labeled docs (unl pool → 22422 left)\n",
      "Epoch 010 | train_loss=0.037  val_loss=0.036  val_f1=0.209\n",
      "  + Added 1 pseudo-labeled docs (unl pool → 22421 left)\n",
      "Epoch 011 | train_loss=0.036  val_loss=0.035  val_f1=0.224\n",
      "  + Added 9 pseudo-labeled docs (unl pool → 22412 left)\n",
      "Epoch 012 | train_loss=0.035  val_loss=0.033  val_f1=0.241\n",
      "  + Added 294 pseudo-labeled docs (unl pool → 22118 left)\n",
      "Epoch 013 | train_loss=0.033  val_loss=0.032  val_f1=0.241\n",
      "  + Added 810 pseudo-labeled docs (unl pool → 21308 left)\n",
      "Epoch 014 | train_loss=0.032  val_loss=0.031  val_f1=0.275\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 015 | train_loss=0.031  val_loss=0.030  val_f1=0.310\n",
      "  + Added 523 pseudo-labeled docs (unl pool → 20785 left)\n",
      "Epoch 016 | train_loss=0.030  val_loss=0.029  val_f1=0.316\n",
      "  + Added 224 pseudo-labeled docs (unl pool → 20561 left)\n",
      "Epoch 017 | train_loss=0.029  val_loss=0.028  val_f1=0.326\n",
      "  + Added 1259 pseudo-labeled docs (unl pool → 19302 left)\n",
      "Epoch 018 | train_loss=0.028  val_loss=0.027  val_f1=0.335\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 019 | train_loss=0.027  val_loss=0.028  val_f1=0.321\n",
      "  + Added 2289 pseudo-labeled docs (unl pool → 17013 left)\n",
      "Epoch 020 | train_loss=0.027  val_loss=0.026  val_f1=0.355\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 021 | train_loss=0.026  val_loss=0.028  val_f1=0.354\n",
      "  + Added 3097 pseudo-labeled docs (unl pool → 13916 left)\n",
      "Epoch 022 | train_loss=0.026  val_loss=0.025  val_f1=0.380\n",
      "  + Added 1811 pseudo-labeled docs (unl pool → 12105 left)\n",
      "Epoch 023 | train_loss=0.025  val_loss=0.024  val_f1=0.397\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 024 | train_loss=0.024  val_loss=0.024  val_f1=0.414\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 025 | train_loss=0.024  val_loss=0.024  val_f1=0.423\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 026 | train_loss=0.023  val_loss=0.023  val_f1=0.443\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 027 | train_loss=0.023  val_loss=0.024  val_f1=0.447\n",
      "  + Added 2111 pseudo-labeled docs (unl pool → 9994 left)\n",
      "Epoch 028 | train_loss=0.023  val_loss=0.023  val_f1=0.466\n",
      "  + Added 1 pseudo-labeled docs (unl pool → 9993 left)\n",
      "Epoch 029 | train_loss=0.022  val_loss=0.022  val_f1=0.462\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 030 | train_loss=0.022  val_loss=0.022  val_f1=0.465\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 031 | train_loss=0.022  val_loss=0.022  val_f1=0.472\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 032 | train_loss=0.022  val_loss=0.022  val_f1=0.465\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 033 | train_loss=0.021  val_loss=0.022  val_f1=0.483\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 034 | train_loss=0.021  val_loss=0.022  val_f1=0.491\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 035 | train_loss=0.021  val_loss=0.021  val_f1=0.482\n",
      "  + Added 354 pseudo-labeled docs (unl pool → 9639 left)\n",
      "Epoch 036 | train_loss=0.021  val_loss=0.036  val_f1=0.420\n",
      "  + Added 4 pseudo-labeled docs (unl pool → 9635 left)\n",
      "Epoch 037 | train_loss=0.023  val_loss=0.022  val_f1=0.447\n",
      "  + Added 363 pseudo-labeled docs (unl pool → 9272 left)\n",
      "Epoch 038 | train_loss=0.021  val_loss=0.021  val_f1=0.481\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 039 | train_loss=0.020  val_loss=0.021  val_f1=0.482\n",
      "  + Added 79 pseudo-labeled docs (unl pool → 9193 left)\n",
      "Epoch 040 | train_loss=0.020  val_loss=0.020  val_f1=0.499\n",
      "  + Added 5 pseudo-labeled docs (unl pool → 9188 left)\n",
      "Epoch 041 | train_loss=0.020  val_loss=0.020  val_f1=0.479\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 042 | train_loss=0.020  val_loss=0.020  val_f1=0.501\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 043 | train_loss=0.020  val_loss=0.020  val_f1=0.483\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 044 | train_loss=0.020  val_loss=0.020  val_f1=0.492\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 045 | train_loss=0.020  val_loss=0.020  val_f1=0.508\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 046 | train_loss=0.019  val_loss=0.020  val_f1=0.485\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 047 | train_loss=0.019  val_loss=0.019  val_f1=0.519\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 048 | train_loss=0.019  val_loss=0.019  val_f1=0.510\n",
      "  + Added 6 pseudo-labeled docs (unl pool → 9182 left)\n",
      "Epoch 049 | train_loss=0.019  val_loss=0.019  val_f1=0.522\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 050 | train_loss=0.019  val_loss=0.019  val_f1=0.505\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 051 | train_loss=0.019  val_loss=0.019  val_f1=0.516\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 052 | train_loss=0.019  val_loss=0.019  val_f1=0.507\n",
      "  + Added 2 pseudo-labeled docs (unl pool → 9180 left)\n",
      "Epoch 053 | train_loss=0.019  val_loss=0.019  val_f1=0.524\n",
      "  + Added 4 pseudo-labeled docs (unl pool → 9176 left)\n",
      "Epoch 054 | train_loss=0.019  val_loss=0.019  val_f1=0.528\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 055 | train_loss=0.019  val_loss=0.019  val_f1=0.512\n",
      "  + Added 6 pseudo-labeled docs (unl pool → 9170 left)\n",
      "Epoch 056 | train_loss=0.018  val_loss=0.018  val_f1=0.520\n",
      "  + Added 5 pseudo-labeled docs (unl pool → 9165 left)\n",
      "Epoch 057 | train_loss=0.019  val_loss=0.019  val_f1=0.515\n",
      "  + Added 109 pseudo-labeled docs (unl pool → 9056 left)\n",
      "Epoch 058 | train_loss=0.018  val_loss=0.018  val_f1=0.528\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 059 | train_loss=0.018  val_loss=0.018  val_f1=0.531\n",
      "  + Added 5 pseudo-labeled docs (unl pool → 9051 left)\n",
      "Epoch 060 | train_loss=0.018  val_loss=0.019  val_f1=0.532\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 061 | train_loss=0.018  val_loss=0.018  val_f1=0.532\n",
      "  + Added 1 pseudo-labeled docs (unl pool → 9050 left)\n",
      "Epoch 062 | train_loss=0.018  val_loss=0.019  val_f1=0.506\n",
      "  + Added 71 pseudo-labeled docs (unl pool → 8979 left)\n",
      "Epoch 063 | train_loss=0.018  val_loss=0.018  val_f1=0.533\n",
      "  + Added 8 pseudo-labeled docs (unl pool → 8971 left)\n",
      "Epoch 064 | train_loss=0.018  val_loss=0.018  val_f1=0.522\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 065 | train_loss=0.018  val_loss=0.018  val_f1=0.535\n",
      "  + Added 15 pseudo-labeled docs (unl pool → 8956 left)\n",
      "Epoch 066 | train_loss=0.017  val_loss=0.018  val_f1=0.529\n",
      "  + Added 2 pseudo-labeled docs (unl pool → 8954 left)\n",
      "Epoch 067 | train_loss=0.018  val_loss=0.018  val_f1=0.533\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 068 | train_loss=0.018  val_loss=0.018  val_f1=0.524\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 069 | train_loss=0.018  val_loss=0.019  val_f1=0.510\n",
      "  + Added 49 pseudo-labeled docs (unl pool → 8905 left)\n",
      "Epoch 070 | train_loss=0.017  val_loss=0.019  val_f1=0.543\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 071 | train_loss=0.017  val_loss=0.017  val_f1=0.536\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 072 | train_loss=0.017  val_loss=0.018  val_f1=0.537\n",
      "  + Added 43 pseudo-labeled docs (unl pool → 8862 left)\n",
      "Epoch 073 | train_loss=0.017  val_loss=0.021  val_f1=0.525\n",
      "  + Added 1802 pseudo-labeled docs (unl pool → 7060 left)\n",
      "Epoch 074 | train_loss=0.017  val_loss=0.018  val_f1=0.516\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 075 | train_loss=0.017  val_loss=0.018  val_f1=0.536\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 076 | train_loss=0.017  val_loss=0.018  val_f1=0.519\n",
      "  + Added 4 pseudo-labeled docs (unl pool → 7056 left)\n",
      "Epoch 077 | train_loss=0.017  val_loss=0.027  val_f1=0.500\n",
      "  + Added 2387 pseudo-labeled docs (unl pool → 4669 left)\n",
      "Epoch 078 | train_loss=0.018  val_loss=0.018  val_f1=0.544\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 079 | train_loss=0.017  val_loss=0.019  val_f1=0.538\n",
      "  + Added 224 pseudo-labeled docs (unl pool → 4445 left)\n",
      "Epoch 080 | train_loss=0.017  val_loss=0.019  val_f1=0.524\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 081 | train_loss=0.017  val_loss=0.018  val_f1=0.518\n",
      "  + Added 1 pseudo-labeled docs (unl pool → 4444 left)\n",
      "Epoch 082 | train_loss=0.017  val_loss=0.017  val_f1=0.534\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 083 | train_loss=0.017  val_loss=0.018  val_f1=0.544\n",
      "  + Added 22 pseudo-labeled docs (unl pool → 4422 left)\n",
      "Epoch 084 | train_loss=0.017  val_loss=0.017  val_f1=0.540\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 085 | train_loss=0.017  val_loss=0.018  val_f1=0.536\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 086 | train_loss=0.017  val_loss=0.017  val_f1=0.547\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 087 | train_loss=0.017  val_loss=0.017  val_f1=0.539\n",
      "  + Added 18 pseudo-labeled docs (unl pool → 4404 left)\n",
      "Epoch 088 | train_loss=0.016  val_loss=0.017  val_f1=0.541\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 089 | train_loss=0.017  val_loss=0.017  val_f1=0.549\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 090 | train_loss=0.016  val_loss=0.017  val_f1=0.547\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 091 | train_loss=0.016  val_loss=0.018  val_f1=0.551\n",
      "  + Added 1 pseudo-labeled docs (unl pool → 4403 left)\n",
      "Epoch 092 | train_loss=0.016  val_loss=0.018  val_f1=0.553\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 093 | train_loss=0.017  val_loss=0.017  val_f1=0.541\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 094 | train_loss=0.016  val_loss=0.017  val_f1=0.551\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 095 | train_loss=0.016  val_loss=0.017  val_f1=0.557\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 096 | train_loss=0.016  val_loss=0.017  val_f1=0.547\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 097 | train_loss=0.016  val_loss=0.017  val_f1=0.544\n",
      "  + Added 12 pseudo-labeled docs (unl pool → 4391 left)\n",
      "Epoch 098 | train_loss=0.016  val_loss=0.017  val_f1=0.550\n",
      "  + Added 1 pseudo-labeled docs (unl pool → 4390 left)\n",
      "Epoch 099 | train_loss=0.016  val_loss=0.017  val_f1=0.548\n",
      "  + Added 11 pseudo-labeled docs (unl pool → 4379 left)\n",
      "Epoch 100 | train_loss=0.016  val_loss=0.017  val_f1=0.555\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 101 | train_loss=0.016  val_loss=0.016  val_f1=0.549\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 102 | train_loss=0.016  val_loss=0.017  val_f1=0.532\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 103 | train_loss=0.016  val_loss=0.017  val_f1=0.555\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 104 | train_loss=0.016  val_loss=0.017  val_f1=0.552\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 105 | train_loss=0.016  val_loss=0.017  val_f1=0.550\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 106 | train_loss=0.016  val_loss=0.016  val_f1=0.551\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 107 | train_loss=0.016  val_loss=0.017  val_f1=0.540\n",
      "  + Added 33 pseudo-labeled docs (unl pool → 4346 left)\n",
      "Epoch 108 | train_loss=0.016  val_loss=0.016  val_f1=0.558\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 109 | train_loss=0.016  val_loss=0.017  val_f1=0.541\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 110 | train_loss=0.016  val_loss=0.017  val_f1=0.560\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 111 | train_loss=0.016  val_loss=0.018  val_f1=0.531\n",
      "  + Added 2 pseudo-labeled docs (unl pool → 4344 left)\n",
      "Epoch 112 | train_loss=0.016  val_loss=0.017  val_f1=0.555\n",
      "  + Added 5 pseudo-labeled docs (unl pool → 4339 left)\n",
      "Epoch 113 | train_loss=0.016  val_loss=0.016  val_f1=0.557\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 114 | train_loss=0.015  val_loss=0.017  val_f1=0.548\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 115 | train_loss=0.015  val_loss=0.016  val_f1=0.556\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 116 | train_loss=0.016  val_loss=0.017  val_f1=0.542\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 117 | train_loss=0.015  val_loss=0.016  val_f1=0.561\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 118 | train_loss=0.015  val_loss=0.018  val_f1=0.566\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 119 | train_loss=0.016  val_loss=0.017  val_f1=0.553\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 120 | train_loss=0.015  val_loss=0.016  val_f1=0.557\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 121 | train_loss=0.016  val_loss=0.017  val_f1=0.547\n",
      "  + Added 1 pseudo-labeled docs (unl pool → 4338 left)\n",
      "Epoch 122 | train_loss=0.015  val_loss=0.017  val_f1=0.556\n",
      "  + Added 2 pseudo-labeled docs (unl pool → 4336 left)\n",
      "Epoch 123 | train_loss=0.015  val_loss=0.016  val_f1=0.560\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 124 | train_loss=0.015  val_loss=0.017  val_f1=0.558\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 125 | train_loss=0.015  val_loss=0.017  val_f1=0.556\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 126 | train_loss=0.015  val_loss=0.016  val_f1=0.557\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 127 | train_loss=0.015  val_loss=0.016  val_f1=0.564\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 128 | train_loss=0.015  val_loss=0.018  val_f1=0.551\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 129 | train_loss=0.015  val_loss=0.017  val_f1=0.542\n",
      "  + Added 1 pseudo-labeled docs (unl pool → 4335 left)\n",
      "Epoch 130 | train_loss=0.015  val_loss=0.018  val_f1=0.549\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 131 | train_loss=0.015  val_loss=0.016  val_f1=0.564\n",
      "  + Added 1 pseudo-labeled docs (unl pool → 4334 left)\n",
      "Epoch 132 | train_loss=0.015  val_loss=0.017  val_f1=0.527\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 133 | train_loss=0.015  val_loss=0.017  val_f1=0.562\n",
      "Early stopping at epoch 133 (best f1=0.5658)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 0) 문서/라벨 임베딩 로드\n",
    "doc_ids, X = load_embeddings_csv(\"Amazon_products/document_embeddings_768d.csv\")\n",
    "label_ids, L = load_embeddings_csv(\"Amazon_products/label_embeddings_gat.csv\")\n",
    "\n",
    "# 1) 라벨 순서와 B(부모->자식) 맞추기\n",
    "order = np.argsort(label_ids)\n",
    "label_ids = [label_ids[i] for i in order]\n",
    "L = L[order]\n",
    "assert B.shape == (L.shape[0], L.shape[0]), \"Adjacency/label size mismatch\"\n",
    "\n",
    "# 2) 계층 silver 만들기\n",
    "silver = make_initial_silver_hier(\n",
    "    X,          # docs (N, d)\n",
    "    L,          # label_emb (C, d)\n",
    "    B,          # upper adj (C, C)\n",
    "    roots=roots,\n",
    "    silver_threshold=0.55,\n",
    "    silver_topk=3,\n",
    "    beam=5,\n",
    "    per_parent=\"l+2\",\n",
    "    tau=0.35,\n",
    ")\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 1) 계층 정보에서 parents / children 뽑기\n",
    "#    B[parent, child] = 1 이라고 했으니까 그대로 씀\n",
    "# -------------------------------------------------\n",
    "# B: [C, C] (parent -> child)\n",
    "def build_parents_children(adj):\n",
    "    C = adj.shape[0]\n",
    "    parents = [np.flatnonzero(adj[:, j]).astype(np.int64) for j in range(C)]\n",
    "    children = [np.flatnonzero(adj[j]).astype(np.int64) for j in range(C)]\n",
    "    return parents, children\n",
    "\n",
    "parents, children = build_parents_children(B)\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2) silver → 계층 pos/neg 마스크로 변환\n",
    "# -------------------------------------------------\n",
    "def build_pos_neg_masks(silver, parents, children, num_labels):\n",
    "    \"\"\"\n",
    "    silver: list[list[int]]  # 문서마다 core label index들\n",
    "    parents / children: list[np.ndarray]\n",
    "    return:\n",
    "      pos_masks: np.array [N_docs, C]\n",
    "      neg_masks: np.array [N_docs, C]\n",
    "    \"\"\"\n",
    "    N = len(silver)\n",
    "    C = num_labels\n",
    "    pos_masks = np.zeros((N, C), dtype=np.float32)\n",
    "    neg_masks = np.zeros((N, C), dtype=np.float32)\n",
    "\n",
    "    all_idx = np.arange(C)\n",
    "\n",
    "    for i, core in enumerate(silver):\n",
    "        core = list(core)\n",
    "        # 1) core의 부모까지 positive\n",
    "        pos_set = set(core)\n",
    "        for c in core:\n",
    "            for p in parents[c]:\n",
    "                pos_set.add(int(p))\n",
    "\n",
    "        # 2) children은 나중에 negative에서 제외\n",
    "        child_set = set()\n",
    "        for c in core:\n",
    "            for ch in children[c]:\n",
    "                child_set.add(int(ch))\n",
    "\n",
    "        # pos 마스크\n",
    "        for p in pos_set:\n",
    "            pos_masks[i, p] = 1.0\n",
    "\n",
    "        # neg = 전체 - pos - children\n",
    "        for j in all_idx:\n",
    "            if j in pos_set:\n",
    "                continue\n",
    "            if j in child_set:\n",
    "                continue\n",
    "            neg_masks[i, j] = 1.0\n",
    "\n",
    "    return pos_masks, neg_masks\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3) Dataset: 문서 임베딩 + pos/neg 마스크\n",
    "# -------------------------------------------------\n",
    "class HierMultiLabelDataset(Dataset):\n",
    "    def __init__(self, X, pos_masks, neg_masks, indices=None):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.pos = pos_masks.astype(np.float32)\n",
    "        self.neg = neg_masks.astype(np.float32)\n",
    "        if indices is None:\n",
    "            self.indices = np.arange(self.X.shape[0], dtype=np.int64)\n",
    "        else:\n",
    "            self.indices = np.array(indices, dtype=np.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.indices.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i = int(self.indices[idx])\n",
    "        x = torch.from_numpy(self.X[i])\n",
    "        pos = torch.from_numpy(self.pos[i])\n",
    "        neg = torch.from_numpy(self.neg[i])\n",
    "        return x, pos, neg\n",
    "\n",
    "class UnlabeledDataset(Dataset):\n",
    "    def __init__(self, X, indices):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.indices = np.array(indices, dtype=np.int64)\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    def __getitem__(self, idx):\n",
    "        i = int(self.indices[idx])\n",
    "        return torch.from_numpy(self.X[i]), i\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 4) Bilinear classifier\n",
    "#    doc_emb: [B, d_doc]\n",
    "#    label_emb: [C, d_lab]  (미리 GAT로 만든 거)\n",
    "#    점수: doc @ W @ label_emb^T\n",
    "# -------------------------------------------------\n",
    "class BilinearHierClassifier(nn.Module):\n",
    "    def __init__(self, doc_dim, label_emb, hidden_dim=None):\n",
    "        super().__init__()\n",
    "        # label_emb는 파라미터로 들고있되, 업데이트 안 한다고 가정(원하면 nn.Parameter로)\n",
    "        self.register_buffer(\"label_emb\", torch.tensor(label_emb, dtype=torch.float32))\n",
    "        C, d_lab = self.label_emb.shape\n",
    "        self.doc_dim = doc_dim\n",
    "        self.label_dim = d_lab\n",
    "\n",
    "        if hidden_dim is None:\n",
    "            # 바로 doc_dim -> label_dim\n",
    "            self.interaction = nn.Linear(doc_dim, d_lab, bias=False)\n",
    "            self.proj = None\n",
    "        else:\n",
    "            # doc_dim -> hidden -> label_dim 같은 것도 가능\n",
    "            self.interaction = nn.Sequential(\n",
    "                nn.Linear(doc_dim, hidden_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(hidden_dim, d_lab, bias=False),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [B, d_doc]\n",
    "        return: logits [B, C]\n",
    "        \"\"\"\n",
    "        # x -> same dim as label\n",
    "        h = self.interaction(x)                             # [B, d_lab]\n",
    "        # [B, d_lab] @ [d_lab, C] -> [B, C]\n",
    "        logits = torch.matmul(h, self.label_emb.t())\n",
    "        return logits\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 5) loss: 계층 마스크를 씌운 BCE\n",
    "# -------------------------------------------------\n",
    "def hierarchical_bce_loss(logits, pos_mask, neg_mask):\n",
    "    # logits: [B, C]\n",
    "    # pos_mask, neg_mask: [B, C]\n",
    "    loss_pos = -(pos_mask * F.logsigmoid(logits)).sum()\n",
    "    loss_neg = -(neg_mask * F.logsigmoid(-logits)).sum()\n",
    "    denom = (pos_mask.sum() + neg_mask.sum()).clamp(min=1.0)\n",
    "    return (loss_pos + loss_neg) / denom\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 6) 학습 루프 예시\n",
    "# -------------------------------------------------\n",
    "# 이미 있는 것들: X (문서 BERT 임베딩) : [N_docs, d_doc]\n",
    "#                  L (라벨 GAT 임베딩)  : [C, d_lab]\n",
    "#                  B_adj (부모->자식)   : [C, C]\n",
    "#                  silver (list[list[int]]) : 문서별 core label index\n",
    "def train_epoch_hier(model, loader, opt, device):\n",
    "    model.train()\n",
    "    total = 0.0\n",
    "    for xb, posb, negb in loader:\n",
    "        xb = xb.to(device)\n",
    "        posb = posb.to(device)\n",
    "        negb = negb.to(device)\n",
    "        logits = model(xb)\n",
    "        loss = hierarchical_bce_loss(logits, posb, negb)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        total += loss.item() * xb.size(0)\n",
    "    return total / len(loader.dataset)\n",
    "\n",
    "# 1) micro F1 계산\n",
    "def micro_f1_from_logits(logits, pos_mask, thr=0.5, eps=1e-9):\n",
    "    \"\"\"\n",
    "    logits: [B, C]\n",
    "    pos_mask: [B, C]  (1: positive, 0: else)\n",
    "    \"\"\"\n",
    "    probs = torch.sigmoid(logits)\n",
    "    preds = (probs >= thr).float()\n",
    "\n",
    "    y_true = pos_mask\n",
    "    y_pred = preds\n",
    "\n",
    "    tp = (y_true * y_pred).sum()\n",
    "    fp = ((1 - y_true) * y_pred).sum()\n",
    "    fn = (y_true * (1 - y_pred)).sum()\n",
    "\n",
    "    precision = tp / (tp + fp + eps)\n",
    "    recall    = tp / (tp + fn + eps)\n",
    "    f1 = 2 * precision * recall / (precision + recall + eps)\n",
    "    return f1.item()\n",
    "\n",
    "# 2) eval 함수 수정: loss + f1 둘 다\n",
    "def eval_epoch_hier(model, loader, device, k=3, thr=None):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    f1_list = []\n",
    "    with torch.no_grad():\n",
    "        for xb, posb, negb in loader:\n",
    "            xb = xb.to(device)\n",
    "            posb = posb.to(device)\n",
    "            negb = negb.to(device)\n",
    "\n",
    "            logits = model(xb)\n",
    "            loss = hierarchical_bce_loss(logits, posb, negb)  # 위에 바꾼 버전\n",
    "            total_loss += loss.item() * xb.size(0)\n",
    "\n",
    "            probs = torch.sigmoid(logits)\n",
    "\n",
    "            if thr is not None:\n",
    "                pred = (probs >= thr).float()\n",
    "            else:\n",
    "                # top-k 방식\n",
    "                B, C = probs.shape\n",
    "                pred = torch.zeros_like(probs)\n",
    "                topk = probs.topk(k, dim=1).indices\n",
    "                pred.scatter_(1, topk, 1.0)\n",
    "\n",
    "            # micro-f1\n",
    "            y_true = posb\n",
    "            y_pred = pred\n",
    "            tp = (y_true * y_pred).sum().item()\n",
    "            fp = ((1 - y_true) * y_pred).sum().item()\n",
    "            fn = (y_true * (1 - y_pred)).sum().item()\n",
    "            prec = tp / (tp + fp + 1e-9)\n",
    "            rec  = tp / (tp + fn + 1e-9)\n",
    "            f1   = 2 * prec * rec / (prec + rec + 1e-9)\n",
    "            f1_list.append(f1)\n",
    "\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    avg_f1 = float(np.mean(f1_list)) if f1_list else 0.0\n",
    "    return avg_loss, avg_f1\n",
    "def pseudo_label_and_grow_hier(\n",
    "    model,\n",
    "    unl_ds,             # UnlabeledDataset\n",
    "    X_all,              # 전체 문서 임베딩 (numpy)\n",
    "    parents, children,\n",
    "    num_labels,\n",
    "    device,\n",
    "    pseudo_threshold=0.6,\n",
    "    pseudo_topk=3,\n",
    "    batch_size=512,\n",
    "):\n",
    "    if len(unl_ds) == 0:\n",
    "        return [], None, None\n",
    "\n",
    "    loader = DataLoader(unl_ds, batch_size=batch_size, shuffle=False)\n",
    "    model.eval()\n",
    "    new_idx = []\n",
    "    new_pos_list = []\n",
    "    new_neg_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, idxs in loader:\n",
    "            xb = xb.to(device)\n",
    "            logits = model(xb)\n",
    "            prob = torch.sigmoid(logits).cpu().numpy()\n",
    "\n",
    "            for p, i_doc in zip(prob, idxs.numpy().tolist()):\n",
    "                order = np.argsort(-p)\n",
    "                top1 = p[order[0]]\n",
    "                # 1) top-1이 threshold를 못 넘으면 그냥 버린다\n",
    "                if top1 < pseudo_threshold:\n",
    "                    continue\n",
    "                core = [j for j in order if p[j] >= pseudo_threshold][:pseudo_topk]\n",
    "                if len(core) == 0:\n",
    "                    # 아예 이 문서는 이번 epoch에 안 넣음\n",
    "                    continue\n",
    "\n",
    "                # 계층 pos/neg 구성\n",
    "                pos = set(core)\n",
    "                for c in core:\n",
    "                    for pa in parents[c]:\n",
    "                        pos.add(int(pa))\n",
    "                child = set()\n",
    "                for c in core:\n",
    "                    for ch in children[c]:\n",
    "                        child.add(int(ch))\n",
    "\n",
    "                pos_mask = np.zeros(num_labels, dtype=np.float32)\n",
    "                neg_mask = np.zeros(num_labels, dtype=np.float32)\n",
    "                for j in pos:\n",
    "                    pos_mask[j] = 1.0\n",
    "                for j in range(num_labels):\n",
    "                    if j in pos:    # 이미 양성\n",
    "                        continue\n",
    "                    if j in child:  # 모르겠음 → negative에서 제외\n",
    "                        continue\n",
    "                    neg_mask[j] = 1.0\n",
    "\n",
    "                new_idx.append(int(i_doc))\n",
    "                new_pos_list.append(pos_mask)\n",
    "                new_neg_list.append(neg_mask)\n",
    "\n",
    "\n",
    "\n",
    "    if len(new_idx) == 0:\n",
    "        return [], None, None\n",
    "\n",
    "    new_pos = np.stack(new_pos_list, axis=0)\n",
    "    new_neg = np.stack(new_neg_list, axis=0)\n",
    "    return new_idx, new_pos, new_neg\n",
    "\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "has_silver = np.array([len(lbls) > 0 for lbls in silver], dtype=bool)\n",
    "N_docs = X.shape[0]\n",
    "C = L.shape[0]\n",
    "\n",
    "# silver 있는 문서 / 없는 문서\n",
    "has_silver = np.array([len(lbls) > 0 for lbls in silver], dtype=bool)\n",
    "idx_silver = np.flatnonzero(has_silver)      # 여기가 train/val 후보\n",
    "idx_unl    = np.flatnonzero(~has_silver)     # 진짜 unl\n",
    "\n",
    "print(\"total:\", N_docs)\n",
    "print(\"with silver:\", len(idx_silver))\n",
    "print(\"unlabeled :\", len(idx_unl))\n",
    "\n",
    "# 이제 train/val은 silver 있는 애들만 섞어서 나눈다\n",
    "rng = np.random.default_rng(42)\n",
    "rng.shuffle(idx_silver)\n",
    "n_val = int(len(idx_silver) * 0.2)\n",
    "idx_val   = idx_silver[:n_val]\n",
    "idx_train = idx_silver[n_val:]\n",
    "\n",
    "# parents, children 만들기\n",
    "def build_parents_children(adj):\n",
    "    C = adj.shape[0]\n",
    "    parents = [np.flatnonzero(adj[:, j]).astype(np.int64) for j in range(C)]\n",
    "    children = [np.flatnonzero(adj[j]).astype(np.int64) for j in range(C)]\n",
    "    return parents, children\n",
    "\n",
    "parents, children = build_parents_children(B)\n",
    "\n",
    "pos_masks = np.zeros((N_docs, C), dtype=np.float32)\n",
    "neg_masks = np.zeros((N_docs, C), dtype=np.float32)\n",
    "\n",
    "for i in idx_silver:  # silver 있는 애만 돈다\n",
    "    core = silver[i]\n",
    "\n",
    "    # 1) core + parents\n",
    "    pos = set(core)\n",
    "    for c in core:\n",
    "        for p in parents[c]:\n",
    "            pos.add(int(p))\n",
    "\n",
    "    # 2) children은 모름\n",
    "    child = set()\n",
    "    for c in core:\n",
    "        for ch in children[c]:\n",
    "            child.add(int(ch))\n",
    "\n",
    "    for p in pos:\n",
    "        pos_masks[i, p] = 1.0\n",
    "\n",
    "    for j in range(C):\n",
    "        if j in pos:      # 이미 양성\n",
    "            continue\n",
    "        if j in child:    # 모름\n",
    "            continue\n",
    "        neg_masks[i, j] = 1.0\n",
    "\n",
    "\n",
    "\n",
    "train_ds = HierMultiLabelDataset(X, pos_masks, neg_masks, indices=idx_train)\n",
    "val_ds   = HierMultiLabelDataset(X, pos_masks, neg_masks, indices=idx_val) if len(idx_val) > 0 else None\n",
    "unl_ds   = UnlabeledDataset(X, idx_unl.tolist())\n",
    "print(len(train_ds),len(val_ds),len(unl_ds))\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=256, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=256, shuffle=False)\n",
    "\n",
    "model = BilinearHierClassifier(doc_dim=X.shape[1], label_emb=L, hidden_dim=256).to(device)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "epochs = 150\n",
    "\n",
    "N_labels = L.shape[0]\n",
    "best_f1 = -1.0\n",
    "patience = 15\n",
    "no_improve = 0\n",
    "warmup_self = 1   # 1 epoch은 self-training 안 하게 해서 한 번 안정화\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    # train\n",
    "    tr_loss = train_epoch_hier(model, train_loader, opt, device)\n",
    "\n",
    "    # val: f1 기준\n",
    "    if val_loader is not None and len(val_ds) > 0:\n",
    "        va_loss, va_f1 = eval_epoch_hier(model, val_loader, device, k=3)\n",
    "        print(f\"Epoch {epoch:03d} | train_loss={tr_loss:.3f}  val_loss={va_loss:.3f}  val_f1={va_f1:.3f}\")\n",
    "\n",
    "        # early stopping을 f1로\n",
    "        if va_f1 > best_f1 + 1e-6:\n",
    "            best_f1 = va_f1\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch} (best f1={best_f1:.4f})\")\n",
    "                break\n",
    "    else:\n",
    "        print(f\"Epoch {epoch:03d} | train_loss={tr_loss:.3f}\")\n",
    "\n",
    "    # self-training: 1에폭에 전부 들어가는 거 방지용으로 warmup 넣음\n",
    "    if epoch <= warmup_self:\n",
    "        print(\"  + (skip pseudo-labeling on warmup epoch)\")\n",
    "        continue\n",
    "\n",
    "    new_idx, new_pos, new_neg = pseudo_label_and_grow_hier(\n",
    "        model,\n",
    "        unl_ds,\n",
    "        X,\n",
    "        parents,\n",
    "        children,\n",
    "        C,                   # num_labels\n",
    "        device=device,\n",
    "        pseudo_threshold=0.60,\n",
    "        pseudo_topk=3,\n",
    "        batch_size=512,\n",
    "    )\n",
    "\n",
    "    if len(new_idx) > 0:\n",
    "        # 전역 마스크 갱신\n",
    "        pos_masks[new_idx] = new_pos\n",
    "        neg_masks[new_idx] = new_neg\n",
    "\n",
    "        # unl에서 제거\n",
    "        keep_mask = ~np.isin(unl_ds.indices, np.array(new_idx, dtype=np.int64))\n",
    "        unl_ds.indices = unl_ds.indices[keep_mask]\n",
    "\n",
    "        # train에 추가\n",
    "        train_ds.indices = np.concatenate([train_ds.indices, np.array(new_idx, dtype=np.int64)])\n",
    "        train_loader = DataLoader(train_ds, batch_size=256, shuffle=True, drop_last=False)\n",
    "\n",
    "        print(f\"  + Added {len(new_idx)} pseudo-labeled docs (unl pool → {len(unl_ds)} left)\")\n",
    "    else:\n",
    "        print(\"  + No pseudo-labeled docs added this epoch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec157776-e172-4c16-93a1-aacdf0a962d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd03ade-045a-4c94-89e4-de3d1e523595",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b88df0-bac0-49b4-9f58-d0c5e77583e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c55049ed-898c-4b80-aba2-d83ca27f82bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T02:05:06.916072Z",
     "iopub.status.busy": "2025-11-11T02:05:06.915804Z",
     "iopub.status.idle": "2025-11-11T02:07:20.506718Z",
     "shell.execute_reply": "2025-11-11T02:07:20.505887Z",
     "shell.execute_reply.started": "2025-11-11T02:05:06.916052Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] saved document embeddings → Amazon_products/test_doc_embeddings.csv  shape=(19658, 769)\n"
     ]
    }
   ],
   "source": [
    "# Minimal submission generator: pick 2–3 labels per sample via hierarchical beam scoring\n",
    "import csv, os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ------------ Paths (edit if needed) ------------\n",
    "TEST_CORPUS = \"Amazon_products/test/test_corpus.txt\"   # lines: pid \\t text\n",
    "DOC_CSV     = \"Amazon_products/test_doc_embeddings.csv\"  # first col: id (pid), rest: feat000..feat127\n",
    "LABEL_CSV   = \"Amazon_products/label_embeddings_gat.csv\" # first col: id (== node index 0..N-1)\n",
    "OUT_PATH    = \"submission.csv\"\n",
    "DOC_CSV = \"Amazon_products/test_doc_embeddings.csv\" # first col: id (pid), rest: feat000..feat127\n",
    "build_and_save_document_embeddings(\n",
    "    corpus_path=TEST_CORPUS,\n",
    "    model_name=\"bert-base-uncased\",\n",
    "    alpha=0.7,\n",
    "    max_length=256,    # 문서 길이에 맞게 조절\n",
    "    batch_size=32,\n",
    "    out_csv=DOC_CSV,\n",
    "    pad_width=2,\n",
    ")\n",
    "# ------------ Hyperparams ------------\n",
    "MIN_LABS  = 2\n",
    "MAX_LABS  = 3\n",
    "BATCH = 1024\n",
    "\n",
    "# load test pids\n",
    "pids = []\n",
    "with open(TEST_CORPUS, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        parts = line.rstrip(\"\\n\").split(\"\\t\", 1)\n",
    "        if len(parts) == 2:\n",
    "            pids.append(parts[0])\n",
    "\n",
    "# load doc embeddings (map pid->vec)\n",
    "df_doc = pd.read_csv(DOC_CSV)\n",
    "doc_ids = df_doc.iloc[:,0].astype(str).tolist()\n",
    "D = df_doc.iloc[:,1:].to_numpy(dtype=np.float32)\n",
    "D = l2_normalize(D)\n",
    "pid2idx = {pid: i for i, pid in enumerate(doc_ids)}\n",
    "\n",
    "# load label embeddings (ensure order aligns with adjacency rows)\n",
    "df_lab = pd.read_csv(LABEL_CSV)\n",
    "lab_ids = df_lab.iloc[:,0].astype(int).to_numpy()\n",
    "L = df_lab.iloc[:,1:].to_numpy(dtype=np.float32)\n",
    "ord = np.argsort(lab_ids)\n",
    "lab_ids = lab_ids[ord]\n",
    "L = L[ord]\n",
    "\n",
    "# precompute children lists\n",
    "children = [np.flatnonzero(A[i]) for i in range(A.shape[0])]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea357b8-0f1b-4069-bbd2-8e56029e65b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3302f3f7-2fb6-4ed9-ba2e-4714cbc76dd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574d819b-4455-4b10-8541-4a9a9f2ddec4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7411b961-6f29-4ac7-ba7a-97098db48b1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T02:09:39.814060Z",
     "iopub.status.busy": "2025-11-11T02:09:39.813930Z",
     "iopub.status.idle": "2025-11-11T02:09:41.275456Z",
     "shell.execute_reply": "2025-11-11T02:09:41.274942Z",
     "shell.execute_reply.started": "2025-11-11T02:09:39.814046Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv, os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ------------ Paths (edit if needed) ------------\n",
    "TEST_CORPUS = \"Amazon_products/test/test_corpus.txt\"   # lines: pid \\t text\n",
    "DOC_CSV     = \"Amazon_products/test_doc_embeddings.csv\"  # first col: id (pid), rest: feat000..feat127\n",
    "LABEL_CSV   = \"Amazon_products/label_embeddings_gat.csv\" # first col: id (== node index 0..N-1)\n",
    "OUT_PATH    = \"submission.csv\"\n",
    "DOC_CSV = \"Amazon_products/test_doc_embeddings.csv\" # first col: id (pid), rest: feat000..feat127\n",
    "# ------------ Hyperparams ------------\n",
    "MIN_LABS  = 2\n",
    "MAX_LABS  = 3\n",
    "BATCH = 1024\n",
    "\n",
    "# load test pids\n",
    "pids = []\n",
    "with open(TEST_CORPUS, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        parts = line.rstrip(\"\\n\").split(\"\\t\", 1)\n",
    "        if len(parts) == 2:\n",
    "            pids.append(parts[0])\n",
    "\n",
    "# load doc embeddings (map pid->vec)\n",
    "df_doc = pd.read_csv(DOC_CSV)\n",
    "doc_ids = df_doc.iloc[:,0].astype(str).tolist()\n",
    "D = df_doc.iloc[:,1:].to_numpy(dtype=np.float32)\n",
    "pid2idx = {pid: i for i, pid in enumerate(doc_ids)}\n",
    "\n",
    "# load label embeddings (ensure order aligns with adjacency rows)\n",
    "df_lab = pd.read_csv(LABEL_CSV)\n",
    "lab_ids = df_lab.iloc[:,0].astype(int).to_numpy()\n",
    "L = df_lab.iloc[:,1:].to_numpy(dtype=np.float32)\n",
    "ord = np.argsort(lab_ids)\n",
    "lab_ids = lab_ids[ord]\n",
    "L = L[ord]\n",
    "\n",
    "# precompute children lists\n",
    "children = [np.flatnonzero(A[i]) for i in range(A.shape[0])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ef344a9-c996-4932-b7ab-9f7c5e7578b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T02:09:41.276361Z",
     "iopub.status.busy": "2025-11-11T02:09:41.276217Z",
     "iopub.status.idle": "2025-11-11T02:09:56.187596Z",
     "shell.execute_reply": "2025-11-11T02:09:56.187118Z",
     "shell.execute_reply.started": "2025-11-11T02:09:41.276347Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: submission.csv | samples=19658 | min-max labels per sample=2-3 | missing_pids=0\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "def ancestors_of(node, adj):\n",
    "    # adj[parent, child] = 1 가정\n",
    "    parents = np.flatnonzero(adj[:, node])  # (N,)\n",
    "    return parents.tolist()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "IN_DIM = D.shape[1]   # ❗️여기 D로\n",
    "missing = 0  # 없는 pid 카운트\n",
    "\n",
    "with open(OUT_PATH, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow([\"id\", \"label\"])\n",
    "\n",
    "    buf_x, buf_pid = [], []\n",
    "\n",
    "    def flush():\n",
    "        if not buf_x:\n",
    "            return\n",
    "        xb = torch.from_numpy(np.stack(buf_x, axis=0).astype(np.float32)).to(device)\n",
    "        with torch.inference_mode():\n",
    "            prob = torch.sigmoid(model(xb)).detach().cpu().numpy()\n",
    "        prob = np.nan_to_num(prob, nan=-1.0, posinf=1.0, neginf=0.0)\n",
    "    \n",
    "        for pid, p in zip(buf_pid, prob):\n",
    "            order = np.argsort(-p)\n",
    "    \n",
    "            # 1) 기본 후보 뽑기\n",
    "            thr_keep = [i for i in order if p[i] >= 0.5][:MAX_LABS]\n",
    "            if len(thr_keep) >= MIN_LABS:\n",
    "                keep = thr_keep[:MAX_LABS]\n",
    "            else:\n",
    "                keep = order[:max(MIN_LABS, len(thr_keep))]\n",
    "                if len(keep) < MIN_LABS:\n",
    "                    keep = order[:MIN_LABS]\n",
    "    \n",
    "            # 2) 부모 후보 모으기 (keep에는 아직 안 들어간 것만)\n",
    "            parent_cands = []\n",
    "            for c in keep:\n",
    "                pars = ancestors_of(c, B)\n",
    "                for pa in pars:\n",
    "                    if pa not in keep and pa not in parent_cands:\n",
    "                        parent_cands.append(pa)\n",
    "    \n",
    "            # 부모 후보를 확률 순으로 정렬 (있으면 더 높은 부모부터)\n",
    "            parent_cands.sort(key=lambda idx: p[idx], reverse=True)\n",
    "    \n",
    "            # 3) 남는 슬롯만큼 부모 채우기\n",
    "            final_idxs = list(keep)\n",
    "            for pa in parent_cands:\n",
    "                if len(final_idxs) >= MAX_LABS:\n",
    "                    break\n",
    "                final_idxs.append(pa)\n",
    "    \n",
    "            # 4) 혹시라도 최소 개수 못 채웠으면 order에서 채우기\n",
    "            if len(final_idxs) < MIN_LABS:\n",
    "                for idx in order:\n",
    "                    if idx not in final_idxs:\n",
    "                        final_idxs.append(idx)\n",
    "                    if len(final_idxs) >= MIN_LABS:\n",
    "                        break\n",
    "    \n",
    "            labels = sorted(int(lab_ids[i]) for i in final_idxs)\n",
    "            w.writerow([pid, \",\".join(map(str, labels))])\n",
    "    \n",
    "        buf_x.clear()\n",
    "        buf_pid.clear()\n",
    "\n",
    "    for pid in pids:\n",
    "        j = pid2idx.get(pid, None)\n",
    "        if j is None:\n",
    "            x = np.zeros(IN_DIM, dtype=np.float32)\n",
    "            missing += 1\n",
    "        else:\n",
    "            x = D[j]\n",
    "            if x.dtype != np.float32:\n",
    "                x = x.astype(np.float32, copy=False)\n",
    "        buf_x.append(x)\n",
    "        buf_pid.append(pid)\n",
    "        if len(buf_x) >= BATCH:\n",
    "            flush()\n",
    "    flush()\n",
    "\n",
    "print(f\"Saved: {OUT_PATH} | samples={len(pids)} | min-max labels per sample={MIN_LABS}-{MAX_LABS} | missing_pids={missing}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e293bbf-8597-4c2c-91e1-b03ce7477217",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b72c5b-1d7a-4d59-a975-e7fa22dfbb32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a677853-591b-488e-835f-cf4b4c164c1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343dfe98-d91d-4c90-9325-ec5a6ff5c560",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49cebb9-e667-46aa-8c96-64a63c801ab3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d535af4-4d65-4127-b625-caac88456b68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b2e45b-844d-437a-9fc5-1d47ad4a9832",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a133dad-b306-4349-b9d3-073022f3eb43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba53595b-f997-468a-a4f9-6deee2dc1a08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb8152f-64ca-44fc-a37b-4ab46e4291d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7419c2a8-6739-4ed6-b9ee-3eeb108873de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-10T13:08:37.137152Z",
     "iopub.status.busy": "2025-11-10T13:08:37.136915Z",
     "iopub.status.idle": "2025-11-10T13:08:37.225118Z",
     "shell.execute_reply": "2025-11-10T13:08:37.224606Z",
     "shell.execute_reply.started": "2025-11-10T13:08:37.137137Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating dummy predictions: 100%|██████████| 19658/19658 [00:00<00:00, 419908.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy submission file saved to: submission.csv\n",
      "Total samples: 19658, Classes per sample: 1-3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# ------------------------\n",
    "# Dummy baseline for Kaggle submission\n",
    "# Generates random multi-label predictions\n",
    "# ------------------------\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Paths ---\n",
    "TEST_DIR = \"Amazon_products/test\"  # modify if needed\n",
    "TEST_CORPUS_PATH = os.path.join(TEST_DIR, \"test_corpus.txt\")  # product_id \\t text\n",
    "SUBMISSION_PATH = \"submission.csv\"  # output file\n",
    "\n",
    "# --- Constants ---\n",
    "NUM_CLASSES = 531  # total number of classes (0–530)\n",
    "MIN_LABELS = 1     # minimum number of labels per sample\n",
    "MAX_LABELS = 3     # maximum number of labels per sample\n",
    "\n",
    "# --- Load test corpus ---\n",
    "def load_corpus(path):\n",
    "    \"\"\"Load test corpus into {pid: text} dictionary.\"\"\"\n",
    "\"\"\"\n",
    "    pid2text = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\", 1)\n",
    "            if len(parts) == 2:\n",
    "                pid, text = parts\n",
    "                pid2text[pid] = text\n",
    "    return pid2text\n",
    "\n",
    "pid2text_test = load_corpus(TEST_CORPUS_PATH)\n",
    "pid_list_test = list(pid2text_test.keys())\n",
    "\n",
    "# --- Generate random predictions ---\n",
    "all_pids, all_labels = [], []\n",
    "for pid in tqdm(pid_list_test, desc=\"Generating dummy predictions\"):\n",
    "    n_labels = random.randint(MIN_LABELS, MAX_LABELS)\n",
    "    labels = random.sample(range(NUM_CLASSES), n_labels)\n",
    "    labels = sorted(labels)\n",
    "    all_pids.append(pid)\n",
    "    all_labels.append(labels)\n",
    "\n",
    "# --- Save submission file ---\n",
    "with open(SUBMISSION_PATH, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"id\", \"label\"])\n",
    "    for pid, labels in zip(all_pids, all_labels):\n",
    "        writer.writerow([pid, \",\".join(map(str, labels))])\n",
    "\n",
    "print(f\"Dummy submission file saved to: {SUBMISSION_PATH}\")\n",
    "print(f\"Total samples: {len(all_pids)}, Classes per sample: {MIN_LABELS}-{MAX_LABELS}\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5d29a7-c5b4-4a26-954b-b65a737f632c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
