{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b2ad09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Colab ë“±ì—ì„œ í•œ ë²ˆë§Œ)\n",
    "# !pip install networkx openai tqdm\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from typing import List, Dict\n",
    "\n",
    "import networkx as nx\n",
    "from tqdm.auto import tqdm\n",
    "from openai import OpenAI\n",
    "import openai  # ì˜ˆì™¸ íƒ€ì…ìš© (APITimeoutError, RateLimitError)\n",
    "\n",
    "# ğŸ”‘ OpenAI í´ë¼ì´ì–¸íŠ¸ (í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEY ì‚¬ìš©)\n",
    "client = OpenAI(api_key = \"your key\")\n",
    "DATA_DIR = \"Amazon_products\"\n",
    "\n",
    "# ====== ì„¤ì •ê°’ ======\n",
    "DOCS_PER_PATH = 5                  # pathë‹¹ ìƒì„±í•  ë¬¸ì„œ ìˆ˜ (ìµœëŒ€ n_docs)\n",
    "\n",
    "MAX_CALLS = 1000   # ì „ì²´ API í˜¸ì¶œ ì˜ˆì‚°\n",
    "call_count = 0     # ì „ì—­ ì¹´ìš´í„°\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e963fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_taxonomy(taxonomy_file: str, id2label: Dict[str, str]) -> nx.DiGraph:\n",
    "    \"\"\"\n",
    "    label_hierarchy.txt ë¥¼ ì½ì–´ì„œ root ê°€ ì¶”ê°€ëœ DiGraph ë¥¼ ë§Œë“ ë‹¤.\n",
    "    íŒŒì¼ í¬ë§·: parent_id \\t child_id\n",
    "    \"\"\"\n",
    "    graph = nx.DiGraph()\n",
    "    with open(taxonomy_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line == \"\\n\":\n",
    "                continue\n",
    "            p_id, c_id = line.strip().split(\"\\t\")\n",
    "            source_node = id2label[p_id]\n",
    "            target_node = id2label[c_id]\n",
    "\n",
    "            graph.add_node(source_node)\n",
    "            graph.add_node(target_node)\n",
    "            graph.add_edge(source_node, target_node)\n",
    "\n",
    "    # ë¶€ëª¨ ì—†ëŠ” ë…¸ë“œë“¤ì— ëŒ€í•´ root ì¶”ê°€\n",
    "    no_parents = [node for node in graph.nodes if graph.in_degree(node) == 0]\n",
    "    if len(no_parents) != 0:\n",
    "        graph.add_node(\"root\")\n",
    "        for np in no_parents:\n",
    "            graph.add_edge(\"root\", np)\n",
    "\n",
    "    return graph\n",
    "\n",
    "\n",
    "def get_all_paths(graph: nx.DiGraph, start_node: str = \"root\") -> List[str]:\n",
    "    \"\"\"\n",
    "    root -> leaf ëª¨ë“  ê²½ë¡œë¥¼ \"\\\\\" êµ¬ë¶„ìë¡œ ì´ì–´ë¶™ì¸ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜\n",
    "    ì˜ˆ: \"Electronics\\\\Camera\"\n",
    "    \"\"\"\n",
    "    leaf_nodes = [node for node in graph.nodes() if graph.out_degree(node) == 0]\n",
    "\n",
    "    all_paths = []\n",
    "    for leaf_node in leaf_nodes:\n",
    "        paths = list(nx.all_simple_paths(graph, source=start_node, target=leaf_node))\n",
    "        for path in paths:\n",
    "            # \"root\"ëŠ” ë¬¸ìì—´ì—ì„œ ì œê±°\n",
    "            if \"root\" in path:\n",
    "                path.pop(path.index(\"root\"))\n",
    "            all_paths.append(\"\\\\\".join(path))\n",
    "    return all_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569acba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json_list(content: str):\n",
    "    \"\"\"\n",
    "    ëª¨ë¸ì´ ì¤€ contentì—ì„œ [ ... ] ë¡œ ë³´ì´ëŠ” JSON array ë¶€ë¶„ë§Œ ë½‘ì•„ì„œ json.loads í•˜ëŠ” í•¨ìˆ˜.\n",
    "    \"\"\"\n",
    "    content = content.strip()\n",
    "\n",
    "    # 1) ì½”ë“œ ë¸”ë¡(``` ... ```) ì•ˆì— JSONì´ ë“¤ì–´ìˆëŠ” ê²½ìš° ë¨¼ì € ì²˜ë¦¬\n",
    "    if \"```\" in content:\n",
    "        parts = content.split(\"```\")\n",
    "        # parts ì¤‘ì—ì„œ [ ë¡œ ì‹œì‘í•˜ê³  ] ë¡œ ëë‚˜ëŠ” ë¶€ë¶„ì„ ì°¾ì•„ë³¸ë‹¤\n",
    "        for part in parts:\n",
    "            part = part.strip()\n",
    "            if part.startswith(\"[\") and part.endswith(\"]\"):\n",
    "                return json.loads(part)\n",
    "\n",
    "    # 2) ì½”ë“œ ë¸”ë¡ì´ ì•„ë‹ˆê³ , ê·¸ëƒ¥ ì•ë’¤ì— í…ìŠ¤íŠ¸ + [ ... ]ë§Œ ìˆëŠ” ê²½ìš°\n",
    "    start = content.find(\"[\")\n",
    "    end = content.rfind(\"]\")\n",
    "    if start != -1 and end != -1 and start < end:\n",
    "        snippet = content[start : end + 1]\n",
    "        return json.loads(snippet)\n",
    "\n",
    "    # ì—¬ê¸°ê¹Œì§€ ì•ˆ ê±¸ë¦¬ë©´ ê·¸ëƒ¥ ì‹¤íŒ¨\n",
    "    raise json.JSONDecodeError(\"Could not extract JSON array\", content, 0)\n",
    "\n",
    "def _build_messages_for_path(path: str, dataset: str):\n",
    "    \"\"\"\n",
    "    path: ì˜ˆ) 'Electronics\\\\Camera'\n",
    "    dataset: 'Amazon-531' ë˜ëŠ” 'DBPedia-298'\n",
    "    \"\"\"\n",
    "    backslash_char = \"\\\\\"\n",
    "    leaf = path.split(backslash_char)[-1]\n",
    "\n",
    "    system_prompt = (\n",
    "            \"You are an Amazon product reviewer. \"\n",
    "            \"You write detailed, realistic product reviews.\"\n",
    "        )\n",
    "    user_prompt = f\"\"\"\\\n",
    "Suppose you are an Amazon Reviewer, please generate 5 various and reliable passages following the requirements below:\n",
    "1. Must generate reviews following the themes of the taxonomy path: {path} but **without** explicitly mentioning \"{path}\".\n",
    "2. Each review must be longer than 100 words.\n",
    "3. The writing style and format of the text should be a product review.\n",
    "4. Keep the generated texts diverse, specific, and consistent with the given taxonomy path.\n",
    "You should focus on \"{leaf}\" as the main topic of the documents.\n",
    "\n",
    "Your output MUST be a valid JSON list of objects like:\n",
    "[{{\"doc\": \"text1\"}}, {{\"doc\": \"text2\"}}, ...]\n",
    "\"\"\"\n",
    "    min_len = 100\n",
    "\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "    return messages, min_len\n",
    "\n",
    "\n",
    "def generate_pseudo_docs_for_path(\n",
    "    path: str,\n",
    "    dataset: str,\n",
    "    n_docs: int = 5,\n",
    "    max_retries: int = 5,\n",
    "):\n",
    "    \"\"\"\n",
    "    í•˜ë‚˜ì˜ taxonomy path ì— ëŒ€í•´ pseudo documentsë¥¼ ìƒì„±í•´ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ë¦¬í„´.\n",
    "    - path: 'Electronics\\\\Camera' ê°™ì€ í˜•íƒœ\n",
    "    - dataset: 'Amazon-531' ë˜ëŠ” 'DBPedia-298'\n",
    "    - n_docs: ìµœì¢…ì ìœ¼ë¡œ ë°›ê³  ì‹¶ì€ ë¬¸ì„œ ê°œìˆ˜ (ìµœëŒ€)\n",
    "    \"\"\"\n",
    "    global call_count\n",
    "\n",
    "    docs = []\n",
    "    retries = 0\n",
    "\n",
    "    while len(docs) < n_docs and retries < max_retries:\n",
    "        if call_count >= MAX_CALLS:\n",
    "            print(\"[WARN] MAX_CALLS reached, stop generation.\")\n",
    "            break\n",
    "\n",
    "        messages, min_len = _build_messages_for_path(path, dataset)\n",
    "\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",  # í•„ìš”í•˜ë©´ ë‹¤ë¥¸ ëª¨ë¸ë¡œ ë³€ê²½\n",
    "                temperature=0.8,\n",
    "                max_tokens=3000,\n",
    "                messages=messages,\n",
    "            )\n",
    "            call_count += 1\n",
    "\n",
    "            content = response.choices[0].message.content.strip()\n",
    "\n",
    "            try:\n",
    "                results_json = extract_json_list(content)\n",
    "            except json.JSONDecodeError:\n",
    "                print(\"[WARN] JSON decode error, retrying...\")\n",
    "                retries += 1\n",
    "                continue\n",
    "\n",
    "            for item in results_json:\n",
    "                if \"doc\" not in item:\n",
    "                    continue\n",
    "                text = item[\"doc\"].strip().replace(\"\\n\", \" \")\n",
    "                if len(text) < min_len:\n",
    "                    continue\n",
    "                # ì›ë˜ ì½”ë“œì—ì„œ ê±¸ëŸ¬ì£¼ë˜ ì´ìƒ ì¼€ì´ìŠ¤\n",
    "                if \":\" in text or \"=\" in text:\n",
    "                    continue\n",
    "\n",
    "                docs.append(text)\n",
    "                if len(docs) >= n_docs:\n",
    "                    break\n",
    "\n",
    "        except openai.APITimeoutError:\n",
    "            print(\"[WARN] APITimeoutError, retrying...\")\n",
    "            retries += 1\n",
    "            continue\n",
    "        except openai.RateLimitError:\n",
    "            print(\"[WARN] RateLimitError, sleep 10s...\")\n",
    "            time.sleep(10)\n",
    "            retries += 1\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Unexpected error: {e}\")\n",
    "            retries += 1\n",
    "            continue\n",
    "\n",
    "        time.sleep(0.1)  # rate limit ì™„í™”ìš©\n",
    "\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80a1dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 1) ë¼ë²¨ íŒŒì¼ ì½ê¸° =====\n",
    "labels_path = os.path.join(DATA_DIR, \"classes.txt\")\n",
    "id2label = {}\n",
    "label2id = {}\n",
    "\n",
    "with open(labels_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f.read().split(\"\\n\"):\n",
    "        if line == \"\":\n",
    "            continue\n",
    "        id_str, label_name = line.split(\"\\t\")\n",
    "        id2label[id_str] = label_name\n",
    "        label2id[label_name] = id_str\n",
    "\n",
    "print(f\"# labels: {len(id2label)}\")\n",
    "\n",
    "\n",
    "# ===== 2) taxonomy & path ë¦¬ìŠ¤íŠ¸ êµ¬í•˜ê¸° =====\n",
    "taxo_path = os.path.join(DATA_DIR,\"class_hierarchy.txt\")\n",
    "taxo = read_taxonomy(taxo_path, id2label)\n",
    "paths = get_all_paths(taxo, start_node=\"root\")\n",
    "print(f\"# taxonomy paths (rootâ†’leaf): {len(paths)}\")\n",
    "\n",
    "\n",
    "# ===== 3) ì¶œë ¥ ê²½ë¡œ ì¤€ë¹„ =====\n",
    "out_dir = os.path.join(DATA_DIR, \"train\")\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "docs_txt_path = os.path.join(out_dir, \"generated_docs.txt\")\n",
    "json_path = os.path.join(out_dir, \"generated_doc2label.json\")\n",
    "\n",
    "id_docs = {}\n",
    "doc_id_labels = {}\n",
    "num_doc = 0\n",
    "\n",
    "# ===== 4) pathë³„ë¡œ pseudo docs ìƒì„± + íŒŒì¼ì— ì €ì¥ =====\n",
    "with open(docs_txt_path, \"w\", encoding=\"utf-8\") as doc_file:\n",
    "    for path in tqdm(paths, desc=\"Generating pseudo docs per path\"):\n",
    "        if call_count >= MAX_CALLS:\n",
    "            print(\"[INFO] Reached MAX_CALLS, stop generation loop.\")\n",
    "            break\n",
    "\n",
    "        path_list = path.split(\"\\\\\")\n",
    "        leaf_label = path_list[-1]\n",
    "\n",
    "        pseudo_docs = generate_pseudo_docs_for_path(\n",
    "            path=path,\n",
    "            dataset=DATASET,\n",
    "            n_docs=DOCS_PER_PATH,\n",
    "        )\n",
    "\n",
    "        for doc in pseudo_docs:\n",
    "            doc_id = str(num_doc)\n",
    "            id_docs[doc_id] = doc\n",
    "\n",
    "            local_doc_to_label = {\n",
    "                \"core_classes\": label2id[leaf_label],\n",
    "                \"with ancestors\": [label2id[label] for label in path_list],\n",
    "            }\n",
    "            doc_id_labels[doc_id] = local_doc_to_label\n",
    "\n",
    "            safe_text = doc.replace(\"\\n\", \" \")\n",
    "            doc_file.write(f\"{doc_id}\\t{safe_text}\\n\")\n",
    "\n",
    "            num_doc += 1\n",
    "\n",
    "# ===== 5) doc2label json ì €ì¥ =====\n",
    "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(doc_id_labels, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Total pseudo docs generated: {num_doc}\")\n",
    "print(f\"Docs file saved to: {docs_txt_path}\")\n",
    "print(f\"Doc2label json saved to: {json_path}\")\n",
    "print(f\"Total API calls used: {call_count}/{MAX_CALLS}\")\n",
    "#0.24 $\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
