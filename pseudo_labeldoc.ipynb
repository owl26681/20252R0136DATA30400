{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ea9cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: imports & config\n",
    "\n",
    "import os\n",
    "import json\n",
    "import copy\n",
    "import math\n",
    "import queue\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity as cos\n",
    "\n",
    "# -----------------------\n",
    "# 데이터 경로 설정 (여기만 네 상황에 맞게 수정)\n",
    "# -----------------------\n",
    "DATA_DIR = \"Amazon_products\"      # 예: \"/content/data\"\n",
    "DATASET = \"Amazon_products/train/train_corpus.txt\"          # document\n",
    "GPU = 0                         # cuda 디바이스 번호\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f1f20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Node 클래스 + 그래프 생성 함수\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, node_id, name):\n",
    "        self.node_id = str(node_id)       # \"0\", \"1\", ...\n",
    "        self.name = name                  # \"grocery_gourmet_food\"\n",
    "        self.childs = []                  # List[Node]\n",
    "        self.parents = []                 # List[Node]\n",
    "        self.path_score = 0.0\n",
    "        self.similarity_score = 0.0\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Node(id={self.node_id}, name={self.name})\"\n",
    "\n",
    "\n",
    "def build_graph_from_files(label_file, edge_file):\n",
    "    \"\"\"\n",
    "    label_file: \"id<tab>label_name\"\n",
    "    edge_file : \"parent_id<tab>child_id\"\n",
    "    \"\"\"\n",
    "    id2label = {}\n",
    "    label2id = {}\n",
    "    id2node = {}\n",
    "\n",
    "    # 1) 라벨 파일 읽기\n",
    "    with open(label_file, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            parts = line.split(\"\\t\")\n",
    "            if len(parts) != 2:\n",
    "                continue\n",
    "            idx, name = parts\n",
    "            idx = str(idx)\n",
    "            name = name.strip()\n",
    "            id2label[idx] = name\n",
    "            label2id[name] = idx\n",
    "            id2node[idx] = Node(idx, name)\n",
    "\n",
    "    # 2) 부모-자식 엣지 파일 읽기\n",
    "    with open(edge_file, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            parts = line.split(\"\\t\")\n",
    "            if len(parts) != 2:\n",
    "                continue\n",
    "            p, c = parts\n",
    "            p = str(p); c = str(c)\n",
    "            if p not in id2node or c not in id2node:\n",
    "                continue\n",
    "            parent = id2node[p]\n",
    "            child = id2node[c]\n",
    "            parent.childs.append(child)\n",
    "            child.parents.append(parent)\n",
    "\n",
    "    # 3) 부모가 없는 노드들 = top-level 루트들\n",
    "    roots = [n for n in id2node.values() if len(n.parents) == 0]\n",
    "\n",
    "    if len(roots) == 1:\n",
    "        root = roots[0]\n",
    "    else:\n",
    "        # top-level이 여러 개면 슈퍼루트 하나 만들어서 모두 연결\n",
    "        root = Node(\"-1\", \"ROOT\")\n",
    "        for r in roots:\n",
    "            root.childs.append(r)\n",
    "            r.parents.append(root)\n",
    "\n",
    "    print(f\"#labels: {len(id2label)}, #roots(before super-root): {len(roots)}\")\n",
    "    return root, id2label, label2id, id2node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f9a71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mmap\n",
    "def get_num_lines(file_path):\n",
    "    fp = open(file_path, \"r+\")\n",
    "    buf = mmap.mmap(fp.fileno(), 0)\n",
    "    lines = 0\n",
    "    while buf.readline():\n",
    "        lines += 1\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f7d497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: graph, corpus, embedding 준비\n",
    "\n",
    "# 1) 라벨 키워드 로드 (llm_enrichment.txt)\n",
    "enriched_file = os.path.join(DATA_DIR, \"class_related_keywords.txt\")\n",
    "label_keyterm_dict = {}\n",
    "\n",
    "with open(enriched_file, encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        components = line.split(\":\")\n",
    "        node = components[0]          # label_name (with underscore)\n",
    "        keywords = components[1]\n",
    "        keyword_list = [k for k in keywords.split(\",\") if k]\n",
    "        label_keyterm_dict[node] = keyword_list\n",
    "\n",
    "print(\"num labels with keywords:\", len(label_keyterm_dict))\n",
    "\n",
    "\n",
    "# 3) taxonomy 그래프 로드\n",
    "LABEL_FILE = os.path.join(DATA_DIR, \"classes.txt\")      # 예시 이름\n",
    "EDGE_FILE  = os.path.join(DATA_DIR, \"class_hierarchy.txt\") # 예시 이름\n",
    "\n",
    "root, id2label, label2id, id2node = build_graph_from_files(LABEL_FILE, EDGE_FILE)\n",
    "\n",
    "num_class = len(id2label)\n",
    "print(\"num_class:\", num_class)\n",
    "# 4) corpus.txt 로드: \"doc_id \\t text\"\n",
    "corpus_path = os.path.join(DATASET)\n",
    "num_line = get_num_lines(corpus_path)\n",
    "\n",
    "all_docs = []\n",
    "all_docs_id = []\n",
    "\n",
    "with open(corpus_path, encoding=\"utf-8\") as f:\n",
    "    for i, line in tqdm(enumerate(f), total=num_line):\n",
    "        line = line.rstrip(\"\\n\")\n",
    "        if not line:\n",
    "            continue\n",
    "        doc_id, doc = line.split(\"\\t\", 1)\n",
    "        all_docs.append(doc)\n",
    "        all_docs_id.append(doc_id)\n",
    "\n",
    "print(\"num_docs:\", len(all_docs))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fd55ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U openai pydantic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8c6c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"]=\"your key\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140bb3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai, sys, os\n",
    "print(\"python:\", sys.version)\n",
    "print(\"openai:\", openai.__version__)\n",
    "print(\"OPENAI_API_KEY set:\", bool(os.environ.get(\"OPENAI_API_KEY\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896700a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: adjacency matrix + parents/siblings\n",
    "import json\n",
    "import numpy as np\n",
    "adj_upper = np.zeros((num_class, num_class), dtype=np.int32)\n",
    "\n",
    "with open(EDGE_FILE, encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        p_str, c_str = line.split(\"\\t\")\n",
    "        p = int(p_str)\n",
    "        c = int(c_str)\n",
    "        adj_upper[p, c] = 1\n",
    "\n",
    "\n",
    "print(\"adj_upper shape:\", adj_upper.shape)\n",
    "\n",
    "# ----- 위에서 이미 정의했던 함수들을 그대로 사용 -----\n",
    "import numpy as np\n",
    "\n",
    "def build_parents_children(adj: np.ndarray):\n",
    "    C = adj.shape[0]\n",
    "    parents = [np.flatnonzero(adj[:, j]).astype(np.int64) for j in range(C)]\n",
    "    children = [np.flatnonzero(adj[j]).astype(np.int64) for j in range(C)]\n",
    "    return parents, children\n",
    "\n",
    "def build_siblings(parents, children):\n",
    "    C = len(parents)\n",
    "    sibs = [set() for _ in range(C)]\n",
    "    for c in range(C):\n",
    "        for p in parents[c]:\n",
    "            for ch in children[p]:\n",
    "                if ch != c:\n",
    "                    sibs[c].add(int(ch))\n",
    "    sibs = [np.array(sorted(s), dtype=np.int64) for s in sibs]\n",
    "    return sibs\n",
    "\n",
    "parents, children = build_parents_children(adj_upper)\n",
    "siblings = build_siblings(parents, children)\n",
    "roots = [i for i, ps in enumerate(parents) if len(ps) == 0]\n",
    "\n",
    "print(\"built parents & siblings\")\n",
    "print(roots)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3113e045",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, csv, time, random\n",
    "from datetime import datetime\n",
    "from typing import List, Optional, Dict, Any\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "# 디버깅 단계에선 1로 추천(문제 원인 좁히기)\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "# 규격 체크(원하면 꺼도 됨)\n",
    "CHECK_WORDS = True\n",
    "MIN_WORDS, MAX_WORDS = 80, 200\n",
    "CHECK_4_SENTENCES = True\n",
    "\n",
    "# 이번 실행에서 최대 호출 수(노트북이라 안전하게 작게)\n",
    "MAX_CALLS_THIS_RUN = 20\n",
    "\n",
    "# 저장 경로(실행마다 폴더 분리)\n",
    "RUN_ID = datetime.utcnow().strftime(\"%Y%m%dT%H%M%S\")\n",
    "RUN_DIR = f\"Amazon_products/run_{RUN_ID}\"\n",
    "os.makedirs(RUN_DIR, exist_ok=True)\n",
    "\n",
    "OUT_CSV = os.path.join(RUN_DIR, \"label_docs.csv\")            # label, doc, valid\n",
    "RAW_JSONL = os.path.join(RUN_DIR, \"raw_generations.jsonl\")   # GPT가 만든 원문/파싱 결과 전부\n",
    "ERR_JSONL = os.path.join(RUN_DIR, \"errors.jsonl\")            # 예외/실패 기록\n",
    "PROG_JSON = os.path.join(RUN_DIR, \"progress.json\")           # 요약 로그\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "progress = {\n",
    "    \"attempted_calls\": 0,\n",
    "    \"api_ok_calls\": 0,        # API 응답 자체 성공\n",
    "    \"parsed_ok_calls\": 0,     # docs 파싱까지 성공\n",
    "    \"valid_docs\": 0,          # 규격 통과 doc 개수\n",
    "    \"invalid_docs\": 0,        # 규격 실패 doc 개수\n",
    "    \"exceptions\": 0,\n",
    "}\n",
    "with open(PROG_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(progress, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"RUN_DIR:\", RUN_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5378a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아래 변수들이 이미 로드되어 있어야 함:\n",
    "# num_class, id2label, parents, children, label_keyterm_dict\n",
    "missing = []\n",
    "for name in [\"num_class\", \"id2label\", \"parents\", \"children\", \"label_keyterm_dict\"]:\n",
    "    if name not in globals():\n",
    "        missing.append(name)\n",
    "\n",
    "if missing:\n",
    "    raise RuntimeError(f\"Missing globals: {missing}. 먼저 taxonomy 데이터 로드 셀을 실행해줘.\")\n",
    "else:\n",
    "    print(\"OK. num_class =\", num_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72abacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "MAX_CHILDREN = 6\n",
    "MAX_SIBLINGS = 6\n",
    "\n",
    "def get_parent(cid: int) -> Optional[int]:\n",
    "    ps = parents[cid]\n",
    "    return int(ps[0]) if (ps is not None and len(ps) > 0) else None\n",
    "\n",
    "def get_children(cid: int) -> List[int]:\n",
    "    ch = children[cid]\n",
    "    return [int(x) for x in ch] if ch is not None else []\n",
    "\n",
    "def get_siblings(cid: int) -> List[int]:\n",
    "    p = get_parent(cid)\n",
    "    if p is None:\n",
    "        return []\n",
    "    return [x for x in get_children(p) if x != cid]\n",
    "\n",
    "def build_ctx(cid: int) -> dict:\n",
    "    lab = id2label[str(cid)]\n",
    "    p = get_parent(cid)\n",
    "    ch = get_children(cid)[:MAX_CHILDREN]\n",
    "    sib = get_siblings(cid)[:MAX_SIBLINGS]\n",
    "    kws = label_keyterm_dict.get(lab, [])[:10]\n",
    "    return {\n",
    "        \"cid\": cid,\n",
    "        \"label\": lab,\n",
    "        \"label_text\": lab.replace(\"_\", \" \"),\n",
    "        \"parent\": id2label[str(p)] if p is not None else None,\n",
    "        \"children\": [id2label[str(x)] for x in ch],\n",
    "        \"siblings\": [id2label[str(x)] for x in sib],\n",
    "        \"given_keywords\": kws,\n",
    "    }\n",
    "\n",
    "def normalize_doc(doc: str) -> str:\n",
    "    # single paragraph 만들기\n",
    "    return \" \".join(str(doc).split())\n",
    "\n",
    "def word_count(doc: str) -> int:\n",
    "    return len(doc.split())\n",
    "\n",
    "def sentence_count_rough(doc: str) -> int:\n",
    "    # 아주 러프하게 ., ?, ! 기준\n",
    "    s = re.split(r'(?<=[.!?])\\s+', doc.strip())\n",
    "    s = [x for x in s if x.strip()]\n",
    "    return len(s)\n",
    "\n",
    "def is_valid_doc(doc: str) -> (bool, dict):\n",
    "    doc2 = normalize_doc(doc)\n",
    "    wc = word_count(doc2)\n",
    "    sc = sentence_count_rough(doc2)\n",
    "    ok = True\n",
    "    reasons = []\n",
    "    if CHECK_WORDS and not (MIN_WORDS <= wc <= MAX_WORDS):\n",
    "        ok = False\n",
    "        reasons.append(f\"word_count={wc}\")\n",
    "    if CHECK_4_SENTENCES and sc != 5:\n",
    "        ok = False\n",
    "        reasons.append(f\"sentence_count={sc}\")\n",
    "    return ok, {\"word_count\": wc, \"sentence_count\": sc, \"reasons\": reasons}\n",
    "\n",
    "def build_request_payload(ctx_list: List[dict]) -> dict:\n",
    "    system = (\n",
    "        \"You write precise Amazon marketplace product-category descriptions for taxonomy labels.\\n\"\n",
    "        \"ENGLISH ONLY.\\n\"\n",
    "        \"Return JSON that matches the provided schema.\\n\"\n",
    "        \"You will receive an array 'labels' with contexts. Produce docs in the SAME ORDER.\\n\"\n",
    "        \"\\n\"\n",
    "        \"Core rule:\\n\"\n",
    "        \"- Write a standalone description of the label itself only.\\n\"\n",
    "        \"- Do NOT reference or mention parent, children, siblings, hierarchy, or any provided keywords.\\n\"\n",
    "        \"- Even if those fields are present in the input, ignore them completely.\\n\"\n",
    "        \"\\n\"\n",
    "        \"Content guidance:\\n\"\n",
    "        \"- Describe what products sold on Amazon belong in this category, including typical item types, common variants, and typical use cases.\\n\"\n",
    "        \"- Include practical details that matter in customer reviews, such as fit, sizing, compatibility, durability, performance, materials, comfort, ease of use, and value.\\n\"\n",
    "        \"- Define clear boundaries by stating what is included and what is excluded, without naming related categories or using comparisons to parent or sibling labels.\\n\"\n",
    "        \"\\n\"\n",
    "        \"Hard formatting constraints for each doc:\\n\"\n",
    "        \"- Exactly 5 sentences.\\n\"\n",
    "        \"- Single paragraph with no newline characters.\\n\"\n",
    "        \"- Plain text only: no markdown, no bullets, no numbering.\\n\"\n",
    "        \"- 120 to 170 words total.\\n\"\n",
    "        \"- Use exactly five period characters '.' total: one at the end of each sentence.\\n\"\n",
    "        \"- Do NOT use any other periods anywhere (no abbreviations, no initials, no decimals).\\n\"\n",
    "        \"- Do NOT use semicolons or colons.\\n\"\n",
    "    )\n",
    "\n",
    "    user_obj = {\"labels\": ctx_list}\n",
    "\n",
    "    # JSON Schema 방식(Responses API)\n",
    "    schema = {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"docs\": {\n",
    "                \"type\": \"array\",\n",
    "                \"items\": {\"type\": \"string\"},\n",
    "                \"minItems\": len(ctx_list),\n",
    "                \"maxItems\": len(ctx_list),\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"docs\"],\n",
    "        \"additionalProperties\": False,\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"model\": MODEL,\n",
    "        \"input\": [\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\", \"content\": json.dumps(user_obj, ensure_ascii=False)},\n",
    "        ],\n",
    "        \"max_output_tokens\": 7000,\n",
    "        \"text\": {\n",
    "            \"format\": {\n",
    "                \"type\": \"json_schema\",\n",
    "                \"name\": \"label_docs_v1\",\n",
    "                \"strict\": True,\n",
    "                \"schema\": schema,\n",
    "            }\n",
    "        },\n",
    "        \"store\": False,\n",
    "    }\n",
    "\n",
    "def response_to_dict(resp) -> Dict[str, Any]:\n",
    "    if hasattr(resp, \"model_dump\"):\n",
    "        return resp.model_dump()\n",
    "    if hasattr(resp, \"to_dict\"):\n",
    "        return resp.to_dict()\n",
    "    try:\n",
    "        return dict(resp)\n",
    "    except Exception:\n",
    "        return {\"_raw\": str(resp)}\n",
    "\n",
    "def extract_raw_text(resp) -> str:\n",
    "    # SDK 편의 필드(output_text) 우선 (Responses 가이드에 언급) :contentReference[oaicite:0]{index=0}\n",
    "    ot = getattr(resp, \"output_text\", None)\n",
    "    if isinstance(ot, str) and ot.strip():\n",
    "        return ot.strip()\n",
    "\n",
    "    body = response_to_dict(resp)\n",
    "    texts = []\n",
    "    for out in body.get(\"output\", []) or []:\n",
    "        for c in out.get(\"content\", []) or []:\n",
    "            t = c.get(\"text\")\n",
    "            if isinstance(t, str) and t.strip():\n",
    "                texts.append(t)\n",
    "    return \"\\n\".join(texts).strip()\n",
    "\n",
    "def parse_docs_from_response(resp) -> List[str]:\n",
    "    raw = extract_raw_text(resp)\n",
    "    # 혹시 뒤에 공백/개행이 붙어도 JSON이면 파싱 가능해야 함\n",
    "    raw2 = raw.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "    parsed = json.loads(raw2)\n",
    "    docs = parsed.get(\"docs\", None)\n",
    "    if not isinstance(docs, list):\n",
    "        raise RuntimeError(\"parsed JSON has no list field 'docs'\")\n",
    "    return docs, raw2\n",
    "\n",
    "def write_jsonl(path: str, obj: dict):\n",
    "    with open(path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def save_progress():\n",
    "    with open(PROG_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(progress, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def call_with_retry(payload: dict, max_retries: int = 5):\n",
    "    for attempt in range(max_retries + 1):\n",
    "        try:\n",
    "            return client.responses.create(**payload)  # Responses API :contentReference[oaicite:1]{index=1}\n",
    "        except Exception as e:\n",
    "            if attempt >= max_retries:\n",
    "                raise\n",
    "            # 지수 백오프\n",
    "            sleep = min(30.0, (2 ** attempt) * 1.0 + random.random() * 0.5)\n",
    "            time.sleep(sleep)\n",
    "\n",
    "from pydantic import create_model, conlist\n",
    "import openai, pydantic\n",
    "print(\"openai:\", openai.__version__)\n",
    "print(\"pydantic:\", pydantic.__version__)\n",
    "\n",
    "from pydantic import BaseModel\n",
    "try:\n",
    "    # pydantic v2\n",
    "    from pydantic import ConfigDict, conlist\n",
    "    PYDANTIC_V2 = True\n",
    "except Exception:\n",
    "    # pydantic v1\n",
    "    from pydantic import conlist\n",
    "    PYDANTIC_V2 = False\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "def ts_utc():\n",
    "    return datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "\n",
    "def make_docs_text_format(n_docs: int):\n",
    "    # 길이 고정(conlist) 제거: parse 자체는 되게 만들고,\n",
    "    # 길이/규격은 우리가 후처리로 체크/보정\n",
    "    if PYDANTIC_V2:\n",
    "        from pydantic import ConfigDict\n",
    "        class LabelDocs(BaseModel):\n",
    "            model_config = ConfigDict(extra=\"forbid\")\n",
    "            docs: List[str]\n",
    "        return LabelDocs\n",
    "    else:\n",
    "        class LabelDocs(BaseModel):\n",
    "            docs: List[str]\n",
    "            class Config:\n",
    "                extra = \"forbid\"\n",
    "        return LabelDocs\n",
    "def safe_response_dump(resp):\n",
    "    if hasattr(resp, \"model_dump\"):\n",
    "        return resp.model_dump()\n",
    "    if hasattr(resp, \"to_dict\"):\n",
    "        return resp.to_dict()\n",
    "    try:\n",
    "        return dict(resp)\n",
    "    except Exception:\n",
    "        return {\"_raw\": str(resp)}\n",
    "\n",
    "def summarize_response(resp):\n",
    "    d = safe_response_dump(resp)\n",
    "    print(\"status:\", d.get(\"status\"))\n",
    "    print(\"incomplete_details:\", d.get(\"incomplete_details\"))\n",
    "    print(\"error:\", d.get(\"error\"))\n",
    "\n",
    "    usage = d.get(\"usage\") or {}\n",
    "    print(\"usage:\", usage)\n",
    "    # usage 안에 output_tokens_details.reasoning_tokens가 찍히면 진짜 원인 확정\n",
    "\n",
    "    outs = d.get(\"output\") or []\n",
    "    print(\"output_items:\", len(outs))\n",
    "    for i, o in enumerate(outs[:3]):\n",
    "        print(f\"  output[{i}].type =\", o.get(\"type\"))\n",
    "        c = o.get(\"content\") or []\n",
    "        print(f\"    content_items={len(c)}\", [x.get(\"type\") for x in c[:5]])\n",
    "\n",
    "    ot = getattr(resp, \"output_text\", None)\n",
    "    if isinstance(ot, str):\n",
    "        print(\"output_text(head):\", ot[:200])\n",
    "    return d\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bcfc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_run_state(reset_logs: bool = True):\n",
    "    global progress\n",
    "    progress = {\n",
    "        \"attempted_calls\": 0,\n",
    "        \"api_ok_calls\": 0,\n",
    "        \"parsed_ok_calls\": 0,\n",
    "        \"valid_docs\": 0,\n",
    "        \"invalid_docs\": 0,\n",
    "        \"exceptions\": 0,\n",
    "    }\n",
    "    with open(PROG_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(progress, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    if reset_logs:\n",
    "        # 로그 파일 비우기(주의: 기존 기록 삭제됨)\n",
    "        for p in [RAW_JSONL, ERR_JSONL, OUT_CSV]:\n",
    "            with open(p, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(\"\")\n",
    "reset_run_state(reset_logs=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a31a607",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, time, random\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "def write_jsonl(path: str, obj: dict):\n",
    "    with open(path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "        f.flush()\n",
    "        os.fsync(f.fileno())\n",
    "\n",
    "def compact_resp_meta(resp) -> Dict[str, Any]:\n",
    "    d = safe_response_dump(resp)\n",
    "    return {\n",
    "        \"status\": d.get(\"status\"),\n",
    "        \"incomplete_details\": d.get(\"incomplete_details\"),\n",
    "        \"error\": d.get(\"error\"),\n",
    "        \"usage\": d.get(\"usage\"),\n",
    "        \"output_text\": getattr(resp, \"output_text\", None),\n",
    "        \"output_types\": [o.get(\"type\") for o in (d.get(\"output\") or [])],\n",
    "    }\n",
    "\n",
    "def try_manual_parse(TextFmt, resp):\n",
    "    raw = extract_raw_text(resp)\n",
    "    if not raw:\n",
    "        return None, None\n",
    "    raw2 = raw.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "    try:\n",
    "        if PYDANTIC_V2:\n",
    "            return TextFmt.model_validate_json(raw2), raw2\n",
    "        else:\n",
    "            return TextFmt.parse_raw(raw2), raw2\n",
    "    except Exception:\n",
    "        return None, raw2\n",
    "\n",
    "def reset_run_state(reset_logs: bool = True):\n",
    "    global progress\n",
    "    progress = {\n",
    "        \"attempted_calls\": 0,\n",
    "        \"api_ok_calls\": 0,\n",
    "        \"parsed_ok_calls\": 0,\n",
    "        \"valid_docs\": 0,\n",
    "        \"invalid_docs\": 0,\n",
    "        \"exceptions\": 0,\n",
    "    }\n",
    "    save_progress()\n",
    "    if reset_logs:\n",
    "        for p in [RAW_JSONL, ERR_JSONL, OUT_CSV]:\n",
    "            with open(p, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(\"\")\n",
    "\n",
    "def generate_and_log_docs(\n",
    "    ctx_list: List[dict],\n",
    "    labels: List[str],\n",
    "    cids: List[int],\n",
    "    *,\n",
    "    base_max_output_tokens: int = 2048,\n",
    "    max_retries: int = 4,\n",
    "    reasoning_effort: str = \"low\",\n",
    "):\n",
    "    TextFmt = make_docs_text_format(len(ctx_list))\n",
    "\n",
    "    system = (\n",
    "        \"You write precise Amazon marketplace product-category descriptions for taxonomy labels.\\n\"\n",
    "        \"ENGLISH ONLY.\\n\"\n",
    "        \"Return JSON that matches the provided schema.\\n\"\n",
    "        \"You will receive an array 'labels' with contexts. Produce docs in the SAME ORDER.\\n\"\n",
    "        \"\\n\"\n",
    "        \"Core rule:\\n\"\n",
    "        \"- Write a standalone description of the label itself only.\\n\"\n",
    "        \"- Do NOT reference or mention parent, children, siblings, hierarchy, or any provided keywords.\\n\"\n",
    "        \"- Even if those fields are present in the input, ignore them completely.\\n\"\n",
    "        \"\\n\"\n",
    "        \"Content guidance:\\n\"\n",
    "        \"- Describe what products sold on Amazon belong in this category, including typical item types, common variants, and typical use cases.\\n\"\n",
    "        \"- Include practical details that matter in customer reviews, such as fit, sizing, compatibility, durability, performance, materials, comfort, ease of use, and value.\\n\"\n",
    "        \"- Define clear boundaries by stating what is included and what is excluded, without naming related categories or using comparisons to parent or sibling labels.\\n\"\n",
    "        \"\\n\"\n",
    "        \"Hard formatting constraints for each doc:\\n\"\n",
    "        \"- Exactly 5 sentences.\\n\"\n",
    "        \"- Single paragraph with no newline characters.\\n\"\n",
    "        \"- Plain text only: no markdown, no bullets, no numbering.\\n\"\n",
    "        \"- 120 to 170 words total.\\n\"\n",
    "        \"- Use exactly five period characters '.' total: one at the end of each sentence.\\n\"\n",
    "        \"- Do NOT use any other periods anywhere (no abbreviations, no initials, no decimals).\\n\"\n",
    "        \"- Do NOT use semicolons or colons.\\n\"\n",
    "    )\n",
    "\n",
    "    user_obj = {\"labels\": ctx_list}\n",
    "\n",
    "    last_err: Optional[Exception] = None\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        mot = base_max_output_tokens * (2 ** attempt)\n",
    "\n",
    "        progress[\"attempted_calls\"] += 1\n",
    "        save_progress()\n",
    "\n",
    "        resp = None\n",
    "        try:\n",
    "            resp = client.responses.parse(\n",
    "                model=MODEL,\n",
    "                input=[\n",
    "                    {\"role\": \"system\", \"content\": system},\n",
    "                    {\"role\": \"user\", \"content\": json.dumps(user_obj, ensure_ascii=False)},\n",
    "                ],\n",
    "                text_format=TextFmt,\n",
    "                max_output_tokens=mot,\n",
    "                store=False,\n",
    "                temperature=0.0,  # 선택(포맷 안정성에 도움)\n",
    "            )\n",
    "            progress[\"api_ok_calls\"] += 1\n",
    "\n",
    "            rec = {\n",
    "                \"ts\": ts_utc(),\n",
    "                \"stage\": \"attempt\",\n",
    "                \"attempt\": attempt,\n",
    "                \"max_output_tokens\": mot,\n",
    "                \"cids\": cids,\n",
    "                \"labels\": labels,\n",
    "                \"n_docs_expected\": len(ctx_list),\n",
    "            }\n",
    "            rec.update(compact_resp_meta(resp))\n",
    "\n",
    "            # 1) parse (structured)\n",
    "            parsed = getattr(resp, \"output_parsed\", None)\n",
    "            manual_parsed, manual_raw_json = (None, None)\n",
    "            if parsed is None:\n",
    "                manual_parsed, manual_raw_json = try_manual_parse(TextFmt, resp)\n",
    "                parsed = manual_parsed\n",
    "                if manual_raw_json is not None:\n",
    "                    rec[\"raw_json\"] = manual_raw_json  # output_text 말고 실제 파싱 대상도 같이 저장\n",
    "\n",
    "            if parsed is None:\n",
    "                rec[\"parse_ok\"] = False\n",
    "                rec[\"parse_error\"] = \"output_parsed is None\"\n",
    "                write_jsonl(RAW_JSONL, rec)\n",
    "                last_err = RuntimeError(rec[\"parse_error\"])\n",
    "                save_progress()\n",
    "                continue\n",
    "\n",
    "            docs = list(parsed.docs)\n",
    "\n",
    "            n_expected = len(ctx_list)\n",
    "            docs = [str(x) for x in docs]  # 방어\n",
    "\n",
    "            rec[\"docs_len_expected\"] = n_expected\n",
    "            rec[\"docs_len_got\"] = len(docs)\n",
    "\n",
    "            # 길이 보정: 부족하면 placeholder로 채우고, 넘치면 자르기\n",
    "            if len(docs) < n_expected:\n",
    "                docs = docs + [\"__MISSING__\"] * (n_expected - len(docs))\n",
    "            elif len(docs) > n_expected:\n",
    "                docs = docs[:n_expected]\n",
    "\n",
    "            progress[\"parsed_ok_calls\"] += 1\n",
    "\n",
    "            # 2) validate\n",
    "            val_list = []\n",
    "            v_ok = 0\n",
    "            v_bad = 0\n",
    "            for d in docs:\n",
    "                d2 = normalize_doc(d)\n",
    "                ok, meta = is_valid_doc(d2)\n",
    "                val_list.append({\"valid\": ok, **meta})\n",
    "                if ok:\n",
    "                    v_ok += 1\n",
    "                else:\n",
    "                    v_bad += 1\n",
    "            progress[\"valid_docs\"] += v_ok\n",
    "            progress[\"invalid_docs\"] += v_bad\n",
    "\n",
    "            rec[\"parse_ok\"] = True\n",
    "            rec[\"docs\"] = docs\n",
    "            rec[\"validation\"] = val_list\n",
    "            rec[\"all_valid\"] = (v_bad == 0)\n",
    "\n",
    "            write_jsonl(RAW_JSONL, rec)\n",
    "            save_progress()\n",
    "            return docs, rec  # 성공 시 종료\n",
    "\n",
    "        except Exception as e:\n",
    "            progress[\"exceptions\"] += 1\n",
    "            save_progress()\n",
    "            last_err = e\n",
    "\n",
    "            err = {\n",
    "                \"ts\": ts_utc(),\n",
    "                \"where\": \"generate_and_log_docs\",\n",
    "                \"stage\": \"exception\",\n",
    "                \"attempt\": attempt,\n",
    "                \"max_output_tokens\": mot,\n",
    "                \"cids\": cids,\n",
    "                \"labels\": labels,\n",
    "                \"error_type\": type(e).__name__,\n",
    "                \"error\": str(e),\n",
    "            }\n",
    "            if resp is not None:\n",
    "                err.update(compact_resp_meta(resp))\n",
    "            write_jsonl(ERR_JSONL, err)\n",
    "            continue\n",
    "\n",
    "    raise RuntimeError(f\"failed after retries: {type(last_err).__name__}: {last_err}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb53e3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (원하면) 같은 RUN_DIR에서 완전 초기화\n",
    "reset_run_state(reset_logs=True)\n",
    "\n",
    "test_cid = 0\n",
    "ctx_list = [build_ctx(test_cid)]\n",
    "docs, rec = generate_and_log_docs(\n",
    "    ctx_list=ctx_list,\n",
    "    labels=[id2label[str(test_cid)]],\n",
    "    cids=[test_cid],\n",
    "    base_max_output_tokens=2048,\n",
    "    max_retries=4,\n",
    "    reasoning_effort=\"low\",   # 안정화 되면 \"minimal\"도 시도 가능\n",
    ")\n",
    "\n",
    "print(\"docs_len:\", len(docs))\n",
    "print(\"saved to:\", RAW_JSONL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadf2d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "\n",
    "\n",
    "CSV_HEADER = [\n",
    "    \"cid\", \"label\", \"doc\", \"final_valid\",\n",
    "    \"word_count\", \"sentence_count\", \"reasons\",\n",
    "    \"from_attempt\", \"max_output_tokens\", \"reasoning_effort\",\n",
    "    \"usage_total_tokens\", \"usage_output_tokens\", \"usage_reasoning_tokens\",\n",
    "    \"status\"\n",
    "]\n",
    "\n",
    "def ensure_csv_header(path: str):\n",
    "    if not os.path.exists(path) or os.path.getsize(path) == 0:\n",
    "        with open(path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "            w = csv.writer(f)\n",
    "            w.writerow(CSV_HEADER)\n",
    "\n",
    "def append_csv_rows(path: str, rows: List[List[Any]]):\n",
    "    with open(path, \"a\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerows(rows)\n",
    "\n",
    "ensure_csv_header(OUT_CSV)\n",
    "\n",
    "# =========================\n",
    "# 1) RAW_JSONL에서 완료 cid 로드 (resume)\n",
    "# =========================\n",
    "def load_done_cids_from_jsonl(path: str) -> set:\n",
    "    done = set()\n",
    "    if not os.path.exists(path):\n",
    "        return done\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            # \"합친 attempt 레코드\"에서 parse_ok & all_valid인 것만 완료로 간주\n",
    "            if obj.get(\"stage\") == \"attempt\" and obj.get(\"parse_ok\") is True and obj.get(\"all_valid\") is True:\n",
    "                cids = obj.get(\"cids\") or []\n",
    "                docs = obj.get(\"docs\") or []\n",
    "                if isinstance(cids, list) and isinstance(docs, list) and len(cids) == len(docs):\n",
    "                    for cid in cids:\n",
    "                        try:\n",
    "                            done.add(int(cid))\n",
    "                        except Exception:\n",
    "                            pass\n",
    "    return done\n",
    "\n",
    "# =========================\n",
    "# 2) 배치 유틸\n",
    "# =========================\n",
    "def chunks(lst: List[int], n: int):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i+n]\n",
    "\n",
    "# =========================\n",
    "# 3) 라벨 1개 단독 보정(규격 실패시)\n",
    "# =========================\n",
    "def regen_single(cid: int,\n",
    "                 base_max_output_tokens: int,\n",
    "                 max_retries: int,\n",
    "                 reasoning_effort: str) -> Tuple[str, bool, Dict[str, Any], Dict[str, Any]]:\n",
    "    ctx_list = [build_ctx(cid)]\n",
    "    label = id2label[str(cid)]\n",
    "    docs, rec = generate_and_log_docs(\n",
    "        ctx_list=ctx_list,\n",
    "        labels=[label],\n",
    "        cids=[cid],\n",
    "        base_max_output_tokens=base_max_output_tokens,\n",
    "        max_retries=max_retries,\n",
    "        reasoning_effort=reasoning_effort,\n",
    "    )\n",
    "    doc = normalize_doc(docs[0])\n",
    "    ok, meta = is_valid_doc(doc)\n",
    "    return doc, ok, meta, rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e15b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# 호출 예산도 같이 초기화\n",
    "CALL_BUDGET = 444\n",
    "\n",
    "# progress 초기화 템플릿\n",
    "def _fresh_progress():\n",
    "    return {\n",
    "        \"attempted_calls\": 0,   # 실제 API 호출 횟수(너 코드 기준)\n",
    "        \"api_ok_calls\": 0,\n",
    "        \"parsed_ok_calls\": 0,\n",
    "        \"valid_docs\": 0,\n",
    "        \"invalid_docs\": 0,\n",
    "        \"exceptions\": 0,\n",
    "    }\n",
    "\n",
    "def init_new_run(run_root=\"/content/drive/MyDrive/BDA/Amazon_products\", reset_logs=True):\n",
    "    \"\"\"\n",
    "    새 RUN_DIR 생성 + 경로 변수/진행변수 초기화\n",
    "    \"\"\"\n",
    "    global RUN_ID, RUN_DIR, OUT_CSV, RAW_JSONL, ERR_JSONL, PROG_JSON, progress\n",
    "\n",
    "    RUN_ID = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "    RUN_DIR = os.path.join(run_root, f\"run_{RUN_ID}\")\n",
    "    os.makedirs(RUN_DIR, exist_ok=True)\n",
    "\n",
    "    OUT_CSV   = os.path.join(RUN_DIR, \"label_docs.csv\")\n",
    "    RAW_JSONL = os.path.join(RUN_DIR, \"raw_generations.jsonl\")\n",
    "    ERR_JSONL = os.path.join(RUN_DIR, \"errors.jsonl\")\n",
    "    PROG_JSON = os.path.join(RUN_DIR, \"progress.json\")\n",
    "\n",
    "    progress = _fresh_progress()\n",
    "    with open(PROG_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(progress, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # CSV 헤더 보장(네가 이미 ensure_csv_header 정의해둔 상태라고 가정)\n",
    "    ensure_csv_header(OUT_CSV)\n",
    "\n",
    "    # 로그 파일 비우기(새 폴더라 보통 필요 없지만, 안전빵)\n",
    "    if reset_logs:\n",
    "        with open(RAW_JSONL, \"w\", encoding=\"utf-8\") as f: f.write(\"\")\n",
    "        with open(ERR_JSONL, \"w\", encoding=\"utf-8\") as f: f.write(\"\")\n",
    "\n",
    "    print(\"RUN_DIR:\", RUN_DIR)\n",
    "    print(\"OUT_CSV:\", OUT_CSV)\n",
    "    print(\"RAW_JSONL:\", RAW_JSONL)\n",
    "    print(\"ERR_JSONL:\", ERR_JSONL)\n",
    "    print(\"PROG_JSON:\", PROG_JSON)\n",
    "    print(\"CALL_BUDGET:\", CALL_BUDGET)\n",
    "\n",
    "# 실행\n",
    "init_new_run(reset_logs=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b689e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_all_labels(\n",
    "    *,\n",
    "    start_cid: int = 0,\n",
    "    end_cid: Optional[int] = None,\n",
    "    batch_size: int = 1,\n",
    "    resume: bool = True,\n",
    "    base_max_output_tokens: int = 12000,\n",
    "    max_retries: int = 4,\n",
    "    reasoning_effort: str = \"minimal\",\n",
    "    fix_reasoning_effort: str = \"low\",\n",
    "    fix_base_max_output_tokens: int = 15000,\n",
    "    fix_max_retries: int = 4,\n",
    "    max_batches_this_run: Optional[int] = None,\n",
    "    call_budget: int = CALL_BUDGET,   # ✅ 추가\n",
    "):\n",
    "    if end_cid is None:\n",
    "        end_cid = num_class\n",
    "\n",
    "    # resume 처리\n",
    "    done = load_done_cids_from_jsonl(RAW_JSONL) if resume else set()\n",
    "    pending = [cid for cid in range(start_cid, end_cid) if cid not in done]\n",
    "\n",
    "    calls_start = progress.get(\"attempted_calls\", 0)  # ✅ 시작 호출 수\n",
    "\n",
    "    print(f\"[RUN] total={end_cid-start_cid} done={len(done)} pending={len(pending)} batch_size={batch_size}\")\n",
    "    print(f\"[BUDGET] used={calls_start}/{call_budget} remaining={max(0, call_budget-calls_start)}\")\n",
    "\n",
    "    if not pending:\n",
    "        summary = {\n",
    "            \"calls_used_this_run\": 0,\n",
    "            \"calls_total\": calls_start,\n",
    "            \"calls_remaining\": max(0, call_budget - calls_start),\n",
    "            \"final_ok\": 0,\n",
    "            \"final_bad\": 0,\n",
    "            \"stopped_reason\": \"nothing_to_do\",\n",
    "        }\n",
    "        return summary\n",
    "\n",
    "    total_final_ok = 0\n",
    "    total_final_bad = 0\n",
    "    batches_done = 0\n",
    "    stopped_reason = \"completed_all\"\n",
    "\n",
    "    for batch in chunks(pending, batch_size):\n",
    "        if max_batches_this_run is not None and batches_done >= max_batches_this_run:\n",
    "            stopped_reason = f\"reached max_batches_this_run={max_batches_this_run}\"\n",
    "            break\n",
    "\n",
    "        # ✅ 예산 체크(배치 시작 전에)\n",
    "        if progress.get(\"attempted_calls\", 0) >= call_budget:\n",
    "            stopped_reason = \"call_budget_exhausted_before_batch\"\n",
    "            break\n",
    "\n",
    "        ctx_list = [build_ctx(cid) for cid in batch]\n",
    "        labels = [id2label[str(cid)] for cid in batch]\n",
    "\n",
    "        # 1차 생성\n",
    "        try:\n",
    "            docs, rec = generate_and_log_docs(\n",
    "                ctx_list=ctx_list,\n",
    "                labels=labels,\n",
    "                cids=batch,\n",
    "                base_max_output_tokens=base_max_output_tokens,\n",
    "                max_retries=max_retries,\n",
    "                reasoning_effort=reasoning_effort,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"[BATCH FAIL] cids={batch[:5]}.. ({len(batch)}) err={type(e).__name__}: {str(e)[:120]}\")\n",
    "            time.sleep(min(10.0, 1.0 + random.random() * 2.0))\n",
    "            batches_done += 1\n",
    "            # ✅ 예산 체크\n",
    "            if progress.get(\"attempted_calls\", 0) >= call_budget:\n",
    "                stopped_reason = \"call_budget_exhausted_after_batch_fail\"\n",
    "                break\n",
    "            continue\n",
    "\n",
    "        final_docs: Dict[int, Dict[str, Any]] = {}\n",
    "        for i, cid in enumerate(batch):\n",
    "            doc = normalize_doc(docs[i])\n",
    "            ok, meta = is_valid_doc(doc)\n",
    "            final_docs[cid] = {\n",
    "                \"cid\": cid,\n",
    "                \"label\": labels[i],\n",
    "                \"doc\": doc,\n",
    "                \"valid\": ok,\n",
    "                \"meta\": meta,\n",
    "                \"rec\": rec,\n",
    "                \"from_attempt\": rec.get(\"attempt\"),\n",
    "                \"max_output_tokens\": rec.get(\"max_output_tokens\"),\n",
    "                \"reasoning_effort\": reasoning_effort,\n",
    "            }\n",
    "\n",
    "        # 규격 실패 cid만 단독 보정 (단, 예산 남아 있을 때만)\n",
    "        bad_cids = [cid for cid in batch if not final_docs[cid][\"valid\"]]\n",
    "        if bad_cids:\n",
    "            cid0 = bad_cids[0]\n",
    "            print(\"[SAMPLE INVALID]\", cid0, final_docs[cid0][\"meta\"])\n",
    "            print(final_docs[cid0][\"doc\"][:240])\n",
    "            # ✅ 보정 들어가기 전에 예산 체크\n",
    "            if progress.get(\"attempted_calls\", 0) >= call_budget:\n",
    "                stopped_reason = \"call_budget_exhausted_before_fix\"\n",
    "                break\n",
    "\n",
    "            print(f\"[FIX] invalid={len(bad_cids)}/{len(batch)} -> single regen\")\n",
    "            for cid in bad_cids:\n",
    "                if progress.get(\"attempted_calls\", 0) >= call_budget:\n",
    "                    stopped_reason = \"call_budget_exhausted_mid_fix\"\n",
    "                    break\n",
    "                try:\n",
    "                    doc2, ok2, meta2, rec2 = regen_single(\n",
    "                        cid,\n",
    "                        base_max_output_tokens=fix_base_max_output_tokens,\n",
    "                        max_retries=fix_max_retries,\n",
    "                        reasoning_effort=fix_reasoning_effort,\n",
    "                    )\n",
    "                    final_docs[cid].update({\n",
    "                        \"doc\": doc2,\n",
    "                        \"valid\": ok2,\n",
    "                        \"meta\": meta2,\n",
    "                        \"rec\": rec2,\n",
    "                        \"from_attempt\": rec2.get(\"attempt\"),\n",
    "                        \"max_output_tokens\": rec2.get(\"max_output_tokens\"),\n",
    "                        \"reasoning_effort\": fix_reasoning_effort,\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"[FIX FAIL] cid={cid} err={type(e).__name__}: {str(e)[:120]}\")\n",
    "                    time.sleep(min(10.0, 1.0 + random.random() * 2.0))\n",
    "            if stopped_reason.startswith(\"call_budget_exhausted\"):\n",
    "                break\n",
    "\n",
    "        # CSV 저장\n",
    "        rows = []\n",
    "        for cid in batch:\n",
    "            item = final_docs[cid]\n",
    "            meta = item[\"meta\"] or {}\n",
    "            rec_used = item[\"rec\"] or {}\n",
    "            usage = (rec_used.get(\"usage\") or {}) if isinstance(rec_used, dict) else {}\n",
    "\n",
    "            out_tokens = usage.get(\"output_tokens\")\n",
    "            total_tokens = usage.get(\"total_tokens\")\n",
    "            od = usage.get(\"output_tokens_details\") or {}\n",
    "            reasoning_tokens = od.get(\"reasoning_tokens\")\n",
    "\n",
    "            rows.append([\n",
    "                cid,\n",
    "                item[\"label\"],\n",
    "                item[\"doc\"],\n",
    "                item[\"valid\"],\n",
    "                meta.get(\"word_count\"),\n",
    "                meta.get(\"sentence_count\"),\n",
    "                \"|\".join(meta.get(\"reasons\") or []),\n",
    "                item.get(\"from_attempt\"),\n",
    "                item.get(\"max_output_tokens\"),\n",
    "                item.get(\"reasoning_effort\"),\n",
    "                total_tokens,\n",
    "                out_tokens,\n",
    "                reasoning_tokens,\n",
    "                rec_used.get(\"status\"),\n",
    "            ])\n",
    "\n",
    "            if item[\"valid\"]:\n",
    "                total_final_ok += 1\n",
    "            else:\n",
    "                total_final_bad += 1\n",
    "\n",
    "        append_csv_rows(OUT_CSV, rows)\n",
    "\n",
    "        batches_done += 1\n",
    "        if batches_done % 10 == 0:\n",
    "            used_now = progress.get(\"attempted_calls\", 0)\n",
    "            print(f\"[PROGRESS] batches={batches_done} ok={total_final_ok} bad={total_final_bad} calls_used={used_now}/{call_budget}\")\n",
    "\n",
    "        # ✅ 배치 끝날 때도 예산 체크\n",
    "        if progress.get(\"attempted_calls\", 0) >= call_budget:\n",
    "            stopped_reason = \"call_budget_exhausted_after_batch\"\n",
    "            break\n",
    "\n",
    "    calls_end = progress.get(\"attempted_calls\", 0)\n",
    "    summary = {\n",
    "        \"batches_done\": batches_done,\n",
    "        \"final_ok\": total_final_ok,\n",
    "        \"final_bad\": total_final_bad,\n",
    "        \"calls_used_this_run\": calls_end - calls_start,\n",
    "        \"calls_total\": calls_end,\n",
    "        \"calls_remaining\": max(0, call_budget - calls_end),\n",
    "        \"call_budget\": call_budget,\n",
    "        \"stopped_reason\": stopped_reason,\n",
    "        \"OUT_CSV\": OUT_CSV,\n",
    "        \"RAW_JSONL\": RAW_JSONL,\n",
    "        \"ERR_JSONL\": ERR_JSONL,\n",
    "    }\n",
    "    print(\"[SUMMARY]\", summary)\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab2d161",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensure_csv_header(OUT_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472a8582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset_run_state(reset_logs=True)  # 새로 시작이면\n",
    "\n",
    "\n",
    "summary = run_all_labels(\n",
    "    start_cid=0,\n",
    "    end_cid=num_class,\n",
    "    batch_size=3,      # 444회 제한이면 4 추천(여유 호출로 fix 가능)\n",
    "    resume=False,      # 새 run이면 False\n",
    "    call_budget=444,\n",
    ")\n",
    "\n",
    "print(\"API calls used this run:\", summary[\"calls_used_this_run\"])\n",
    "print(\"API calls total:\", summary[\"calls_total\"])\n",
    "print(\"API calls remaining:\", summary[\"calls_remaining\"])\n",
    "print(\"Stopped reason:\", summary[\"stopped_reason\"])\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
