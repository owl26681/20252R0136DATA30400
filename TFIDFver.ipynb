{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26217918-5c27-43a7-a504-84e4cd5bba75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T07:21:07.043070Z",
     "iopub.status.busy": "2025-11-13T07:21:07.042928Z",
     "iopub.status.idle": "2025-11-13T07:21:08.326670Z",
     "shell.execute_reply": "2025-11-13T07:21:08.326125Z",
     "shell.execute_reply.started": "2025-11-13T07:21:07.043055Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25eba133-3361-42f0-8305-6df95935d14f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T07:21:08.327432Z",
     "iopub.status.busy": "2025-11-13T07:21:08.327148Z",
     "iopub.status.idle": "2025-11-13T07:21:10.733411Z",
     "shell.execute_reply": "2025-11-13T07:21:10.732866Z",
     "shell.execute_reply.started": "2025-11-13T07:21:08.327417Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chocolate_bars 0.5059\n",
      "chocolate_gifts 0.4276\n",
      "chocolate 0.3754\n",
      "chocolate_covered_fruit 0.3548\n",
      "dried_fruit_raisins 0.2765\n",
      "chocolate_pretzels 0.2373\n",
      "fresh_baked_cookies 0.2332\n",
      "grocery_gourmet_food 0.2266\n",
      "snack_gifts 0.2254\n",
      "chocolate_assortments 0.2173\n",
      "candy_chocolate 0.1635\n",
      "hot_cocoa 0.1609\n",
      "food 0.1457\n",
      "gourmet_gifts 0.1342\n",
      "snack_food 0.121\n",
      "trail_mix 0.1181\n",
      "granola_trail_mix_bars 0.1044\n",
      "fruit_leather 0.1023\n",
      "toaster_pastries 0.1015\n",
      "cookies 0.0996\n",
      "fruit 0.0941\n",
      "raisins 0.0925\n",
      "meat_poultry 0.0903\n",
      "marshmallows 0.087\n",
      "changing_table_pads_covers 0.0863\n",
      "popcorn 0.085\n",
      "granola_bars 0.0846\n",
      "produce 0.0833\n",
      "solid_feeding 0.0831\n",
      "milk 0.0827\n",
      "chocolate_truffles 0.0822\n",
      "rice_cakes 0.0777\n",
      "nutrition_wellness 0.0746\n",
      "party_mix 0.0734\n",
      "p_t_s 0.0716\n",
      "fruit_gifts 0.0684\n",
      "sensual_delights 0.0649\n",
      "foie_gras_p_t_s 0.062\n",
      "sugars_sweeteners 0.0604\n",
      "salsas 0.0594\n",
      "eggs 0.059\n",
      "cakes 0.0569\n",
      "nutrition_bars_drinks 0.0569\n",
      "chocolate_covered_nuts 0.0565\n",
      "dessert_gifts 0.0551\n",
      "spices_gifts 0.0545\n",
      "meat_gifts 0.0541\n",
      "crackers 0.0533\n",
      "juices 0.0517\n",
      "puffed_snacks 0.0512\n",
      "bars 0.0506\n",
      "cloth_diapers 0.0498\n",
      "sausages 0.0495\n",
      "baby_food 0.0489\n",
      "assortments 0.0484\n",
      "jams 0.0457\n",
      "pudding 0.0455\n",
      "jams_preserves_gifts 0.0446\n",
      "fruits 0.0429\n",
      "pretzels 0.0393\n",
      "sauces_gifts 0.0365\n",
      "seafood_gifts 0.0356\n",
      "nut_clusters 0.0322\n",
      "jerky_dried_meats 0.0322\n",
      "suckers_lollipops 0.0318\n",
      "treats 0.0312\n",
      "cheese_gifts 0.0306\n",
      "candy_gifts 0.0302\n",
      "baby_formula 0.0299\n",
      "gummy_candy 0.0229\n",
      "jerky 0.0\n",
      "toys_games 0.0\n",
      "games 0.0\n",
      "puzzles 0.0\n",
      "jigsaw_puzzles 0.0\n",
      "board_games 0.0\n",
      "beverages 0.0\n",
      "beauty 0.0\n",
      "makeup 0.0\n",
      "nails 0.0\n",
      "arts_crafts 0.0\n",
      "drawing_painting_supplies 0.0\n",
      "action_toy_figures 0.0\n",
      "figures 0.0\n",
      "dolls_accessories 0.0\n",
      "dolls 0.0\n",
      "card_games 0.0\n",
      "drawing_sketching_tablets 0.0\n",
      "baby_toddler_toys 0.0\n",
      "shape_sorters 0.0\n",
      "health_personal_care 0.0\n",
      "personal_care 0.0\n",
      "deodorants_antiperspirants 0.0\n",
      "learning_education 0.0\n",
      "habitats 0.0\n",
      "electronics_for_kids 0.0\n",
      "household_supplies 0.0\n",
      "household_batteries 0.0\n",
      "push_pull_toys 0.0\n",
      "stuffed_animals_plush 0.0\n",
      "tricycles 0.0\n",
      "scooters_wagons 0.0\n",
      "clay_dough 0.0\n",
      "health_care 0.0\n",
      "allergy 0.0\n",
      "baby_products 0.0\n",
      "gear 0.0\n",
      "baby_gyms_playmats 0.0\n",
      "shaving_hair_removal 0.0\n",
      "skin_care 0.0\n",
      "face 0.0\n",
      "animals_figures 0.0\n",
      "feminine_care 0.0\n",
      "music_sound 0.0\n",
      "oral_hygiene 0.0\n",
      "grown_up_toys 0.0\n",
      "dress_up_pretend_play 0.0\n",
      "pretend_play 0.0\n",
      "novelty_gag_toys 0.0\n",
      "bath_body 0.0\n",
      "cleansers 0.0\n",
      "playsets 0.0\n",
      "d_puzzles 0.0\n",
      "dollhouses 0.0\n",
      "lip_care_products 0.0\n",
      "tools_accessories 0.0\n",
      "nail_tools 0.0\n",
      "eye_care 0.0\n",
      "pill_cases_splitters 0.0\n",
      "hair_care 0.0\n",
      "styling_products 0.0\n",
      "electronic_toys 0.0\n",
      "body 0.0\n",
      "toy_balls 0.0\n",
      "eyes 0.0\n",
      "trading_card_games 0.0\n",
      "foot_care 0.0\n",
      "hands_nails 0.0\n",
      "sun 0.0\n",
      "medical_supplies_equipment 0.0\n",
      "daily_living_aids 0.0\n",
      "baby_child_care 0.0\n",
      "paper_plastic 0.0\n",
      "incontinence 0.0\n",
      "shampoos 0.0\n",
      "conditioners 0.0\n",
      "music_players_karaoke 0.0\n",
      "cough_cold 0.0\n",
      "bath 0.0\n",
      "tests 0.0\n",
      "building_toys 0.0\n",
      "building_sets 0.0\n",
      "stress_reduction 0.0\n",
      "family_planning_contraceptives 0.0\n",
      "vitamins_supplements 0.0\n",
      "hair_color 0.0\n",
      "pain_relievers 0.0\n",
      "cotton_swabs 0.0\n",
      "styling_tools 0.0\n",
      "first_aid 0.0\n",
      "scrubs_body_treatments 0.0\n",
      "cleaning_tools 0.0\n",
      "pegged_puzzles 0.0\n",
      "diabetes 0.0\n",
      "magic_kits_accessories 0.0\n",
      "gifts 0.0\n",
      "albums 0.0\n",
      "crib_toys_attachments 0.0\n",
      "digestion_nausea 0.0\n",
      "electronic_pets 0.0\n",
      "sexual_wellness 0.0\n",
      "safer_sex 0.0\n",
      "thermometers 0.0\n",
      "stacking_nesting_toys 0.0\n",
      "makeup_remover 0.0\n",
      "temporary_tattoos 0.0\n",
      "sports_outdoor_play 0.0\n",
      "play_tents_tunnels 0.0\n",
      "science 0.0\n",
      "sports 0.0\n",
      "bath_toys 0.0\n",
      "puppets 0.0\n",
      "systems_accessories 0.0\n",
      "health_monitors 0.0\n",
      "inflatable_bouncers 0.0\n",
      "hobbies 0.0\n",
      "model_building_kits_tools 0.0\n",
      "blackboards_whiteboards 0.0\n",
      "pools_water_fun 0.0\n",
      "rattles 0.0\n",
      "sandboxes_accessories 0.0\n",
      "activity_play_centers 0.0\n",
      "car_seat_stroller_toys 0.0\n",
      "feeding 0.0\n",
      "bottle_feeding 0.0\n",
      "breastfeeding 0.0\n",
      "diapering 0.0\n",
      "diaper_changing_kits 0.0\n",
      "puzzle_accessories 0.0\n",
      "diaper_pails_refills 0.0\n",
      "safety 0.0\n",
      "bathroom_safety 0.0\n",
      "massage_relaxation 0.0\n",
      "gates_doorways 0.0\n",
      "nursery 0.0\n",
      "furniture 0.0\n",
      "monitors 0.0\n",
      "plush_backpacks_purses 0.0\n",
      "statues 0.0\n",
      "bathing_skin_care 0.0\n",
      "bathing_tubs_seats 0.0\n",
      "vehicles_remote_control 0.0\n",
      "play_vehicles 0.0\n",
      "backpacks_carriers 0.0\n",
      "craft_kits 0.0\n",
      "car_seats_accessories 0.0\n",
      "car_seats 0.0\n",
      "nursery_d_cor 0.0\n",
      "hammering_pounding_toys 0.0\n",
      "bedding 0.0\n",
      "play_trains_railway_sets 0.0\n",
      "rockets 0.0\n",
      "stacking_blocks 0.0\n",
      "diaper_bags 0.0\n",
      "strollers 0.0\n",
      "gym_sets_swings 0.0\n",
      "pregnancy_maternity 0.0\n",
      "maternity_pillows 0.0\n",
      "rocking_spring_ride_ons 0.0\n",
      "braces 0.0\n",
      "accessories 0.0\n",
      "vehicle_playsets 0.0\n",
      "doll_accessories 0.0\n",
      "pet_supplies 0.0\n",
      "cats 0.0\n",
      "litter_housebreaking 0.0\n",
      "spinning_tops 0.0\n",
      "sets 0.0\n",
      "travel_games 0.0\n",
      "pillows_stools 0.0\n",
      "battling_tops 0.0\n",
      "cameras_camcorders 0.0\n",
      "dance_mats 0.0\n",
      "radio_control 0.0\n",
      "grooming_healthcare_kits 0.0\n",
      "balls 0.0\n",
      "tile_games 0.0\n",
      "potty_training 0.0\n",
      "potties_seats 0.0\n",
      "highchairs_booster_seats 0.0\n",
      "stuffed_animals_toys 0.0\n",
      "dvd_games 0.0\n",
      "edge_corner_guards 0.0\n",
      "basic_life_skills_toys 0.0\n",
      "activity_centers_entertainers 0.0\n",
      "thermometer_accessories 0.0\n",
      "wipes_holders 0.0\n",
      "gift_sets 0.0\n",
      "joggers 0.0\n",
      "facial_steamers 0.0\n",
      "kites_wind_spinners 0.0\n",
      "dogs 0.0\n",
      "toys 0.0\n",
      "walkers 0.0\n",
      "slumber_bags 0.0\n",
      "die_cast_vehicles 0.0\n",
      "easels 0.0\n",
      "lips 0.0\n",
      "tea 0.0\n",
      "reading_writing 0.0\n",
      "stacking_games 0.0\n",
      "sauces_dips 0.0\n",
      "sauces 0.0\n",
      "breakfast_foods 0.0\n",
      "cereals 0.0\n",
      "shopping_cart_covers 0.0\n",
      "pantry_staples 0.0\n",
      "scaled_model_vehicles 0.0\n",
      "cooking_baking_supplies 0.0\n",
      "personal_video_players_accessories 0.0\n",
      "fragrance 0.0\n",
      "women_s 0.0\n",
      "keepsakes 0.0\n",
      "swings 0.0\n",
      "trains_accessories 0.0\n",
      "disposable_diapers 0.0\n",
      "plug_play_video_games 0.0\n",
      "floor_puzzles 0.0\n",
      "fresh_flowers_live_indoor_plants 0.0\n",
      "live_indoor_plants 0.0\n",
      "weight_loss_products 0.0\n",
      "smoking_cessation 0.0\n",
      "beauty_fashion 0.0\n",
      "mirrors 0.0\n",
      "coffee 0.0\n",
      "cabinet_locks_straps 0.0\n",
      "plush_pillows 0.0\n",
      "floor_games 0.0\n",
      "makeup_brushes_tools 0.0\n",
      "alternative_medicine 0.0\n",
      "men_s 0.0\n",
      "step_stools 0.0\n",
      "rails_rail_guards 0.0\n",
      "laundry 0.0\n",
      "women_s_health 0.0\n",
      "standard 0.0\n",
      "beds_furniture 0.0\n",
      "herbs 0.0\n",
      "sleep_positioners 0.0\n",
      "health_supplies 0.0\n",
      "breakfast_cereal_bars 0.0\n",
      "body_art 0.0\n",
      "condiments 0.0\n",
      "breads_bakery 0.0\n",
      "dried_beans 0.0\n",
      "household_cleaning 0.0\n",
      "collars 0.0\n",
      "educational_repellents 0.0\n",
      "adult_toys_games 0.0\n",
      "teddy_bears 0.0\n",
      "therapeutic_skin_care 0.0\n",
      "sand_water_tables 0.0\n",
      "slot_cars 0.0\n",
      "soft_drinks 0.0\n",
      "chips_crisps 0.0\n",
      "licorice 0.0\n",
      "feeding_watering_supplies 0.0\n",
      "blasters_foam_play 0.0\n",
      "meat_seafood 0.0\n",
      "wild_game_fowl 0.0\n",
      "spices_seasonings 0.0\n",
      "training_behavior_aids 0.0\n",
      "gardening_tools 0.0\n",
      "bathroom_aids_safety 0.0\n",
      "pogo_sticks_hoppers 0.0\n",
      "powdered_drink_mixes 0.0\n",
      "playards 0.0\n",
      "gag_toys_practical_jokes 0.0\n",
      "lighters 0.0\n",
      "money_banks 0.0\n",
      "marble_runs 0.0\n",
      "game_collections 0.0\n",
      "kitchen_safety 0.0\n",
      "fish_aquatic_pets 0.0\n",
      "gum 0.0\n",
      "outdoor_safety 0.0\n",
      "hair_nails 0.0\n",
      "aquarium_lights 0.0\n",
      "blocks 0.0\n",
      "tandem 0.0\n",
      "occupational_physical_therapy_aids 0.0\n",
      "packaged_meals_side_dishes 0.0\n",
      "indoor_climbers_play_structures 0.0\n",
      "pumps_filters 0.0\n",
      "beds_accessories 0.0\n",
      "energy_drinks 0.0\n",
      "sleep_snoring 0.0\n",
      "geography 0.0\n",
      "small_animals 0.0\n",
      "houses_habitats 0.0\n",
      "dairy_eggs 0.0\n",
      "cheese 0.0\n",
      "travel_systems 0.0\n",
      "walkie_talkies 0.0\n",
      "mobility_aids_equipment 0.0\n",
      "sexual_enhancers 0.0\n",
      "dips 0.0\n",
      "dollhouse_accessories 0.0\n",
      "bathing_accessories 0.0\n",
      "grooming 0.0\n",
      "baby_seats 0.0\n",
      "wind_up_toys 0.0\n",
      "dishwashing 0.0\n",
      "carriers_strollers 0.0\n",
      "flash_cards 0.0\n",
      "brain_teasers 0.0\n",
      "nesting_dolls 0.0\n",
      "test_kits 0.0\n",
      "lightweight 0.0\n",
      "hair_loss_products 0.0\n",
      "water_treatments 0.0\n",
      "birds 0.0\n",
      "hair_scalp_treatments 0.0\n",
      "cages_accessories 0.0\n",
      "gummy_candies 0.0\n",
      "houses 0.0\n",
      "ear_care 0.0\n",
      "pizza_crusts 0.0\n",
      "hard_candies 0.0\n",
      "sports_supplements 0.0\n",
      "baking_mixes 0.0\n",
      "pork_rinds 0.0\n",
      "pasta_noodles 0.0\n",
      "carriers_travel_products 0.0\n",
      "fresh_fruits 0.0\n",
      "chips 0.0\n",
      "mathematics_counting 0.0\n",
      "toy_banks 0.0\n",
      "training_pants 0.0\n",
      "tea_gifts 0.0\n",
      "oils 0.0\n",
      "aquarium_hoods 0.0\n",
      "tortillas 0.0\n",
      "doors 0.0\n",
      "standard_playing_card_decks 0.0\n",
      "fudge 0.0\n",
      "syrups 0.0\n",
      "printing_stamping 0.0\n",
      "toy_gift_sets 0.0\n",
      "canned_jarred_food 0.0\n",
      "fresh_vegetables 0.0\n",
      "apparel_accessories 0.0\n",
      "chewing_gum 0.0\n",
      "puzzle_play_mats 0.0\n",
      "electrical_safety 0.0\n",
      "marble_games 0.0\n",
      "miniatures 0.0\n",
      "finger_boards_finger_bikes 0.0\n",
      "coconut_water 0.0\n",
      "handheld_games 0.0\n",
      "slime_putty_toys 0.0\n",
      "pastries 0.0\n",
      "health_baby_care 0.0\n",
      "teethers 0.0\n",
      "butter 0.0\n",
      "breakfast_bakery 0.0\n",
      "stickers 0.0\n",
      "soaps_cleansers 0.0\n",
      "fitness_equipment 0.0\n",
      "water 0.0\n",
      "portable_changing_pads 0.0\n",
      "dice_gaming_dice 0.0\n",
      "pacifiers_accessories 0.0\n",
      "cocktail_mixers 0.0\n",
      "aquariums 0.0\n",
      "ball_pits_accessories 0.0\n",
      "seafood 0.0\n",
      "bags_cases 0.0\n",
      "jelly_beans 0.0\n",
      "novelty_spinning_tops 0.0\n",
      "automatic_feeders 0.0\n",
      "mints 0.0\n",
      "makeup_sets 0.0\n",
      "cleaners 0.0\n",
      "fresh_cut_flowers 0.0\n",
      "prams 0.0\n",
      "nuts_seeds 0.0\n",
      "taffy 0.0\n",
      "bunny_rabbit_central 0.0\n",
      "rabbit_hutches 0.0\n",
      "aquarium_d_cor 0.0\n",
      "viewfinders 0.0\n",
      "harnesses_leashes 0.0\n",
      "game_accessories 0.0\n",
      "game_room_games 0.0\n",
      "cages 0.0\n",
      "non_slip_bath_mats 0.0\n",
      "halva 0.0\n",
      "stimulants 0.0\n",
      "beanbags_foot_bags 0.0\n",
      "shampoo_conditioner_sets 0.0\n",
      "breadcrumbs 0.0\n",
      "extracts_flavoring 0.0\n",
      "plush_puppets 0.0\n",
      "shampoo_plus_conditioner 0.0\n",
      "memorials 0.0\n",
      "die_cast_toy_vehicles 0.0\n",
      "aquarium_starter_kits 0.0\n",
      "coffee_gifts 0.0\n",
      "air_fresheners 0.0\n",
      "sugar_substitutes 0.0\n",
      "bacon 0.0\n",
      "cat_flaps 0.0\n",
      "aquarium_heaters 0.0\n",
      "hair_relaxers 0.0\n",
      "breads 0.0\n",
      "packaged_breads 0.0\n",
      "dessert_toppings 0.0\n",
      "diaper_stackers_caddies 0.0\n",
      "prisms_kaleidoscopes 0.0\n",
      "maternity 0.0\n",
      "crackers_biscuits 0.0\n",
      "coin_collecting 0.0\n",
      "kickball_playground_balls 0.0\n",
      "hair_perms_texturizers 0.0\n",
      "yo_yos 0.0\n",
      "flours_meals 0.0\n",
      "beef 0.0\n",
      "molding_sculpting_sticks 0.0\n",
      "washcloths_towels 0.0\n",
      "stuffing 0.0\n",
      "baking_powder 0.0\n",
      "cereal 0.0\n",
      "exotic_meats 0.0\n",
      "breadsticks 0.0\n",
      "cloth_diaper_accessories 0.0\n",
      "carriers 0.0\n",
      "toffee 0.0\n",
      "hair_coloring_tools 0.0\n",
      "caramels 0.0\n",
      "aromatherapy 0.0\n",
      "seat_covers 0.0\n",
      "bondage_gear_accessories 0.0\n",
      "sun_protection 0.0\n",
      "dinners 0.0\n",
      "aquarium_stands 0.0\n",
      "teaching_clocks 0.0\n",
      "milk_substitutes 0.0\n",
      "bubble_bath 0.0\n",
      "novelties 0.0\n",
      "beads 0.0\n",
      "fish_bowls 0.0\n",
      "odor_stain_removers 0.0\n",
      "food_coloring 0.0\n",
      "children_s 0.0\n",
      "ice_cream_frozen_desserts 0.0\n",
      "pastry_decorations 0.0\n",
      "chicken 0.0\n",
      "sports_drinks 0.0\n",
      "aprons_smocks 0.0\n",
      "electronics 0.0\n",
      "sex_furniture 0.0\n",
      "pork 0.0\n",
      "dried_fruit 0.0\n",
      "flying_toys 0.0\n",
      "shampoo 0.0\n",
      "coatings_batters 0.0\n",
      "hydrometers 0.0\n",
      "lamb 0.0\n",
      "exercise_wheels 0.0\n",
      "breeding_tanks 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, ConcatDataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "def load_label_file(path: str) -> str:\n",
    "    \"\"\"key: value1,value2,... 형식으로 된 .txt 파일을 통째로 읽어서 문자열로 반환\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def parse_key_value_lines(text: str):\n",
    "    \"\"\"'key:val1,val2,...' 여러 줄을 딕셔너리로 변환\"\"\"\n",
    "    id2label = {}\n",
    "    for line in text.splitlines():\n",
    "        line = line.strip()\n",
    "        if not line or \":\" not in line:\n",
    "            continue\n",
    "        key, vals = line.split(\":\", 1)\n",
    "        id2label[key.strip()] = vals.strip()\n",
    "    return id2label\n",
    "\n",
    "def preprocess_label_text(label_path_str: str):\n",
    "    cleaned = label_path_str.lower()\n",
    "    cleaned = re.sub(r\"[:,]\", \" \", cleaned)\n",
    "    cleaned = re.sub(r\"_\", \" \", cleaned)\n",
    "    cleaned = re.sub(r\"[^a-z0-9 ]\", \" \", cleaned)\n",
    "    cleaned = re.sub(r\"\\s+\", \" \", cleaned).strip()\n",
    "    return cleaned\n",
    "\n",
    "def build_tfidf_vectorizer(label_texts):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    label_tfidf = vectorizer.fit_transform(label_texts)\n",
    "    return vectorizer, label_tfidf\n",
    "\n",
    "def compute_lexical_similarity(doc_text, vectorizer, label_tfidf):\n",
    "    doc_vec = vectorizer.transform([doc_text])\n",
    "    sims = cosine_similarity(doc_vec, label_tfidf)[0]\n",
    "    return sims\n",
    "\n",
    "    \n",
    "label_raw_text = load_label_file(\"Amazon_products/class_related_keywords.txt\")  # 네 파일 이름에 맞춰 바꿔\n",
    "id2label = parse_key_value_lines(label_raw_text)\n",
    "\n",
    "# 2) 라벨 텍스트 전처리해서 TF-IDF 학습\n",
    "label_keys = list(id2label.keys())\n",
    "label_texts = [\n",
    "    preprocess_label_text(f\"{k} {id2label[k]}\")\n",
    "    for k in label_keys\n",
    "]\n",
    "vectorizer, label_tfidf = build_tfidf_vectorizer(label_texts)\n",
    "\n",
    "# 3) 테스트용 문서 하나 넣어보기\n",
    "doc = \"gourmet organic chocolate snack\"\n",
    "doc_clean = preprocess_label_text(doc)\n",
    "sims = compute_lexical_similarity(doc_clean, vectorizer, label_tfidf)\n",
    "\n",
    "# 4) 결과 보기\n",
    "label_sims = list(zip(label_keys, sims))\n",
    "label_sims.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for lbl, score in label_sims:\n",
    "    print(lbl, round(score, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19ba131e-72d7-4e8c-b1fe-2f3ef78b1466",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T07:21:10.739464Z",
     "iopub.status.busy": "2025-11-13T07:21:10.739314Z",
     "iopub.status.idle": "2025-11-13T07:21:10.752770Z",
     "shell.execute_reply": "2025-11-13T07:21:10.752268Z",
     "shell.execute_reply.started": "2025-11-13T07:21:10.739449Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3466,)\n"
     ]
    }
   ],
   "source": [
    "def build_label_embeddings(label_keys, label_tfidf, dense: bool = True):\n",
    "    \"\"\"\n",
    "    label_keys: 라벨 이름 리스트 (vectorize할 때 썼던 순서와 같아야 함)\n",
    "    label_tfidf: shape = (n_labels, vocab_size) 인 sparse matrix\n",
    "    dense: True면 numpy array로 바꿔서 돌려줌\n",
    "\n",
    "    return:\n",
    "        dict: {label_name: embedding_vector}\n",
    "    \"\"\"\n",
    "    embeddings = {}\n",
    "    if dense:\n",
    "        label_tfidf_dense = label_tfidf.toarray()\n",
    "        for i, label in enumerate(label_keys):\n",
    "            embeddings[label] = label_tfidf_dense[i]\n",
    "    else:\n",
    "        # sparse 그대로\n",
    "        for i, label in enumerate(label_keys):\n",
    "            embeddings[label] = label_tfidf[i]\n",
    "    return embeddings\n",
    "\n",
    "label_embeddings = build_label_embeddings(label_keys, label_tfidf, dense=True)\n",
    "print(label_embeddings[\"grocery_gourmet_food\"].shape)  # (vocab_size,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e1bcacb-8765-49a0-b6d6-d18d6f5e5531",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T01:28:59.947460Z",
     "iopub.status.busy": "2025-11-11T01:28:59.947322Z",
     "iopub.status.idle": "2025-11-11T01:28:59.988985Z",
     "shell.execute_reply": "2025-11-11T01:28:59.988493Z",
     "shell.execute_reply.started": "2025-11-11T01:28:59.947446Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5efad2e-4006-4b90-920b-1e70cc6b7ca7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T07:21:10.753329Z",
     "iopub.status.busy": "2025-11-13T07:21:10.753182Z",
     "iopub.status.idle": "2025-11-13T07:21:10.758734Z",
     "shell.execute_reply": "2025-11-13T07:21:10.758249Z",
     "shell.execute_reply.started": "2025-11-13T07:21:10.753316Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roots: [0, 3, 10, 23, 40, 169]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_edges(path):\n",
    "    edges = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            raw = line.strip()\n",
    "            if not raw or raw.startswith(\"#\"):\n",
    "                continue\n",
    "            parts = raw.split()\n",
    "            if len(parts) < 2:\n",
    "                continue\n",
    "            try:\n",
    "                u, v = int(parts[0]), int(parts[1])\n",
    "            except ValueError:\n",
    "                continue\n",
    "            edges.append((u, v))\n",
    "    return edges\n",
    "\n",
    "def find_roots(edges):\n",
    "    parents = set()\n",
    "    children = set()\n",
    "    for u, v in edges:\n",
    "        parents.add(u)\n",
    "        children.add(v)\n",
    "    # 부모로만 나온 애들 = 루트들\n",
    "    roots = parents - children\n",
    "    return sorted(roots)\n",
    "\n",
    "# --- 사용 ---\n",
    "E = load_edges(\"Amazon_products/class_hierarchy.txt\")\n",
    "\n",
    "N = 531\n",
    "A = np.zeros((N, N), dtype=np.uint8)\n",
    "for u, v in E:\n",
    "    A[u, v] = 1\n",
    "    A[v, u] = 1   # 탐색용으로는 무방향 인접행렬 써도 됨\n",
    "\n",
    "B = np.zeros((N, N), dtype=np.uint8)\n",
    "for u, v in E:\n",
    "    B[u, v] = 1\n",
    "\n",
    "roots = find_roots(E)\n",
    "print(\"roots:\", roots)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cde32d7f-3c5e-4810-ab28-07a7abd05c59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T07:21:11.730884Z",
     "iopub.status.busy": "2025-11-13T07:21:11.730698Z",
     "iopub.status.idle": "2025-11-13T07:21:11.738861Z",
     "shell.execute_reply": "2025-11-13T07:21:11.738265Z",
     "shell.execute_reply.started": "2025-11-13T07:21:11.730867Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------\n",
    "# GAT \n",
    "# ---------------------------\n",
    "\n",
    "class SimpleGATLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, heads=4, concat=True, dropout=0.2, negative_slope=0.2, residual=True):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.out_dim = out_dim\n",
    "        self.concat = concat\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.leaky_relu = nn.LeakyReLU(negative_slope)\n",
    "        self.lin = nn.Linear(in_dim, heads * out_dim, bias=False)\n",
    "        self.a_src = nn.Parameter(torch.Tensor(heads, out_dim))\n",
    "        self.a_dst = nn.Parameter(torch.Tensor(heads, out_dim))\n",
    "        self.residual = residual\n",
    "        if residual and (in_dim == (heads * out_dim if concat else out_dim)):\n",
    "            self.res_proj = nn.Identity()\n",
    "        elif residual:\n",
    "            self.res_proj = nn.Linear(in_dim, heads * out_dim if concat else out_dim, bias=False)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.lin.weight)\n",
    "        nn.init.xavier_uniform_(self.a_src)\n",
    "        nn.init.xavier_uniform_(self.a_dst)\n",
    "        if self.residual and not isinstance(getattr(self, \"res_proj\", None), nn.Identity):\n",
    "            nn.init.xavier_uniform_(self.res_proj.weight)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        \"\"\"\n",
    "        x: [N, Fin]\n",
    "        adj: [N, N] (0/1; self-loop 없음)\n",
    "        \"\"\"\n",
    "        N = x.size(0)\n",
    "        Wh = self.lin(x).view(N, self.heads, self.out_dim)  # [N, H, F]\n",
    "\n",
    "        e_src = (Wh * self.a_src).sum(dim=-1)  # [N, H]\n",
    "        e_dst = (Wh * self.a_dst).sum(dim=-1)  # [N, H]\n",
    "        e = e_src.unsqueeze(1) + e_dst.unsqueeze(0)  # [N, N, H]\n",
    "        e = self.leaky_relu(e)\n",
    "        # --- 안전한 masked softmax ---\n",
    "        mask = (adj > 0).unsqueeze(-1)                    # [N, N, 1]\n",
    "        e = e.masked_fill(~mask, -1e9)                    # -inf 대신 -1e9로 NaN 방지\n",
    "        alpha = torch.softmax(e, dim=1)                   # 소프트맥스\n",
    "        alpha = alpha * mask.float()                      # 마스크로 0 처리\n",
    "        denom = alpha.sum(dim=1, keepdim=True).clamp(min=1e-12)  # 이웃 없을 때 0 분모 방지\n",
    "        alpha = alpha / denom                             # 이웃들로 정규화\n",
    "\n",
    "        out = torch.einsum(\"ijh,jhf->ihf\", alpha, Wh)     # [N, H, F]\n",
    "        out = out.reshape(N, self.heads * self.out_dim) if self.concat else out.mean(dim=1)\n",
    "        out = self.dropout(out)\n",
    "        if self.residual:\n",
    "            out = out + self.res_proj(x)                  # self-loop 없는 대신 residual로 자기정보 유지\n",
    "        return out\n",
    "\n",
    "class GATEncoder(nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim=64, out_dim=768, heads1=4, heads2=4, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.gat1 = SimpleGATLayer(in_dim, hid_dim, heads=heads1, concat=True,  dropout=dropout, residual=True)\n",
    "        self.gat2 = SimpleGATLayer(hid_dim*heads1, out_dim, heads=heads2, concat=False, dropout=dropout, residual=True)\n",
    "        self.act = nn.ELU(); self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x, adj):\n",
    "        h = self.gat1(x, adj); h = self.act(h); h = self.dropout(h)\n",
    "        z = self.gat2(h, adj)\n",
    "        return z  # [N, out_dim]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b080c703-91ce-4a83-b745-e87986fcddc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T07:21:12.561243Z",
     "iopub.status.busy": "2025-11-13T07:21:12.560929Z",
     "iopub.status.idle": "2025-11-13T07:21:12.564014Z",
     "shell.execute_reply": "2025-11-13T07:21:12.563522Z",
     "shell.execute_reply.started": "2025-11-13T07:21:12.561227Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1122124-d4c5-496b-9e54-9537c4e747bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T07:21:13.003253Z",
     "iopub.status.busy": "2025-11-13T07:21:13.002858Z",
     "iopub.status.idle": "2025-11-13T07:21:13.445206Z",
     "shell.execute_reply": "2025-11-13T07:21:13.444386Z",
     "shell.execute_reply.started": "2025-11-13T07:21:13.003219Z"
    }
   },
   "outputs": [],
   "source": [
    "hidden_dim=64\n",
    "out_dim=3466\n",
    "heads1=8\n",
    "heads2=8\n",
    "dropout=0.2\n",
    "epochs=200\n",
    "lr=1e-3\n",
    "weight_decay=5e-4\n",
    "neg_ratio=1.0\n",
    "eval_every=20\n",
    "use_full_graph_for_final=True\n",
    "pad_width=2\n",
    "normalize_out = True\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "X = np.vstack([label_embeddings[k] for k in label_keys]).astype(np.float32)\n",
    "X = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "\n",
    "N, d0 = X.shape\n",
    "\n",
    "\n",
    "gat = GATEncoder(in_dim=d0, hid_dim=hidden_dim, out_dim=out_dim, heads1=heads1, heads2=heads2, dropout=dropout).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3db08a7-6154-49f7-b54f-07b725579112",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T07:21:13.987559Z",
     "iopub.status.busy": "2025-11-13T07:21:13.987326Z",
     "iopub.status.idle": "2025-11-13T07:21:16.061546Z",
     "shell.execute_reply": "2025-11-13T07:21:16.060684Z",
     "shell.execute_reply.started": "2025-11-13T07:21:13.987541Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_docs_txt(path):\n",
    "    \"\"\"\n",
    "    'idx<TAB>text' 형태의 파일을 읽어서\n",
    "    ids: [int, ...]\n",
    "    texts: [str, ...]\n",
    "    을 리턴\n",
    "    \"\"\"\n",
    "    ids = []\n",
    "    texts = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            # 탭 기준\n",
    "            idx_str, txt = line.split(\"\\t\", 1)\n",
    "            ids.append(int(idx_str))\n",
    "            texts.append(txt)\n",
    "    return ids, texts\n",
    "\n",
    "\n",
    "\n",
    "def build_doc_embeddings_from_existing_vectorizer(doc_texts, vectorizer):\n",
    "    \"\"\"\n",
    "    doc_texts: 전처리 전의 원문 리스트\n",
    "    vectorizer: 라벨에 대해 fit되어 있는 TfidfVectorizer\n",
    "    return: dense numpy array [N_docs, vocab]\n",
    "    \"\"\"\n",
    "    # 라벨이랑 동일 규칙으로 전처리\n",
    "    cleaned_docs = [preprocess_label_text(t) for t in doc_texts]\n",
    "    doc_tfidf = vectorizer.transform(cleaned_docs)   # sparse\n",
    "    doc_emb = doc_tfidf.toarray().astype(np.float32)\n",
    "    return doc_emb\n",
    "\n",
    "# 사용 예시\n",
    "# 1) 문서 읽기\n",
    "doc_ids, doc_texts = load_docs_txt(\"Amazon_products/train/train_corpus.txt\")\n",
    "\n",
    "# 2) 라벨 때 만든 vectorizer 재사용해서 임베딩 만들기\n",
    "doc_embeddings = build_doc_embeddings_from_existing_vectorizer(doc_texts, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bd3ba19-4969-452b-9bbe-e2cdbfce97e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T07:21:21.910171Z",
     "iopub.status.busy": "2025-11-13T07:21:21.909837Z",
     "iopub.status.idle": "2025-11-13T07:21:21.913160Z",
     "shell.execute_reply": "2025-11-13T07:21:21.912650Z",
     "shell.execute_reply.started": "2025-11-13T07:21:21.910158Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29487, 3466)\n"
     ]
    }
   ],
   "source": [
    "print(doc_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cd05771-6906-4acf-8372-ba0bfd7d4a19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T07:21:28.699951Z",
     "iopub.status.busy": "2025-11-13T07:21:28.699725Z",
     "iopub.status.idle": "2025-11-13T07:21:28.708457Z",
     "shell.execute_reply": "2025-11-13T07:21:28.708017Z",
     "shell.execute_reply.started": "2025-11-13T07:21:28.699935Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy as np\n",
    "\n",
    "def hierarchical_beam_similarity_avg(\n",
    "    doc_vec: np.ndarray,\n",
    "    label_emb: np.ndarray,\n",
    "    adj_upper: np.ndarray,\n",
    "    roots: list[int] = [0],       # 여러 루트\n",
    "    beam: int = 5,\n",
    "    per_parent: str | int = \"l+2\",\n",
    "    tau: float = 0.35,\n",
    "    eps: float = 1e-9,\n",
    "    max_depth: int | None = None,\n",
    "    normalize: bool = False,      # 필요하면 True로\n",
    "):\n",
    "    doc = np.asarray(doc_vec, dtype=np.float32)\n",
    "    L = np.asarray(label_emb, dtype=np.float32)\n",
    "    A = np.asarray(adj_upper).astype(bool)\n",
    "    N, d = L.shape\n",
    "\n",
    "    if normalize:\n",
    "        doc = doc / (np.linalg.norm(doc) + eps)\n",
    "        L = L / (np.linalg.norm(L, axis=1, keepdims=True) + eps)\n",
    "\n",
    "    # 로컬 점수\n",
    "    sims = L @ doc\n",
    "    p = 1.0 / (1.0 + np.exp(-sims / max(tau, 1e-6)))\n",
    "\n",
    "    children = [np.flatnonzero(A[i]) for i in range(N)]\n",
    "\n",
    "    S = np.full(N, -np.inf, dtype=np.float32)\n",
    "    K = np.full(N, -np.inf, dtype=np.float32)\n",
    "    Llen = np.zeros(N, dtype=np.int32)\n",
    "\n",
    "    roots = list(roots)\n",
    "    for r in roots:\n",
    "        S[r] = 0.0\n",
    "        Llen[r] = 0\n",
    "        K[r] = -np.inf\n",
    "\n",
    "    levels = [roots[:]]\n",
    "    cur = roots[:]\n",
    "    level_id = 0\n",
    "\n",
    "    while True:\n",
    "        cand_best = {}\n",
    "        k_parent = (level_id + 2) if (per_parent == \"l+2\") else int(per_parent)\n",
    "\n",
    "        for par in cur:\n",
    "            ch = children[par]\n",
    "            if ch.size == 0:\n",
    "                continue\n",
    "            if ch.size > k_parent:\n",
    "                idx = np.argpartition(-sims[ch], k_parent - 1)[:k_parent]\n",
    "                ch = ch[idx]\n",
    "            for c in ch:\n",
    "                S_c = S[par] + float(p[c])\n",
    "                L_c = Llen[par] + 1\n",
    "                K_c = S_c / (L_c + eps)\n",
    "                if (c not in cand_best) or (K_c > cand_best[c][2]):\n",
    "                    cand_best[c] = (S_c, L_c, K_c)\n",
    "\n",
    "        if not cand_best:\n",
    "            break\n",
    "\n",
    "        kept = sorted(cand_best.items(), key=lambda x: x[1][2], reverse=True)[:min(beam, len(cand_best))]\n",
    "        next_level = [i for i, _ in kept]\n",
    "        for i, (Si, Li, Ki) in kept:\n",
    "            S[i], Llen[i], K[i] = Si, Li, Ki\n",
    "\n",
    "        levels.append(next_level)\n",
    "        cur = next_level\n",
    "        level_id += 1\n",
    "        if max_depth is not None and level_id >= max_depth:\n",
    "            break\n",
    "\n",
    "    return K, levels, sims, p\n",
    "\n",
    "\n",
    "\n",
    "def topk_labels_by_avg(\n",
    "    doc_vec, label_emb, adj_upper, rootㄴ=(0,), beam=5, per_parent=\"l+2\", k=5, **kw\n",
    "):\n",
    "    \"\"\"평균 점수 기반 최종 상위 k 라벨(루트 제외).\"\"\"\n",
    "    K, levels, sims, p = hierarchical_beam_similarity_avg(\n",
    "        doc_vec, label_emb, adj_upper, root=list(roots), beam=beam, per_parent=per_parent, **kw\n",
    "    )\n",
    "    root_set = set(roots)\n",
    "    order = np.argsort(-K)\n",
    "    order = [i for i in order if i not in root_set and np.isfinite(K[i])]\n",
    "    top = order[:k]\n",
    "    return top, K[top]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb5f7b9f-1627-4a14-aca0-769891ef07db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T04:06:01.589232Z",
     "iopub.status.busy": "2025-11-11T04:06:01.588974Z",
     "iopub.status.idle": "2025-11-11T04:06:01.597957Z",
     "shell.execute_reply": "2025-11-11T04:06:01.597468Z",
     "shell.execute_reply.started": "2025-11-11T04:06:01.589213Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e4244ac-1ed8-470c-bc63-1470f460ea33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T05:54:57.472605Z",
     "iopub.status.busy": "2025-11-13T05:54:57.472115Z",
     "iopub.status.idle": "2025-11-13T05:54:57.487006Z",
     "shell.execute_reply": "2025-11-13T05:54:57.486564Z",
     "shell.execute_reply.started": "2025-11-13T05:54:57.472588Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Self-training pipeline with hierarchical silver labeling and dynamic dataloaders.\n",
    "\n",
    "- Reads document/label embeddings CSVs (first column \"id\", rest feat000..feat127)\n",
    "- Reads upper-triangular adjacency (A[i,j]=1 means i->j)\n",
    "- Makes initial silver labels via hierarchical beam search (average score)\n",
    "- Splits into train/val on silver set; keeps the rest as unlabeled pool\n",
    "- Trains a multi-label classifier (Linear/MLP) with BCEWithLogitsLoss\n",
    "- Each epoch, pseudo-labels unlabeled docs whose predicted probs exceed a threshold\n",
    "- Adds them to the training set (up to top_k per doc), with patience-based early stopping\n",
    "\n",
    "Run example\n",
    "-----------\n",
    "python self_training_pipeline.py \\\n",
    "  --doc_csv docs.csv \\\n",
    "  --label_csv labels.csv \\\n",
    "  --adj adj.npy \\\n",
    "  --val_ratio 0.2 --epochs 50 --patience 5 \\\n",
    "  --silver_threshold 0.60 --silver_topk 3 --beam 5 --tau 0.35 --root_id 0 \\\n",
    "  --pseudo_threshold 0.70 --pseudo_topk 3 --batch_size 256 --lr 1e-3\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import argparse\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "def load_embeddings_csv(path: str | Path, id_col: str = \"id\") -> Tuple[List[int], np.ndarray]:\n",
    "    \"\"\"Load embeddings from CSV where the first column is an id and the rest are feature columns.\n",
    "    Returns (ids, float32 matrix).\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    cols = list(df.columns)\n",
    "    if id_col in df.columns:\n",
    "        id_series = df[id_col]\n",
    "        X = df.drop(columns=[id_col])\n",
    "    else:\n",
    "        # Fallback: use the first column as id\n",
    "        id_series = df.iloc[:, 0]\n",
    "        X = df.iloc[:, 1:]\n",
    "    ids = id_series.astype(int).tolist()\n",
    "    X = X.to_numpy(dtype=np.float32)\n",
    "    return ids, X\n",
    "\n",
    "\n",
    "# ----------------------------- Datasets -----------------------------\n",
    "\n",
    "class MultiLabelDataset(Dataset):\n",
    "    def __init__(self, X: np.ndarray, Y: np.ndarray, indices: List[int] | None = None):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.indices = np.array(indices if indices is not None else np.arange(X.shape[0]), dtype=np.int64)\n",
    "    def __len__(self):\n",
    "        return self.indices.shape[0]\n",
    "    def __getitem__(self, idx: int):\n",
    "        i = int(self.indices[idx])\n",
    "        x = torch.from_numpy(self.X[i])\n",
    "        y = torch.from_numpy(self.Y[i])\n",
    "        return x, y\n",
    "\n",
    "class UnlabeledDataset(Dataset):\n",
    "    def __init__(self, X: np.ndarray, indices: List[int]):\n",
    "        self.X = X\n",
    "        self.indices = np.array(indices, dtype=np.int64)\n",
    "    def __len__(self):\n",
    "        return self.indices.shape[0]\n",
    "    def __getitem__(self, idx: int):\n",
    "        i = int(self.indices[idx])\n",
    "        x = torch.from_numpy(self.X[i])\n",
    "        return x, i\n",
    "\n",
    "# ----------------------------- Model -----------------------------\n",
    "\n",
    "class MLPHead(nn.Module):\n",
    "    def __init__(self, in_dim: int, out_dim: int, hidden: int | None = 256, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        if hidden is None or hidden <= 0:\n",
    "            self.net = nn.Sequential(\n",
    "                nn.LayerNorm(in_dim),\n",
    "                nn.Linear(in_dim, out_dim),\n",
    "            )\n",
    "        else:\n",
    "            self.net = nn.Sequential(\n",
    "                nn.LayerNorm(in_dim),\n",
    "                nn.Linear(in_dim, hidden),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(hidden, out_dim),\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# ----------------------------- Utils -----------------------------\n",
    "\n",
    "def to_device(batch, device):\n",
    "    if isinstance(batch, (tuple, list)):\n",
    "        return [b.to(device) if torch.is_tensor(b) else b for b in batch]\n",
    "    return batch.to(device)\n",
    "\n",
    "\n",
    "def micro_f1(y_true: np.ndarray, y_prob: np.ndarray, thr: float = 0.5, eps: float = 1e-9) -> float:\n",
    "    y_pred = (y_prob >= thr).astype(np.float32)\n",
    "    tp = (y_true * y_pred).sum()\n",
    "    fp = ((1 - y_true) * y_pred).sum()\n",
    "    fn = (y_true * (1 - y_pred)).sum()\n",
    "    prec = tp / (tp + fp + eps)\n",
    "    rec = tp / (tp + fn + eps)\n",
    "    f1 = 2 * prec * rec / (prec + rec + eps)\n",
    "    return float(f1)\n",
    "\n",
    "# -------- Initial silver labeling (no CSV save; in-memory) --------\n",
    "def make_initial_silver_hier(\n",
    "    docs: np.ndarray,\n",
    "    labels: np.ndarray,\n",
    "    adj: np.ndarray,\n",
    "    roots: list[int] = [0],\n",
    "    silver_threshold: float = 0.6,    # 이건 avg(K) 기준\n",
    "    silver_topk: int = 3,\n",
    "    beam: int = 5,\n",
    "    per_parent: str | int = \"l+2\",\n",
    "    tau: float = 0.35,\n",
    ") -> list[list[int]]:\n",
    "    \"\"\"\n",
    "    계층 빔 서치로 각 문서의 라벨 후보를 뽑는다.\n",
    "    - 계층 밖 라벨은 애초에 안 들어옴\n",
    "    - 루트들은 결과에서 제외\n",
    "    - K(경로 평균) >= silver_threshold 인 애들 중 top-k\n",
    "    \"\"\"\n",
    "    N = labels.shape[0]\n",
    "    silver: list[list[int]] = []\n",
    "    root_set = set(roots)\n",
    "\n",
    "    for d in docs:\n",
    "        K, levels, sims, p = hierarchical_beam_similarity_avg(\n",
    "            d, labels, adj,\n",
    "            roots=roots,\n",
    "            beam=beam,\n",
    "            per_parent=per_parent,\n",
    "            tau=tau,\n",
    "            normalize=False,   # 너 임베딩이 이미 L2라면 False\n",
    "        )\n",
    "        # 평균 점수로 정렬\n",
    "        order = np.argsort(-K)\n",
    "        # 루트는 제외, 유한한 것만\n",
    "        order = [i for i in order if (i not in root_set) and np.isfinite(K[i])]\n",
    "        # threshold 통과한 것만\n",
    "        cand = [i for i in order if K[i] >= silver_threshold]\n",
    "        selected = cand[:silver_topk]\n",
    "        silver.append(selected)\n",
    "\n",
    "    return silver\n",
    "\n",
    "def make_initial_silver(\n",
    "    docs: np.ndarray,\n",
    "    labels: np.ndarray,\n",
    "    adj: np.ndarray,              # 이제 안 씀 (호환용으로만 둠)\n",
    "    silver_threshold: float = 0.9,\n",
    "    silver_topk: int = 3,\n",
    "    beam: int = 5,                # 이제 안 씀\n",
    "    tau: float = 0.35,\n",
    "    root_id: int = 0,\n",
    ") -> List[List[int]]:\n",
    "    \"\"\"\n",
    "    문서마다 전 라벨 임베딩과의 유사도를 보고 초기 silver label을 만든다.\n",
    "    - 트리/경로 탐색 안 함\n",
    "    - root_id는 결과에서 제외\n",
    "    - p >= silver_threshold인 라벨 중에서 상위 silver_topk만 남김\n",
    "    \"\"\"\n",
    "    N = labels.shape[0]\n",
    "    silver: List[List[int]] = []\n",
    "\n",
    "    for d in docs:\n",
    "        # 문서 vs 모든 라벨 점수\n",
    "        sims, p = all_label_similarity(d, labels, tau=tau, normalize=True)\n",
    "\n",
    "        # threshold 통과 + root 제외\n",
    "        cand = [\n",
    "            (i, float(p[i]))\n",
    "            for i in range(N)\n",
    "            if i != root_id and np.isfinite(p[i]) and p[i] >= silver_threshold\n",
    "        ]\n",
    "\n",
    "        # 점수 높은 순\n",
    "        cand.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # label index만 추출\n",
    "        selected = [i for i, _ in cand[:silver_topk]]\n",
    "        silver.append(selected)\n",
    "\n",
    "    return silver\n",
    "# ------------------------ Train / Self-Training ------------------------\n",
    "\n",
    "def train_epoch(model, loader, optim, device, criterion):\n",
    "    model.train()\n",
    "    total = 0.0\n",
    "    for x, y in loader:\n",
    "        x, y = to_device(x, device), to_device(y, device)\n",
    "        optim.zero_grad(set_to_none=True)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        total += float(loss.detach().cpu().item()) * x.size(0)\n",
    "    return total / max(1, len(loader.dataset))\n",
    "\n",
    "\n",
    "def eval_epoch(model, loader, device, criterion, thr=0.5):\n",
    "    model.eval()\n",
    "    total = 0.0\n",
    "    ys = []\n",
    "    ps = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = to_device(x, device), to_device(y, device)\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            total += float(loss.detach().cpu().item()) * x.size(0)\n",
    "            prob = torch.sigmoid(logits).detach().cpu().numpy()\n",
    "            ys.append(y.detach().cpu().numpy())\n",
    "            ps.append(prob)\n",
    "    y_true = np.concatenate(ys, axis=0)\n",
    "    y_prob = np.concatenate(ps, axis=0)\n",
    "    f1 = micro_f1(y_true, y_prob, thr=thr)\n",
    "    return total / max(1, len(loader.dataset)), f1, y_prob\n",
    "\n",
    "\n",
    "def pseudo_label_and_grow(model, unl_ds: UnlabeledDataset,\n",
    "                          num_labels: int,\n",
    "                          pseudo_threshold: float = 0.9, pseudo_topk: int = 3,\n",
    "                          device: str = \"cpu\", batch_size: int = 512):\n",
    "    \"\"\"Infer on unlabeled, select labels with prob>=threshold (top-k), and return new_indices and Y matrix.\"\"\"\n",
    "    if len(unl_ds) == 0:\n",
    "        return [], np.zeros((0, num_labels), dtype=np.float32)\n",
    "    loader = DataLoader(unl_ds, batch_size=batch_size, shuffle=False)\n",
    "    model.eval()\n",
    "    all_idx: List[int] = []\n",
    "    all_y: List[np.ndarray] = []\n",
    "    with torch.no_grad():\n",
    "        for xb, idxs in loader:\n",
    "            xb = xb.to(device)\n",
    "            logits = model(xb)\n",
    "            prob = torch.sigmoid(logits).detach().cpu().numpy()\n",
    "            for p, i in zip(prob, idxs.numpy().tolist()):\n",
    "                sel = np.flatnonzero(p >= pseudo_threshold)\n",
    "                if sel.size > 0:\n",
    "                    # keep at most top-k by prob\n",
    "                    if sel.size > pseudo_topk:\n",
    "                        top = np.argpartition(-p[sel], pseudo_topk - 1)[:pseudo_topk]\n",
    "                        sel = sel[top]\n",
    "                    y = np.zeros(num_labels, dtype=np.float32)\n",
    "                    y[sel] = 1.0\n",
    "                    all_idx.append(int(i))\n",
    "                    all_y.append(y)\n",
    "    if len(all_idx) == 0:\n",
    "        return [], np.zeros((0, num_labels), dtype=np.float32)\n",
    "    Y_new = np.stack(all_y, axis=0)\n",
    "    return all_idx, Y_new\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cccc039-f93f-49de-8843-19c7b5e4338d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d15867a-f278-41ab-88a0-e751c62e5873",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0b4cfc-c593-4841-b847-2759ad663984",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4c63ecfd-663d-4ed8-88b2-e8ecbc14de5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T07:06:48.510783Z",
     "iopub.status.busy": "2025-11-13T07:06:48.510550Z",
     "iopub.status.idle": "2025-11-13T07:07:35.266689Z",
     "shell.execute_reply": "2025-11-13T07:07:35.266144Z",
     "shell.execute_reply.started": "2025-11-13T07:06:48.510766Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "doc_ids = np.arange(len(doc_embeddings), dtype=np.int64)   # 0..num_docs-1\n",
    "X = doc_embeddings.astype(np.float32)                      # [num_docs, d_doc]\n",
    "\n",
    "# 라벨 임베딩 세팅\n",
    "label_ids = np.arange(len(label_keys), dtype=np.int64)     # 0..530\n",
    "L = np.vstack([label_embeddings[k] for k in label_keys]).astype(np.float32)   # [531, d_label]\n",
    "\n",
    "\n",
    "# 1) 라벨 순서와 B(부모->자식) 맞추기\n",
    "order = np.argsort(label_ids)\n",
    "label_ids = [label_ids[i] for i in order]\n",
    "L = L[order]\n",
    "assert B.shape == (L.shape[0], L.shape[0]), \"Adjacency/label size mismatch\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 2) 계층 silver 만들기\n",
    "silver = make_initial_silver_hier(\n",
    "    X,          # docs (N, d)\n",
    "    L,          # label_emb (C, d)\n",
    "    B,          # upper adj (C, C)\n",
    "    roots=roots,\n",
    "    silver_threshold=0.6,\n",
    "    silver_topk=3,\n",
    "    beam=5,\n",
    "    per_parent=\"l+2\",\n",
    "    tau=0.35,\n",
    ")\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 1) 계층 정보에서 parents / children 뽑기\n",
    "#    B[parent, child] = 1 이라고 했으니까 그대로 씀\n",
    "# -------------------------------------------------\n",
    "# B: [C, C] (parent -> child)\n",
    "def build_parents_children(adj):\n",
    "    C = adj.shape[0]\n",
    "    parents = [np.flatnonzero(adj[:, j]).astype(np.int64) for j in range(C)]\n",
    "    children = [np.flatnonzero(adj[j]).astype(np.int64) for j in range(C)]\n",
    "    return parents, children\n",
    "\n",
    "parents, children = build_parents_children(B)\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2) silver → 계층 pos/neg 마스크로 변환\n",
    "# -------------------------------------------------\n",
    "def build_pos_neg_masks(silver, parents, children, num_labels):\n",
    "    \"\"\"\n",
    "    silver: list[list[int]]  # 문서마다 core label index들\n",
    "    parents / children: list[np.ndarray]\n",
    "    return:\n",
    "      pos_masks: np.array [N_docs, C]\n",
    "      neg_masks: np.array [N_docs, C]\n",
    "    \"\"\"\n",
    "    N = len(silver)\n",
    "    C = num_labels\n",
    "    pos_masks = np.zeros((N, C), dtype=np.float32)\n",
    "    neg_masks = np.zeros((N, C), dtype=np.float32)\n",
    "\n",
    "    all_idx = np.arange(C)\n",
    "\n",
    "    for i, core in enumerate(silver):\n",
    "        core = list(core)\n",
    "        # 1) core의 부모까지 positive\n",
    "        pos_set = set(core)\n",
    "        for c in core:\n",
    "            for p in parents[c]:\n",
    "                pos_set.add(int(p))\n",
    "\n",
    "        # 2) children은 나중에 negative에서 제외\n",
    "        child_set = set()\n",
    "        for c in core:\n",
    "            for ch in children[c]:\n",
    "                child_set.add(int(ch))\n",
    "\n",
    "        # pos 마스크\n",
    "        for p in pos_set:\n",
    "            pos_masks[i, p] = 1.0\n",
    "\n",
    "        # neg = 전체 - pos - children\n",
    "        for j in all_idx:\n",
    "            if j in pos_set:\n",
    "                continue\n",
    "            if j in child_set:\n",
    "                continue\n",
    "            neg_masks[i, j] = 1.0\n",
    "\n",
    "    return pos_masks, neg_masks\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3) Dataset: 문서 임베딩 + pos/neg 마스크\n",
    "# -------------------------------------------------\n",
    "class HierMultiLabelDataset(Dataset):\n",
    "    def __init__(self, X, pos_masks, neg_masks, indices=None):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.pos = pos_masks.astype(np.float32)\n",
    "        self.neg = neg_masks.astype(np.float32)\n",
    "        if indices is None:\n",
    "            self.indices = np.arange(self.X.shape[0], dtype=np.int64)\n",
    "        else:\n",
    "            self.indices = np.array(indices, dtype=np.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.indices.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i = int(self.indices[idx])\n",
    "        x = torch.from_numpy(self.X[i])\n",
    "        pos = torch.from_numpy(self.pos[i])\n",
    "        neg = torch.from_numpy(self.neg[i])\n",
    "        return x, pos, neg\n",
    "\n",
    "class UnlabeledDataset(Dataset):\n",
    "    def __init__(self, X, indices):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.indices = np.array(indices, dtype=np.int64)\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    def __getitem__(self, idx):\n",
    "        i = int(self.indices[idx])\n",
    "        return torch.from_numpy(self.X[i]), i\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ec157776-e172-4c16-93a1-aacdf0a962d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T07:07:35.267569Z",
     "iopub.status.busy": "2025-11-13T07:07:35.267425Z",
     "iopub.status.idle": "2025-11-13T07:07:36.796105Z",
     "shell.execute_reply": "2025-11-13T07:07:36.795503Z",
     "shell.execute_reply.started": "2025-11-13T07:07:35.267556Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 29487\n",
      "with silver: 12014\n",
      "unlabeled : 17473\n",
      "9612 2402 17473\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -------------------------------------------------\n",
    "# 4) Bilinear classifier\n",
    "#    doc_emb: [B, d_doc]\n",
    "#    label_emb: [C, d_lab]  (미리 GAT로 만든 거)\n",
    "#    점수: doc @ W @ label_emb^T\n",
    "# -------------------------------------------------\n",
    "class GATHierClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        doc_dim,\n",
    "        gat_encoder,       # 미리 만들어서 넣는 GATEncoder\n",
    "        label_features,    # GAT 입력 X_label: [C, d0]  (TF-IDF label emb 등)\n",
    "        label_adj,         # 라벨 그래프 A: [C, C]\n",
    "        hidden_dim=None,\n",
    "        normalize_label=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.gat = gat_encoder\n",
    "        self.normalize_label = normalize_label\n",
    "        dev = next(gat_encoder.parameters()).device  # 보통 cuda:0\n",
    "\n",
    "        # label_features / adj는 매 번 같으니까 buffer로 들고 있음\n",
    "        self.register_buffer(\n",
    "            \"label_feat\",\n",
    "            torch.tensor(label_features, dtype=torch.float32, device=dev)\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"label_adj\",\n",
    "            torch.tensor(label_adj, dtype=torch.float32, device=dev)\n",
    "        )\n",
    "\n",
    "        # 라벨 임베딩 차원(d_lab)은 GAT의 out_dim이랑 같아야 함\n",
    "        # ⇒ GATEncoder(out_dim=...)에서 쓴 값.\n",
    "        # 여기서는 한 번 더 forward 없이, gat_encoder의 설정을 알고 있다고 가정.\n",
    "        # 예: out_dim=3466이었다면 그걸 그대로 넣어줬다고 가정.\n",
    "        # 편하게 하려면 아래처럼 실제로 한 번 뽑아서 써도 됨:\n",
    "        with torch.no_grad():\n",
    "            z_sample = self.gat(self.label_feat, self.label_adj)  # [C, d_lab]\n",
    "        d_lab = z_sample.size(1)\n",
    "\n",
    "        self.doc_dim = doc_dim\n",
    "        self.label_dim = d_lab\n",
    "\n",
    "        if hidden_dim is None:\n",
    "            self.interaction = nn.Linear(doc_dim, d_lab, bias=False)\n",
    "        else:\n",
    "            self.interaction = nn.Sequential(\n",
    "                nn.Linear(doc_dim, hidden_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(hidden_dim, d_lab, bias=False),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [B, d_doc]\n",
    "        return: logits [B, C]\n",
    "        \"\"\"\n",
    "        # 1) 현재 GAT로부터 라벨 임베딩 계산\n",
    "        label_emb = self.gat(self.label_feat, self.label_adj)  # [C, d_lab]\n",
    "        if self.normalize_label:\n",
    "            label_emb = F.normalize(label_emb, p=2, dim=1)\n",
    "\n",
    "        # 2) 문서 → 라벨 공간으로 투영\n",
    "        h = self.interaction(x)                                # [B, d_lab]\n",
    "\n",
    "        # 3) bilinear 점수\n",
    "        logits = torch.matmul(h, label_emb.t())                # [B, C]\n",
    "        return logits\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 5) loss: 계층 마스크를 씌운 BCE\n",
    "# -------------------------------------------------\n",
    "def hierarchical_bce_loss(logits, pos_mask, neg_mask):\n",
    "    # logits: [B, C]\n",
    "    # pos_mask, neg_mask: [B, C]\n",
    "    loss_pos = -(pos_mask * F.logsigmoid(logits)).sum()\n",
    "    loss_neg = -(neg_mask * F.logsigmoid(-logits)).sum()\n",
    "    denom = (pos_mask.sum() + neg_mask.sum()).clamp(min=1.0)\n",
    "    return (loss_pos + loss_neg) / denom\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 6) 학습 루프 예시\n",
    "# -------------------------------------------------\n",
    "# 이미 있는 것들: X (문서 BERT 임베딩) : [N_docs, d_doc]\n",
    "#                  L (라벨 GAT 임베딩)  : [C, d_lab]\n",
    "#                  B_adj (부모->자식)   : [C, C]\n",
    "#                  silver (list[list[int]]) : 문서별 core label index\n",
    "def train_epoch_hier(model, loader, opt, device):\n",
    "    model.train()\n",
    "    total = 0.0\n",
    "    for xb, posb, negb in loader:\n",
    "        xb = xb.to(device)\n",
    "        posb = posb.to(device)\n",
    "        negb = negb.to(device)\n",
    "        logits = model(xb)\n",
    "        loss = hierarchical_bce_loss(logits, posb, negb)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        total += loss.item() * xb.size(0)\n",
    "    return total / len(loader.dataset)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 1) micro F1 계산\n",
    "def micro_f1_from_logits(logits, pos_mask, thr=0.5, eps=1e-9):\n",
    "    \"\"\"\n",
    "    logits: [B, C]\n",
    "    pos_mask: [B, C]  (1: positive, 0: else)\n",
    "    \"\"\"\n",
    "    probs = torch.sigmoid(logits)\n",
    "    preds = (probs >= thr).float()\n",
    "\n",
    "    y_true = pos_mask\n",
    "    y_pred = preds\n",
    "\n",
    "    tp = (y_true * y_pred).sum()\n",
    "    fp = ((1 - y_true) * y_pred).sum()\n",
    "    fn = (y_true * (1 - y_pred)).sum()\n",
    "\n",
    "    precision = tp / (tp + fp + eps)\n",
    "    recall    = tp / (tp + fn + eps)\n",
    "    f1 = 2 * precision * recall / (precision + recall + eps)\n",
    "    return f1.item()\n",
    "\n",
    "# 2) eval 함수 수정: loss + f1 둘 다\n",
    "def eval_epoch_hier(model, loader, device, k=3, thr=None):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    f1_list = []\n",
    "    with torch.no_grad():\n",
    "        for xb, posb, negb in loader:\n",
    "            xb = xb.to(device)\n",
    "            posb = posb.to(device)\n",
    "            negb = negb.to(device)\n",
    "\n",
    "            logits = model(xb)\n",
    "            loss = hierarchical_bce_loss(logits, posb, negb)  # 위에 바꾼 버전\n",
    "            total_loss += loss.item() * xb.size(0)\n",
    "\n",
    "            probs = torch.sigmoid(logits)\n",
    "\n",
    "            if thr is not None:\n",
    "                pred = (probs >= thr).float()\n",
    "            else:\n",
    "                # top-k 방식\n",
    "                B, C = probs.shape\n",
    "                pred = torch.zeros_like(probs)\n",
    "                topk = probs.topk(k, dim=1).indices\n",
    "                pred.scatter_(1, topk, 1.0)\n",
    "\n",
    "            # micro-f1\n",
    "            y_true = posb\n",
    "            y_pred = pred\n",
    "            tp = (y_true * y_pred).sum().item()\n",
    "            fp = ((1 - y_true) * y_pred).sum().item()\n",
    "            fn = (y_true * (1 - y_pred)).sum().item()\n",
    "            prec = tp / (tp + fp + 1e-9)\n",
    "            rec  = tp / (tp + fn + 1e-9)\n",
    "            f1   = 2 * prec * rec / (prec + rec + 1e-9)\n",
    "            f1_list.append(f1)\n",
    "\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    avg_f1 = float(np.mean(f1_list)) if f1_list else 0.0\n",
    "    return avg_loss, avg_f1\n",
    "def pseudo_label_and_grow_hier(\n",
    "    model,\n",
    "    unl_ds,             # UnlabeledDataset\n",
    "    X_all,              # 전체 문서 임베딩 (numpy)\n",
    "    parents, children,\n",
    "    num_labels,\n",
    "    device,\n",
    "    pseudo_threshold=0.45,\n",
    "    pseudo_topk=3,\n",
    "    batch_size=512,\n",
    "):\n",
    "    if len(unl_ds) == 0:\n",
    "        return [], None, None\n",
    "\n",
    "    loader = DataLoader(unl_ds, batch_size=batch_size, shuffle=False)\n",
    "    model.eval()\n",
    "    new_idx = []\n",
    "    new_pos_list = []\n",
    "    new_neg_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, idxs in loader:\n",
    "            xb = xb.to(device)\n",
    "            logits = model(xb)\n",
    "            prob = torch.sigmoid(logits).cpu().numpy()\n",
    "\n",
    "            for p, i_doc in zip(prob, idxs.numpy().tolist()):\n",
    "                order = np.argsort(-p)\n",
    "                top1 = p[order[0]]\n",
    "                # 1) top-1이 threshold를 못 넘으면 그냥 버린다\n",
    "                if top1 < pseudo_threshold:\n",
    "                    continue\n",
    "                core = [j for j in order if p[j] >= pseudo_threshold][:pseudo_topk]\n",
    "                if len(core) == 0:\n",
    "                    # 아예 이 문서는 이번 epoch에 안 넣음\n",
    "                    continue\n",
    "\n",
    "                # 계층 pos/neg 구성\n",
    "                pos = set(core)\n",
    "                for c in core:\n",
    "                    for pa in parents[c]:\n",
    "                        pos.add(int(pa))\n",
    "                child = set()\n",
    "                for c in core:\n",
    "                    for ch in children[c]:\n",
    "                        child.add(int(ch))\n",
    "\n",
    "                pos_mask = np.zeros(num_labels, dtype=np.float32)\n",
    "                neg_mask = np.zeros(num_labels, dtype=np.float32)\n",
    "                for j in pos:\n",
    "                    pos_mask[j] = 1.0\n",
    "                for j in range(num_labels):\n",
    "                    if j in pos:    # 이미 양성\n",
    "                        continue\n",
    "                    if j in child:  # 모르겠음 → negative에서 제외\n",
    "                        continue\n",
    "                    neg_mask[j] = 1.0\n",
    "\n",
    "                new_idx.append(int(i_doc))\n",
    "                new_pos_list.append(pos_mask)\n",
    "                new_neg_list.append(neg_mask)\n",
    "\n",
    "\n",
    "\n",
    "    if len(new_idx) == 0:\n",
    "        return [], None, None\n",
    "\n",
    "    new_pos = np.stack(new_pos_list, axis=0)\n",
    "    new_neg = np.stack(new_neg_list, axis=0)\n",
    "    return new_idx, new_pos, new_neg\n",
    "\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "has_silver = np.array([len(lbls) > 0 for lbls in silver], dtype=bool)\n",
    "N_docs = X.shape[0]\n",
    "C = L.shape[0]\n",
    "\n",
    "# silver 있는 문서 / 없는 문서\n",
    "has_silver = np.array([len(lbls) > 0 for lbls in silver], dtype=bool)\n",
    "idx_silver = np.flatnonzero(has_silver)      # 여기가 train/val 후보\n",
    "idx_unl    = np.flatnonzero(~has_silver)     # 진짜 unl\n",
    "\n",
    "print(\"total:\", N_docs)\n",
    "print(\"with silver:\", len(idx_silver))\n",
    "print(\"unlabeled :\", len(idx_unl))\n",
    "\n",
    "# 이제 train/val은 silver 있는 애들만 섞어서 나눈다\n",
    "rng = np.random.default_rng(42)\n",
    "rng.shuffle(idx_silver)\n",
    "n_val = int(len(idx_silver) * 0.2)\n",
    "idx_val   = idx_silver[:n_val]\n",
    "idx_train = idx_silver[n_val:]\n",
    "\n",
    "# parents, children 만들기\n",
    "def build_parents_children(adj):\n",
    "    C = adj.shape[0]\n",
    "    parents = [np.flatnonzero(adj[:, j]).astype(np.int64) for j in range(C)]\n",
    "    children = [np.flatnonzero(adj[j]).astype(np.int64) for j in range(C)]\n",
    "    return parents, children\n",
    "\n",
    "parents, children = build_parents_children(B)\n",
    "\n",
    "pos_masks = np.zeros((N_docs, C), dtype=np.float32)\n",
    "neg_masks = np.zeros((N_docs, C), dtype=np.float32)\n",
    "\n",
    "for i in idx_silver:  # silver 있는 애만 돈다\n",
    "    core = silver[i]\n",
    "\n",
    "    # 1) core + parents\n",
    "    pos = set(core)\n",
    "    for c in core:\n",
    "        for p in parents[c]:\n",
    "            pos.add(int(p))\n",
    "\n",
    "    # 2) children은 모름\n",
    "    child = set()\n",
    "    for c in core:\n",
    "        for ch in children[c]:\n",
    "            child.add(int(ch))\n",
    "\n",
    "    for p in pos:\n",
    "        pos_masks[i, p] = 1.0\n",
    "\n",
    "    for j in range(C):\n",
    "        if j in pos:      # 이미 양성\n",
    "            continue\n",
    "        if j in child:    # 모름\n",
    "            continue\n",
    "        neg_masks[i, j] = 1.0\n",
    "\n",
    "\n",
    "\n",
    "train_ds = HierMultiLabelDataset(X, pos_masks, neg_masks, indices=idx_train)\n",
    "val_ds   = HierMultiLabelDataset(X, pos_masks, neg_masks, indices=idx_val) if len(idx_val) > 0 else None\n",
    "unl_ds   = UnlabeledDataset(X, idx_unl.tolist())\n",
    "print(len(train_ds),len(val_ds),len(unl_ds))\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=256, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=256, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fbd03ade-045a-4c94-89e4-de3d1e523595",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T07:07:36.796774Z",
     "iopub.status.busy": "2025-11-13T07:07:36.796628Z",
     "iopub.status.idle": "2025-11-13T07:09:44.773524Z",
     "shell.execute_reply": "2025-11-13T07:09:44.772522Z",
     "shell.execute_reply.started": "2025-11-13T07:07:36.796760Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train_loss=0.139  val_loss=0.039  val_f1=0.164\n",
      "  + (skip pseudo-labeling on warmup epoch)\n",
      "Epoch 002 | train_loss=0.034  val_loss=0.031  val_f1=0.164\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 003 | train_loss=0.031  val_loss=0.030  val_f1=0.191\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 004 | train_loss=0.029  val_loss=0.028  val_f1=0.304\n",
      "  + Added 303 pseudo-labeled docs (unl pool → 17170 left)\n",
      "Epoch 005 | train_loss=0.026  val_loss=0.024  val_f1=0.355\n",
      "  + Added 1024 pseudo-labeled docs (unl pool → 16146 left)\n",
      "Epoch 006 | train_loss=0.023  val_loss=0.022  val_f1=0.385\n",
      "  + Added 730 pseudo-labeled docs (unl pool → 15416 left)\n",
      "Epoch 007 | train_loss=0.020  val_loss=0.019  val_f1=0.455\n",
      "  + Added 489 pseudo-labeled docs (unl pool → 14927 left)\n",
      "Epoch 008 | train_loss=0.018  val_loss=0.017  val_f1=0.508\n",
      "  + Added 810 pseudo-labeled docs (unl pool → 14117 left)\n",
      "Epoch 009 | train_loss=0.015  val_loss=0.015  val_f1=0.537\n",
      "  + Added 462 pseudo-labeled docs (unl pool → 13655 left)\n",
      "Epoch 010 | train_loss=0.013  val_loss=0.014  val_f1=0.566\n",
      "  + Added 416 pseudo-labeled docs (unl pool → 13239 left)\n",
      "Epoch 011 | train_loss=0.012  val_loss=0.012  val_f1=0.589\n",
      "  + Added 209 pseudo-labeled docs (unl pool → 13030 left)\n",
      "Epoch 012 | train_loss=0.010  val_loss=0.011  val_f1=0.603\n",
      "  + Added 325 pseudo-labeled docs (unl pool → 12705 left)\n",
      "Epoch 013 | train_loss=0.009  val_loss=0.011  val_f1=0.617\n",
      "  + Added 158 pseudo-labeled docs (unl pool → 12547 left)\n",
      "Epoch 014 | train_loss=0.008  val_loss=0.010  val_f1=0.627\n",
      "  + Added 243 pseudo-labeled docs (unl pool → 12304 left)\n",
      "Epoch 015 | train_loss=0.007  val_loss=0.009  val_f1=0.632\n",
      "  + Added 233 pseudo-labeled docs (unl pool → 12071 left)\n",
      "Epoch 016 | train_loss=0.007  val_loss=0.009  val_f1=0.638\n",
      "  + Added 135 pseudo-labeled docs (unl pool → 11936 left)\n",
      "Epoch 017 | train_loss=0.006  val_loss=0.009  val_f1=0.643\n",
      "  + Added 267 pseudo-labeled docs (unl pool → 11669 left)\n",
      "Epoch 018 | train_loss=0.006  val_loss=0.009  val_f1=0.645\n",
      "  + Added 243 pseudo-labeled docs (unl pool → 11426 left)\n",
      "Epoch 019 | train_loss=0.005  val_loss=0.009  val_f1=0.648\n",
      "  + Added 103 pseudo-labeled docs (unl pool → 11323 left)\n",
      "Epoch 020 | train_loss=0.005  val_loss=0.008  val_f1=0.649\n",
      "  + Added 238 pseudo-labeled docs (unl pool → 11085 left)\n",
      "Epoch 021 | train_loss=0.005  val_loss=0.008  val_f1=0.648\n",
      "  + Added 252 pseudo-labeled docs (unl pool → 10833 left)\n",
      "Epoch 022 | train_loss=0.005  val_loss=0.008  val_f1=0.652\n",
      "  + Added 141 pseudo-labeled docs (unl pool → 10692 left)\n",
      "Epoch 023 | train_loss=0.004  val_loss=0.008  val_f1=0.653\n",
      "  + Added 42 pseudo-labeled docs (unl pool → 10650 left)\n",
      "Epoch 024 | train_loss=0.004  val_loss=0.009  val_f1=0.653\n",
      "  + Added 301 pseudo-labeled docs (unl pool → 10349 left)\n",
      "Epoch 025 | train_loss=0.004  val_loss=0.009  val_f1=0.653\n",
      "  + Added 102 pseudo-labeled docs (unl pool → 10247 left)\n",
      "Epoch 026 | train_loss=0.004  val_loss=0.009  val_f1=0.653\n",
      "  + Added 91 pseudo-labeled docs (unl pool → 10156 left)\n",
      "Epoch 027 | train_loss=0.004  val_loss=0.009  val_f1=0.653\n",
      "  + Added 224 pseudo-labeled docs (unl pool → 9932 left)\n",
      "Epoch 028 | train_loss=0.004  val_loss=0.009  val_f1=0.655\n",
      "  + Added 103 pseudo-labeled docs (unl pool → 9829 left)\n",
      "Epoch 029 | train_loss=0.004  val_loss=0.009  val_f1=0.654\n",
      "  + Added 158 pseudo-labeled docs (unl pool → 9671 left)\n",
      "Epoch 030 | train_loss=0.003  val_loss=0.009  val_f1=0.654\n",
      "  + Added 104 pseudo-labeled docs (unl pool → 9567 left)\n",
      "Epoch 031 | train_loss=0.003  val_loss=0.009  val_f1=0.654\n",
      "  + Added 68 pseudo-labeled docs (unl pool → 9499 left)\n",
      "Epoch 032 | train_loss=0.003  val_loss=0.009  val_f1=0.653\n",
      "  + Added 55 pseudo-labeled docs (unl pool → 9444 left)\n",
      "Epoch 033 | train_loss=0.003  val_loss=0.009  val_f1=0.655\n",
      "  + Added 74 pseudo-labeled docs (unl pool → 9370 left)\n",
      "Epoch 034 | train_loss=0.003  val_loss=0.010  val_f1=0.654\n",
      "  + Added 199 pseudo-labeled docs (unl pool → 9171 left)\n",
      "Epoch 035 | train_loss=0.003  val_loss=0.010  val_f1=0.652\n",
      "  + Added 117 pseudo-labeled docs (unl pool → 9054 left)\n",
      "Epoch 036 | train_loss=0.003  val_loss=0.010  val_f1=0.655\n",
      "  + Added 57 pseudo-labeled docs (unl pool → 8997 left)\n",
      "Epoch 037 | train_loss=0.003  val_loss=0.010  val_f1=0.653\n",
      "  + Added 101 pseudo-labeled docs (unl pool → 8896 left)\n",
      "Epoch 038 | train_loss=0.003  val_loss=0.010  val_f1=0.652\n",
      "  + Added 60 pseudo-labeled docs (unl pool → 8836 left)\n",
      "Epoch 039 | train_loss=0.003  val_loss=0.010  val_f1=0.655\n",
      "  + Added 32 pseudo-labeled docs (unl pool → 8804 left)\n",
      "Epoch 040 | train_loss=0.003  val_loss=0.010  val_f1=0.652\n",
      "  + Added 66 pseudo-labeled docs (unl pool → 8738 left)\n",
      "Epoch 041 | train_loss=0.003  val_loss=0.011  val_f1=0.656\n",
      "  + Added 82 pseudo-labeled docs (unl pool → 8656 left)\n",
      "Epoch 042 | train_loss=0.003  val_loss=0.011  val_f1=0.651\n",
      "  + Added 117 pseudo-labeled docs (unl pool → 8539 left)\n",
      "Epoch 043 | train_loss=0.003  val_loss=0.011  val_f1=0.649\n",
      "  + Added 36 pseudo-labeled docs (unl pool → 8503 left)\n",
      "Epoch 044 | train_loss=0.002  val_loss=0.011  val_f1=0.649\n",
      "  + Added 82 pseudo-labeled docs (unl pool → 8421 left)\n",
      "Epoch 045 | train_loss=0.002  val_loss=0.011  val_f1=0.647\n",
      "  + Added 65 pseudo-labeled docs (unl pool → 8356 left)\n",
      "Epoch 046 | train_loss=0.002  val_loss=0.012  val_f1=0.650\n",
      "  + Added 43 pseudo-labeled docs (unl pool → 8313 left)\n",
      "Epoch 047 | train_loss=0.002  val_loss=0.011  val_f1=0.653\n",
      "  + Added 61 pseudo-labeled docs (unl pool → 8252 left)\n",
      "Epoch 048 | train_loss=0.002  val_loss=0.012  val_f1=0.651\n",
      "  + Added 55 pseudo-labeled docs (unl pool → 8197 left)\n",
      "Epoch 049 | train_loss=0.002  val_loss=0.012  val_f1=0.651\n",
      "  + Added 30 pseudo-labeled docs (unl pool → 8167 left)\n",
      "Epoch 050 | train_loss=0.002  val_loss=0.012  val_f1=0.652\n",
      "  + Added 55 pseudo-labeled docs (unl pool → 8112 left)\n",
      "Epoch 051 | train_loss=0.002  val_loss=0.012  val_f1=0.648\n",
      "Early stopping at epoch 51 (best f1=0.6558)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = GATHierClassifier(doc_dim=X.shape[1], gat_encoder = gat, label_features=L,label_adj = A, hidden_dim=512).to(device)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "epochs = 150\n",
    "\n",
    "N_labels = L.shape[0]\n",
    "best_f1 = -1.0\n",
    "patience = 10\n",
    "no_improve = 0\n",
    "warmup_self = 1   # 1 epoch은 self-training 안 하게 해서 한 번 안정화\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    # train\n",
    "    tr_loss = train_epoch_hier(model, train_loader, opt, device)\n",
    "    \n",
    "\n",
    "    # val: f1 기준\n",
    "    if val_loader is not None and len(val_ds) > 0:\n",
    "        va_loss, va_f1 = eval_epoch_hier(model, val_loader, device, k=3)\n",
    "        print(f\"Epoch {epoch:03d} | train_loss={tr_loss:.3f}  val_loss={va_loss:.3f}  val_f1={va_f1:.3f}\")\n",
    "\n",
    "        # early stopping을 f1로\n",
    "        if va_f1 > best_f1 + 1e-6:\n",
    "            best_f1 = va_f1\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch} (best f1={best_f1:.4f})\")\n",
    "                break\n",
    "    else:\n",
    "        print(f\"Epoch {epoch:03d} | train_loss={tr_loss:.3f}\")\n",
    "\n",
    "    # self-training: 1에폭에 전부 들어가는 거 방지용으로 warmup 넣음\n",
    "    if epoch <= warmup_self:\n",
    "        print(\"  + (skip pseudo-labeling on warmup epoch)\")\n",
    "        continue\n",
    "\n",
    "    new_idx, new_pos, new_neg = pseudo_label_and_grow_hier(\n",
    "        model,\n",
    "        unl_ds,\n",
    "        X,\n",
    "        parents,\n",
    "        children,\n",
    "        C,                   # num_labels\n",
    "        device=device,\n",
    "        pseudo_threshold=0.4,\n",
    "        pseudo_topk=3,\n",
    "        batch_size=512,\n",
    "    )\n",
    "\n",
    "    if len(new_idx) > 0:\n",
    "        # 전역 마스크 갱신\n",
    "        pos_masks[new_idx] = new_pos\n",
    "        neg_masks[new_idx] = new_neg\n",
    "\n",
    "        # unl에서 제거\n",
    "        keep_mask = ~np.isin(unl_ds.indices, np.array(new_idx, dtype=np.int64))\n",
    "        unl_ds.indices = unl_ds.indices[keep_mask]\n",
    "\n",
    "        # train에 추가\n",
    "        train_ds.indices = np.concatenate([train_ds.indices, np.array(new_idx, dtype=np.int64)])\n",
    "        train_loader = DataLoader(train_ds, batch_size=256, shuffle=True, drop_last=False)\n",
    "\n",
    "        print(f\"  + Added {len(new_idx)} pseudo-labeled docs (unl pool → {len(unl_ds)} left)\")\n",
    "    else:\n",
    "        print(\"  + No pseudo-labeled docs added this epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b88df0-bac0-49b4-9f58-d0c5e77583e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574d819b-4455-4b10-8541-4a9a9f2ddec4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7411b961-6f29-4ac7-ba7a-97098db48b1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T07:09:44.774736Z",
     "iopub.status.busy": "2025-11-13T07:09:44.774575Z",
     "iopub.status.idle": "2025-11-13T07:09:46.312390Z",
     "shell.execute_reply": "2025-11-13T07:09:46.311482Z",
     "shell.execute_reply.started": "2025-11-13T07:09:44.774721Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv, os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ------------ Paths (edit if needed) ------------\n",
    "TEST_CORPUS = \"Amazon_products/test/test_corpus.txt\"   # lines: pid \\t text\n",
    "OUT_PATH    = \"submission_bda.csv\"\n",
    "# ------------ Hyperparams ------------\n",
    "MIN_LABS  = 2\n",
    "MAX_LABS  = 3\n",
    "BATCH = 1024\n",
    "doc_ids, doc_texts = load_docs_txt(TEST_CORPUS)\n",
    "\n",
    "# 2) 라벨 때 만든 vectorizer 재사용해서 임베딩 만들기\n",
    "test_embeddings = build_doc_embeddings_from_existing_vectorizer(doc_texts, vectorizer)\n",
    "test_embeddings = test_embeddings.astype(np.float32)   # [num_test, d]\n",
    "# load test pids\n",
    "pids = doc_ids   # 이미 문자열 id\n",
    "if \"L\" in globals():\n",
    "    if not isinstance(L, np.ndarray):\n",
    "        # 예: L이 torch.Tensor인 경우\n",
    "        L = L.detach().cpu().numpy().astype(np.float32)\n",
    "else:\n",
    "    raise ValueError(\"라벨 임베딩 L이 메모리에 없어! GAT 끝난 뒤의 임베딩을 L로 둬야 해.\")\n",
    "\n",
    "# 5) 라벨 id는 0..N-1로 생성 (네가 말한 대로 adjacency랑 순서가 이미 맞다고 했으니까)\n",
    "lab_ids = np.arange(L.shape[0], dtype=np.int64)\n",
    "\n",
    "# 6) adjacency도 메모리에 있는 걸 그대로 쓴다\n",
    "#    여기서 A는 531x531 같은 numpy array라고 가정\n",
    "assert B.shape == (L.shape[0], L.shape[0]), \"Adjacency/label size mismatch\"\n",
    "\n",
    "# 7) children 리스트 미리 만들어두기\n",
    "children = [np.flatnonzero(B[i]) for i in range(B.shape[0])]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9ef344a9-c996-4932-b7ab-9f7c5e7578b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T07:09:46.313179Z",
     "iopub.status.busy": "2025-11-13T07:09:46.313018Z",
     "iopub.status.idle": "2025-11-13T07:10:00.797116Z",
     "shell.execute_reply": "2025-11-13T07:10:00.796563Z",
     "shell.execute_reply.started": "2025-11-13T07:09:46.313165Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: submission_bda.csv | samples=19658 | min-max labels per sample=2-3 | missing_pids=0\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "def ancestors_of(node, adj):\n",
    "    # adj[parent, child] = 1 가정\n",
    "    parents = np.flatnonzero(adj[:, node])  # (N,)\n",
    "    return parents.tolist()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "IN_DIM = test_embeddings.shape[1]\n",
    "missing = 0  # 지금은 쓸 일 없지만 원래 코드랑 형태 맞춰둠\n",
    "\n",
    "with open(OUT_PATH, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow([\"id\", \"label\"])\n",
    "\n",
    "    buf_x, buf_pid = [], []\n",
    "\n",
    "    def flush():\n",
    "        if not buf_x:\n",
    "            return\n",
    "        xb = torch.from_numpy(np.stack(buf_x, axis=0).astype(np.float32)).to(device)\n",
    "        with torch.inference_mode():\n",
    "            prob = torch.sigmoid(model(xb)).detach().cpu().numpy()\n",
    "        prob = np.nan_to_num(prob, nan=-1.0, posinf=1.0, neginf=0.0)\n",
    "\n",
    "        for pid, p in zip(buf_pid, prob):\n",
    "            order = np.argsort(-p)\n",
    "\n",
    "            # 1) 기본 후보 뽑기\n",
    "            thr_keep = [i for i in order if p[i] >= 0.5][:MAX_LABS]\n",
    "            if len(thr_keep) >= MIN_LABS:\n",
    "                keep = thr_keep[:MAX_LABS]\n",
    "            else:\n",
    "                keep = order[:max(MIN_LABS, len(thr_keep))]\n",
    "                if len(keep) < MIN_LABS:\n",
    "                    keep = order[:MIN_LABS]\n",
    "\n",
    "            # 2) 부모 후보\n",
    "            parent_cands = []\n",
    "            for c in keep:\n",
    "                pars = ancestors_of(c, B)\n",
    "                for pa in pars:\n",
    "                    if pa not in keep and pa not in parent_cands:\n",
    "                        parent_cands.append(pa)\n",
    "\n",
    "            parent_cands.sort(key=lambda idx: p[idx], reverse=True)\n",
    "\n",
    "            # 3) 남는 슬롯 부모로 채우기\n",
    "            final_idxs = list(keep)\n",
    "            for pa in parent_cands:\n",
    "                if len(final_idxs) >= MAX_LABS:\n",
    "                    break\n",
    "                final_idxs.append(pa)\n",
    "\n",
    "            # 4) 그래도 모자라면 확률순\n",
    "            if len(final_idxs) < MIN_LABS:\n",
    "                for idx in order:\n",
    "                    if idx not in final_idxs:\n",
    "                        final_idxs.append(idx)\n",
    "                    if len(final_idxs) >= MIN_LABS:\n",
    "                        break\n",
    "\n",
    "            labels = sorted(int(lab_ids[i]) for i in final_idxs)\n",
    "            w.writerow([pid, \",\".join(map(str, labels))])\n",
    "\n",
    "        buf_x.clear()\n",
    "        buf_pid.clear()\n",
    "\n",
    "    # 여기서 바로 pids와 test_embeddings를 같이 순회\n",
    "    for pid, emb in zip(pids, test_embeddings):\n",
    "        x = emb\n",
    "        if x.dtype != np.float32:\n",
    "            x = x.astype(np.float32, copy=False)\n",
    "        buf_x.append(x)\n",
    "        buf_pid.append(pid)\n",
    "        if len(buf_x) >= BATCH:\n",
    "            flush()\n",
    "    flush()\n",
    "\n",
    "print(f\"Saved: {OUT_PATH} | samples={len(pids)} | min-max labels per sample={MIN_LABS}-{MAX_LABS} | missing_pids={missing}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e293bbf-8597-4c2c-91e1-b03ce7477217",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b72c5b-1d7a-4d59-a975-e7fa22dfbb32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a677853-591b-488e-835f-cf4b4c164c1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343dfe98-d91d-4c90-9325-ec5a6ff5c560",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49cebb9-e667-46aa-8c96-64a63c801ab3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d535af4-4d65-4127-b625-caac88456b68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b2e45b-844d-437a-9fc5-1d47ad4a9832",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a133dad-b306-4349-b9d3-073022f3eb43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba53595b-f997-468a-a4f9-6deee2dc1a08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb8152f-64ca-44fc-a37b-4ab46e4291d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5d29a7-c5b4-4a26-954b-b65a737f632c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
