{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26217918-5c27-43a7-a504-84e4cd5bba75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T04:48:07.872424Z",
     "iopub.status.busy": "2025-11-13T04:48:07.872002Z",
     "iopub.status.idle": "2025-11-13T04:48:09.141520Z",
     "shell.execute_reply": "2025-11-13T04:48:09.140947Z",
     "shell.execute_reply.started": "2025-11-13T04:48:07.872405Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25eba133-3361-42f0-8305-6df95935d14f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T04:48:09.142956Z",
     "iopub.status.busy": "2025-11-13T04:48:09.142666Z",
     "iopub.status.idle": "2025-11-13T04:48:11.574868Z",
     "shell.execute_reply": "2025-11-13T04:48:11.574160Z",
     "shell.execute_reply.started": "2025-11-13T04:48:09.142941Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chocolate_bars 0.2752\n",
      "dried_fruit_raisins 0.2514\n",
      "chocolate_pretzels 0.2306\n",
      "granola_bars 0.2077\n",
      "chocolate_gifts 0.2041\n",
      "chocolate 0.1957\n",
      "snack_gifts 0.1881\n",
      "chocolate_assortments 0.1858\n",
      "grocery_gourmet_food 0.1789\n",
      "chocolate_covered_fruit 0.1628\n",
      "candy_chocolate 0.1536\n",
      "fruit_gifts 0.1512\n",
      "hot_cocoa 0.151\n",
      "food 0.1448\n",
      "fresh_baked_cookies 0.1343\n",
      "snack_food 0.1266\n",
      "bars 0.1244\n",
      "assortments 0.1185\n",
      "trail_mix 0.11\n",
      "toaster_pastries 0.1047\n",
      "party_mix 0.1039\n",
      "crackers 0.1006\n",
      "cookies 0.0998\n",
      "granola_trail_mix_bars 0.0992\n",
      "nutrition_bars_drinks 0.0972\n",
      "p_t_s 0.0969\n",
      "fruit 0.0948\n",
      "puffed_snacks 0.0945\n",
      "pudding 0.0914\n",
      "fruit_leather 0.0907\n",
      "chocolate_truffles 0.0907\n",
      "solid_feeding 0.0873\n",
      "gourmet_gifts 0.0862\n",
      "fruits 0.0827\n",
      "cakes 0.0812\n",
      "raisins 0.0796\n",
      "rice_cakes 0.0789\n",
      "treats 0.0788\n",
      "baby_food 0.078\n",
      "meat_poultry 0.0768\n",
      "changing_table_pads_covers 0.0763\n",
      "cheese_gifts 0.0755\n",
      "milk 0.0754\n",
      "jams 0.0748\n",
      "marshmallows 0.0738\n",
      "candy_gifts 0.0734\n",
      "produce 0.0726\n",
      "popcorn 0.0717\n",
      "jerky_dried_meats 0.0702\n",
      "chocolate_covered_nuts 0.0699\n",
      "cloth_diapers 0.0695\n",
      "salsas 0.0694\n",
      "eggs 0.0688\n",
      "juices 0.0687\n",
      "nutrition_wellness 0.0683\n",
      "pretzels 0.0672\n",
      "sausages 0.0661\n",
      "dessert_gifts 0.0655\n",
      "baby_formula 0.064\n",
      "seafood_gifts 0.0632\n",
      "gummy_candy 0.0629\n",
      "sugars_sweeteners 0.0628\n",
      "suckers_lollipops 0.0605\n",
      "jams_preserves_gifts 0.0601\n",
      "nut_clusters 0.0597\n",
      "meat_gifts 0.0583\n",
      "sensual_delights 0.0555\n",
      "sauces_gifts 0.0551\n",
      "spices_gifts 0.055\n",
      "foie_gras_p_t_s 0.0494\n",
      "jerky 0.0\n",
      "toys_games 0.0\n",
      "games 0.0\n",
      "puzzles 0.0\n",
      "jigsaw_puzzles 0.0\n",
      "board_games 0.0\n",
      "beverages 0.0\n",
      "beauty 0.0\n",
      "makeup 0.0\n",
      "nails 0.0\n",
      "arts_crafts 0.0\n",
      "drawing_painting_supplies 0.0\n",
      "action_toy_figures 0.0\n",
      "figures 0.0\n",
      "dolls_accessories 0.0\n",
      "dolls 0.0\n",
      "card_games 0.0\n",
      "drawing_sketching_tablets 0.0\n",
      "baby_toddler_toys 0.0\n",
      "shape_sorters 0.0\n",
      "health_personal_care 0.0\n",
      "personal_care 0.0\n",
      "deodorants_antiperspirants 0.0\n",
      "learning_education 0.0\n",
      "habitats 0.0\n",
      "electronics_for_kids 0.0\n",
      "household_supplies 0.0\n",
      "household_batteries 0.0\n",
      "push_pull_toys 0.0\n",
      "stuffed_animals_plush 0.0\n",
      "tricycles 0.0\n",
      "scooters_wagons 0.0\n",
      "clay_dough 0.0\n",
      "health_care 0.0\n",
      "allergy 0.0\n",
      "baby_products 0.0\n",
      "gear 0.0\n",
      "baby_gyms_playmats 0.0\n",
      "shaving_hair_removal 0.0\n",
      "skin_care 0.0\n",
      "face 0.0\n",
      "animals_figures 0.0\n",
      "feminine_care 0.0\n",
      "music_sound 0.0\n",
      "oral_hygiene 0.0\n",
      "grown_up_toys 0.0\n",
      "dress_up_pretend_play 0.0\n",
      "pretend_play 0.0\n",
      "novelty_gag_toys 0.0\n",
      "bath_body 0.0\n",
      "cleansers 0.0\n",
      "playsets 0.0\n",
      "d_puzzles 0.0\n",
      "dollhouses 0.0\n",
      "lip_care_products 0.0\n",
      "tools_accessories 0.0\n",
      "nail_tools 0.0\n",
      "eye_care 0.0\n",
      "pill_cases_splitters 0.0\n",
      "hair_care 0.0\n",
      "styling_products 0.0\n",
      "electronic_toys 0.0\n",
      "body 0.0\n",
      "toy_balls 0.0\n",
      "eyes 0.0\n",
      "trading_card_games 0.0\n",
      "foot_care 0.0\n",
      "hands_nails 0.0\n",
      "sun 0.0\n",
      "medical_supplies_equipment 0.0\n",
      "daily_living_aids 0.0\n",
      "baby_child_care 0.0\n",
      "paper_plastic 0.0\n",
      "incontinence 0.0\n",
      "shampoos 0.0\n",
      "conditioners 0.0\n",
      "music_players_karaoke 0.0\n",
      "cough_cold 0.0\n",
      "bath 0.0\n",
      "tests 0.0\n",
      "building_toys 0.0\n",
      "building_sets 0.0\n",
      "stress_reduction 0.0\n",
      "family_planning_contraceptives 0.0\n",
      "vitamins_supplements 0.0\n",
      "hair_color 0.0\n",
      "pain_relievers 0.0\n",
      "cotton_swabs 0.0\n",
      "styling_tools 0.0\n",
      "first_aid 0.0\n",
      "scrubs_body_treatments 0.0\n",
      "cleaning_tools 0.0\n",
      "pegged_puzzles 0.0\n",
      "diabetes 0.0\n",
      "magic_kits_accessories 0.0\n",
      "gifts 0.0\n",
      "albums 0.0\n",
      "crib_toys_attachments 0.0\n",
      "digestion_nausea 0.0\n",
      "electronic_pets 0.0\n",
      "sexual_wellness 0.0\n",
      "safer_sex 0.0\n",
      "thermometers 0.0\n",
      "stacking_nesting_toys 0.0\n",
      "makeup_remover 0.0\n",
      "temporary_tattoos 0.0\n",
      "sports_outdoor_play 0.0\n",
      "play_tents_tunnels 0.0\n",
      "science 0.0\n",
      "sports 0.0\n",
      "bath_toys 0.0\n",
      "puppets 0.0\n",
      "systems_accessories 0.0\n",
      "health_monitors 0.0\n",
      "inflatable_bouncers 0.0\n",
      "hobbies 0.0\n",
      "model_building_kits_tools 0.0\n",
      "blackboards_whiteboards 0.0\n",
      "pools_water_fun 0.0\n",
      "rattles 0.0\n",
      "sandboxes_accessories 0.0\n",
      "activity_play_centers 0.0\n",
      "car_seat_stroller_toys 0.0\n",
      "feeding 0.0\n",
      "bottle_feeding 0.0\n",
      "breastfeeding 0.0\n",
      "diapering 0.0\n",
      "diaper_changing_kits 0.0\n",
      "puzzle_accessories 0.0\n",
      "diaper_pails_refills 0.0\n",
      "safety 0.0\n",
      "bathroom_safety 0.0\n",
      "massage_relaxation 0.0\n",
      "gates_doorways 0.0\n",
      "nursery 0.0\n",
      "furniture 0.0\n",
      "monitors 0.0\n",
      "plush_backpacks_purses 0.0\n",
      "statues 0.0\n",
      "bathing_skin_care 0.0\n",
      "bathing_tubs_seats 0.0\n",
      "vehicles_remote_control 0.0\n",
      "play_vehicles 0.0\n",
      "backpacks_carriers 0.0\n",
      "craft_kits 0.0\n",
      "car_seats_accessories 0.0\n",
      "car_seats 0.0\n",
      "nursery_d_cor 0.0\n",
      "hammering_pounding_toys 0.0\n",
      "bedding 0.0\n",
      "play_trains_railway_sets 0.0\n",
      "rockets 0.0\n",
      "stacking_blocks 0.0\n",
      "diaper_bags 0.0\n",
      "strollers 0.0\n",
      "gym_sets_swings 0.0\n",
      "pregnancy_maternity 0.0\n",
      "maternity_pillows 0.0\n",
      "rocking_spring_ride_ons 0.0\n",
      "braces 0.0\n",
      "accessories 0.0\n",
      "vehicle_playsets 0.0\n",
      "doll_accessories 0.0\n",
      "pet_supplies 0.0\n",
      "cats 0.0\n",
      "litter_housebreaking 0.0\n",
      "spinning_tops 0.0\n",
      "sets 0.0\n",
      "travel_games 0.0\n",
      "pillows_stools 0.0\n",
      "battling_tops 0.0\n",
      "cameras_camcorders 0.0\n",
      "dance_mats 0.0\n",
      "radio_control 0.0\n",
      "grooming_healthcare_kits 0.0\n",
      "balls 0.0\n",
      "tile_games 0.0\n",
      "potty_training 0.0\n",
      "potties_seats 0.0\n",
      "highchairs_booster_seats 0.0\n",
      "stuffed_animals_toys 0.0\n",
      "dvd_games 0.0\n",
      "edge_corner_guards 0.0\n",
      "basic_life_skills_toys 0.0\n",
      "activity_centers_entertainers 0.0\n",
      "thermometer_accessories 0.0\n",
      "wipes_holders 0.0\n",
      "gift_sets 0.0\n",
      "joggers 0.0\n",
      "facial_steamers 0.0\n",
      "kites_wind_spinners 0.0\n",
      "dogs 0.0\n",
      "toys 0.0\n",
      "walkers 0.0\n",
      "slumber_bags 0.0\n",
      "die_cast_vehicles 0.0\n",
      "easels 0.0\n",
      "lips 0.0\n",
      "tea 0.0\n",
      "reading_writing 0.0\n",
      "stacking_games 0.0\n",
      "sauces_dips 0.0\n",
      "sauces 0.0\n",
      "breakfast_foods 0.0\n",
      "cereals 0.0\n",
      "shopping_cart_covers 0.0\n",
      "pantry_staples 0.0\n",
      "scaled_model_vehicles 0.0\n",
      "cooking_baking_supplies 0.0\n",
      "personal_video_players_accessories 0.0\n",
      "fragrance 0.0\n",
      "women_s 0.0\n",
      "keepsakes 0.0\n",
      "swings 0.0\n",
      "trains_accessories 0.0\n",
      "disposable_diapers 0.0\n",
      "plug_play_video_games 0.0\n",
      "floor_puzzles 0.0\n",
      "fresh_flowers_live_indoor_plants 0.0\n",
      "live_indoor_plants 0.0\n",
      "weight_loss_products 0.0\n",
      "smoking_cessation 0.0\n",
      "beauty_fashion 0.0\n",
      "mirrors 0.0\n",
      "coffee 0.0\n",
      "cabinet_locks_straps 0.0\n",
      "plush_pillows 0.0\n",
      "floor_games 0.0\n",
      "makeup_brushes_tools 0.0\n",
      "alternative_medicine 0.0\n",
      "men_s 0.0\n",
      "step_stools 0.0\n",
      "rails_rail_guards 0.0\n",
      "laundry 0.0\n",
      "women_s_health 0.0\n",
      "standard 0.0\n",
      "beds_furniture 0.0\n",
      "herbs 0.0\n",
      "sleep_positioners 0.0\n",
      "health_supplies 0.0\n",
      "breakfast_cereal_bars 0.0\n",
      "body_art 0.0\n",
      "condiments 0.0\n",
      "breads_bakery 0.0\n",
      "dried_beans 0.0\n",
      "household_cleaning 0.0\n",
      "collars 0.0\n",
      "educational_repellents 0.0\n",
      "adult_toys_games 0.0\n",
      "teddy_bears 0.0\n",
      "therapeutic_skin_care 0.0\n",
      "sand_water_tables 0.0\n",
      "slot_cars 0.0\n",
      "soft_drinks 0.0\n",
      "chips_crisps 0.0\n",
      "licorice 0.0\n",
      "feeding_watering_supplies 0.0\n",
      "blasters_foam_play 0.0\n",
      "meat_seafood 0.0\n",
      "wild_game_fowl 0.0\n",
      "spices_seasonings 0.0\n",
      "training_behavior_aids 0.0\n",
      "gardening_tools 0.0\n",
      "bathroom_aids_safety 0.0\n",
      "pogo_sticks_hoppers 0.0\n",
      "powdered_drink_mixes 0.0\n",
      "playards 0.0\n",
      "gag_toys_practical_jokes 0.0\n",
      "lighters 0.0\n",
      "money_banks 0.0\n",
      "marble_runs 0.0\n",
      "game_collections 0.0\n",
      "kitchen_safety 0.0\n",
      "fish_aquatic_pets 0.0\n",
      "gum 0.0\n",
      "outdoor_safety 0.0\n",
      "hair_nails 0.0\n",
      "aquarium_lights 0.0\n",
      "blocks 0.0\n",
      "tandem 0.0\n",
      "occupational_physical_therapy_aids 0.0\n",
      "packaged_meals_side_dishes 0.0\n",
      "indoor_climbers_play_structures 0.0\n",
      "pumps_filters 0.0\n",
      "beds_accessories 0.0\n",
      "energy_drinks 0.0\n",
      "sleep_snoring 0.0\n",
      "geography 0.0\n",
      "small_animals 0.0\n",
      "houses_habitats 0.0\n",
      "dairy_eggs 0.0\n",
      "cheese 0.0\n",
      "travel_systems 0.0\n",
      "walkie_talkies 0.0\n",
      "mobility_aids_equipment 0.0\n",
      "sexual_enhancers 0.0\n",
      "dips 0.0\n",
      "dollhouse_accessories 0.0\n",
      "bathing_accessories 0.0\n",
      "grooming 0.0\n",
      "baby_seats 0.0\n",
      "wind_up_toys 0.0\n",
      "dishwashing 0.0\n",
      "carriers_strollers 0.0\n",
      "flash_cards 0.0\n",
      "brain_teasers 0.0\n",
      "nesting_dolls 0.0\n",
      "test_kits 0.0\n",
      "lightweight 0.0\n",
      "hair_loss_products 0.0\n",
      "water_treatments 0.0\n",
      "birds 0.0\n",
      "hair_scalp_treatments 0.0\n",
      "cages_accessories 0.0\n",
      "gummy_candies 0.0\n",
      "houses 0.0\n",
      "ear_care 0.0\n",
      "pizza_crusts 0.0\n",
      "hard_candies 0.0\n",
      "sports_supplements 0.0\n",
      "baking_mixes 0.0\n",
      "pork_rinds 0.0\n",
      "pasta_noodles 0.0\n",
      "carriers_travel_products 0.0\n",
      "fresh_fruits 0.0\n",
      "chips 0.0\n",
      "mathematics_counting 0.0\n",
      "toy_banks 0.0\n",
      "training_pants 0.0\n",
      "tea_gifts 0.0\n",
      "oils 0.0\n",
      "aquarium_hoods 0.0\n",
      "tortillas 0.0\n",
      "doors 0.0\n",
      "standard_playing_card_decks 0.0\n",
      "fudge 0.0\n",
      "syrups 0.0\n",
      "printing_stamping 0.0\n",
      "toy_gift_sets 0.0\n",
      "canned_jarred_food 0.0\n",
      "fresh_vegetables 0.0\n",
      "apparel_accessories 0.0\n",
      "chewing_gum 0.0\n",
      "puzzle_play_mats 0.0\n",
      "electrical_safety 0.0\n",
      "marble_games 0.0\n",
      "miniatures 0.0\n",
      "finger_boards_finger_bikes 0.0\n",
      "coconut_water 0.0\n",
      "handheld_games 0.0\n",
      "slime_putty_toys 0.0\n",
      "pastries 0.0\n",
      "health_baby_care 0.0\n",
      "teethers 0.0\n",
      "butter 0.0\n",
      "breakfast_bakery 0.0\n",
      "stickers 0.0\n",
      "soaps_cleansers 0.0\n",
      "fitness_equipment 0.0\n",
      "water 0.0\n",
      "portable_changing_pads 0.0\n",
      "dice_gaming_dice 0.0\n",
      "pacifiers_accessories 0.0\n",
      "cocktail_mixers 0.0\n",
      "aquariums 0.0\n",
      "ball_pits_accessories 0.0\n",
      "seafood 0.0\n",
      "bags_cases 0.0\n",
      "jelly_beans 0.0\n",
      "novelty_spinning_tops 0.0\n",
      "automatic_feeders 0.0\n",
      "mints 0.0\n",
      "makeup_sets 0.0\n",
      "cleaners 0.0\n",
      "fresh_cut_flowers 0.0\n",
      "prams 0.0\n",
      "nuts_seeds 0.0\n",
      "taffy 0.0\n",
      "bunny_rabbit_central 0.0\n",
      "rabbit_hutches 0.0\n",
      "aquarium_d_cor 0.0\n",
      "viewfinders 0.0\n",
      "harnesses_leashes 0.0\n",
      "game_accessories 0.0\n",
      "game_room_games 0.0\n",
      "cages 0.0\n",
      "non_slip_bath_mats 0.0\n",
      "halva 0.0\n",
      "stimulants 0.0\n",
      "beanbags_foot_bags 0.0\n",
      "shampoo_conditioner_sets 0.0\n",
      "breadcrumbs 0.0\n",
      "extracts_flavoring 0.0\n",
      "plush_puppets 0.0\n",
      "shampoo_plus_conditioner 0.0\n",
      "memorials 0.0\n",
      "die_cast_toy_vehicles 0.0\n",
      "aquarium_starter_kits 0.0\n",
      "coffee_gifts 0.0\n",
      "air_fresheners 0.0\n",
      "sugar_substitutes 0.0\n",
      "bacon 0.0\n",
      "cat_flaps 0.0\n",
      "aquarium_heaters 0.0\n",
      "hair_relaxers 0.0\n",
      "breads 0.0\n",
      "packaged_breads 0.0\n",
      "dessert_toppings 0.0\n",
      "diaper_stackers_caddies 0.0\n",
      "prisms_kaleidoscopes 0.0\n",
      "maternity 0.0\n",
      "crackers_biscuits 0.0\n",
      "coin_collecting 0.0\n",
      "kickball_playground_balls 0.0\n",
      "hair_perms_texturizers 0.0\n",
      "yo_yos 0.0\n",
      "flours_meals 0.0\n",
      "beef 0.0\n",
      "molding_sculpting_sticks 0.0\n",
      "washcloths_towels 0.0\n",
      "stuffing 0.0\n",
      "baking_powder 0.0\n",
      "cereal 0.0\n",
      "exotic_meats 0.0\n",
      "breadsticks 0.0\n",
      "cloth_diaper_accessories 0.0\n",
      "carriers 0.0\n",
      "toffee 0.0\n",
      "hair_coloring_tools 0.0\n",
      "caramels 0.0\n",
      "aromatherapy 0.0\n",
      "seat_covers 0.0\n",
      "bondage_gear_accessories 0.0\n",
      "sun_protection 0.0\n",
      "dinners 0.0\n",
      "aquarium_stands 0.0\n",
      "teaching_clocks 0.0\n",
      "milk_substitutes 0.0\n",
      "bubble_bath 0.0\n",
      "novelties 0.0\n",
      "beads 0.0\n",
      "fish_bowls 0.0\n",
      "odor_stain_removers 0.0\n",
      "food_coloring 0.0\n",
      "children_s 0.0\n",
      "ice_cream_frozen_desserts 0.0\n",
      "pastry_decorations 0.0\n",
      "chicken 0.0\n",
      "sports_drinks 0.0\n",
      "aprons_smocks 0.0\n",
      "electronics 0.0\n",
      "sex_furniture 0.0\n",
      "pork 0.0\n",
      "dried_fruit 0.0\n",
      "flying_toys 0.0\n",
      "shampoo 0.0\n",
      "coatings_batters 0.0\n",
      "hydrometers 0.0\n",
      "lamb 0.0\n",
      "exercise_wheels 0.0\n",
      "breeding_tanks 0.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, ConcatDataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_label_file(path: str) -> str:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def parse_key_value_lines(text: str):\n",
    "    id2label = {}\n",
    "    for line in text.splitlines():\n",
    "        line = line.strip()\n",
    "        if not line or \":\" not in line:\n",
    "            continue\n",
    "        key, vals = line.split(\":\", 1)\n",
    "        id2label[key.strip()] = vals.strip()\n",
    "    return id2label\n",
    "\n",
    "def preprocess_label_text(label_path_str: str):\n",
    "    cleaned = label_path_str.lower()\n",
    "    cleaned = re.sub(r\"[:,]\", \" \", cleaned)\n",
    "    cleaned = re.sub(r\"_\", \" \", cleaned)\n",
    "    cleaned = re.sub(r\"[^a-z0-9 ]\", \" \", cleaned)\n",
    "    cleaned = re.sub(r\"\\s+\", \" \", cleaned).strip()\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "class BM25Vectorizer:\n",
    "    \"\"\"\n",
    "    TF-IDF처럼 fit(...) → transform(...)으로 쓰려는 용도.\n",
    "    문서 집합에 대해 어휘, df, idf, avgdl을 만들고\n",
    "    같은 어휘를 쓰는 새 문서도 transform으로 BM25 벡터화할 수 있게 함.\n",
    "    \"\"\"\n",
    "    def __init__(self, k1=1.5, b=0.75):\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self.vocab_ = {}          # token -> index\n",
    "        self.idf_ = None          # (V,)\n",
    "        self.avgdl_ = None\n",
    "        self.doc_len_ = None      # (N,)\n",
    "        self.fitted_ = False\n",
    "\n",
    "    def fit(self, docs_tokens):\n",
    "        \"\"\"\n",
    "        docs_tokens: 토큰 리스트들의 리스트\n",
    "            예) [[\"gourmet\",\"organic\"], [\"snack\",\"chocolate\",\"organic\"]]\n",
    "        \"\"\"\n",
    "        N = len(docs_tokens)\n",
    "        vocab = {}\n",
    "        df = {}\n",
    "\n",
    "        doc_lens = []\n",
    "\n",
    "        for doc in docs_tokens:\n",
    "            doc_lens.append(len(doc))\n",
    "            seen = set()\n",
    "            for tok in doc:\n",
    "                if tok not in vocab:\n",
    "                    vocab[tok] = len(vocab)\n",
    "                if tok not in seen:\n",
    "                    df[tok] = df.get(tok, 0) + 1\n",
    "                    seen.add(tok)\n",
    "\n",
    "        V = len(vocab)\n",
    "        idf = np.zeros(V, dtype=np.float32)\n",
    "\n",
    "        # BM25 idf\n",
    "        for tok, idx in vocab.items():\n",
    "            dfi = df[tok]\n",
    "            # log((N - dfi + 0.5)/(dfi + 0.5) + 1)\n",
    "            idf[idx] = np.log((N - dfi + 0.5) / (dfi + 0.5) + 1.0)\n",
    "\n",
    "        self.vocab_ = vocab\n",
    "        self.idf_ = idf\n",
    "        self.avgdl_ = np.mean(doc_lens) if N > 0 else 0.0\n",
    "        self.doc_len_ = np.array(doc_lens, dtype=np.float32)\n",
    "        self.fitted_ = True\n",
    "        return self\n",
    "\n",
    "    def _vec_for_doc(self, doc_tokens, doc_len=None):\n",
    "        \"\"\"\n",
    "        단일 문서를 BM25 가중치로 벡터화해서 (V,) numpy로 반환\n",
    "        \"\"\"\n",
    "        assert self.fitted_, \"BM25Vectorizer not fitted\"\n",
    "        V = len(self.vocab_)\n",
    "        vec = np.zeros(V, dtype=np.float32)\n",
    "\n",
    "        # tf 계산\n",
    "        tf = {}\n",
    "        for tok in doc_tokens:\n",
    "            if tok in self.vocab_:\n",
    "                tf[tok] = tf.get(tok, 0) + 1\n",
    "\n",
    "        if doc_len is None:\n",
    "            doc_len = len(doc_tokens)\n",
    "\n",
    "        for tok, freq in tf.items():\n",
    "            idx = self.vocab_[tok]\n",
    "            idf = self.idf_[idx]\n",
    "            # BM25 term weight\n",
    "            denom = freq + self.k1 * (1 - self.b + self.b * (doc_len / self.avgdl_))\n",
    "            score = idf * (freq * (self.k1 + 1)) / denom\n",
    "            vec[idx] = score\n",
    "\n",
    "        return vec\n",
    "\n",
    "    def transform(self, docs_tokens):\n",
    "        \"\"\"\n",
    "        여러 문서를 한꺼번에 변환\n",
    "        docs_tokens: [ [tok,...], [tok,...], ... ]\n",
    "        return: (N, V) numpy array\n",
    "        \"\"\"\n",
    "        mats = []\n",
    "        for doc in docs_tokens:\n",
    "            mats.append(self._vec_for_doc(doc))\n",
    "        return np.stack(mats, axis=0)\n",
    "\n",
    "\n",
    "# ------------------ 여기부터는 네가 쓰던 흐름 ------------------\n",
    "\n",
    "# 1) 라벨 불러오기\n",
    "label_raw_text = load_label_file(\"Amazon_products/class_related_keywords.txt\")\n",
    "id2label = parse_key_value_lines(label_raw_text)\n",
    "\n",
    "# 2) 라벨 텍스트 전처리 & 토큰화\n",
    "label_keys = list(id2label.keys())\n",
    "label_docs_tokens = []\n",
    "for k in label_keys:\n",
    "    # \"key + values\"를 하나의 문서로 본다\n",
    "    text = preprocess_label_text(f\"{k} {id2label[k]}\")\n",
    "    toks = text.split()\n",
    "    label_docs_tokens.append(toks)\n",
    "\n",
    "# 3) BM25 벡터라이저 학습 (라벨들을 코퍼스로 삼음)\n",
    "bm25 = BM25Vectorizer(k1=1.5, b=0.75)\n",
    "bm25.fit(label_docs_tokens)\n",
    "\n",
    "# 4) 라벨들도 BM25 벡터로 \"임베딩\"\n",
    "label_bm25 = bm25.transform(label_docs_tokens)   # shape: (num_labels, vocab_size)\n",
    "\n",
    "# 5) 테스트용 문서 하나\n",
    "doc = \"gourmet organic chocolate snack\"\n",
    "doc_clean = preprocess_label_text(doc)\n",
    "doc_tokens = doc_clean.split()\n",
    "\n",
    "# 문서도 같은 BM25 규칙으로 \"임베딩\"\n",
    "doc_vec = bm25.transform([doc_tokens])           # shape: (1, vocab_size)\n",
    "\n",
    "# 6) 코사인 유사도로 순위 매기기 (TF-IDF 때와 동일)\n",
    "sims = cosine_similarity(doc_vec, label_bm25)[0]  # (num_labels,)\n",
    "\n",
    "label_sims = list(zip(label_keys, sims))\n",
    "label_sims.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for lbl, score in label_sims:\n",
    "    print(lbl, round(float(score), 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19ba131e-72d7-4e8c-b1fe-2f3ef78b1466",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T04:48:11.576857Z",
     "iopub.status.busy": "2025-11-13T04:48:11.576724Z",
     "iopub.status.idle": "2025-11-13T04:48:11.581071Z",
     "shell.execute_reply": "2025-11-13T04:48:11.580344Z",
     "shell.execute_reply.started": "2025-11-13T04:48:11.576843Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3472,)\n"
     ]
    }
   ],
   "source": [
    "def build_label_bm25_embeddings(label_keys, label_bm25, dense: bool = True):\n",
    "    \"\"\"\n",
    "    label_keys: 라벨 이름 리스트 (BM25 transform 할 때 썼던 순서와 같아야 함)\n",
    "    label_bm25: shape = (n_labels, vocab_size) 인 numpy array\n",
    "                (혹은 csr_matrix로 바꿔놨다면 그걸로도 가능)\n",
    "    dense: True면 numpy 1D array로 꺼내서 줌\n",
    "    \"\"\"\n",
    "    embeddings = {}\n",
    "    if dense:\n",
    "        # label_bm25가 이미 ndarray라면 바로 인덱싱\n",
    "        for i, label in enumerate(label_keys):\n",
    "            embeddings[label] = label_bm25[i]\n",
    "    else:\n",
    "        # 만약 label_bm25를 csr_matrix로 만들어놨다면 이 가지도 쓸 수 있음\n",
    "        for i, label in enumerate(label_keys):\n",
    "            embeddings[label] = label_bm25[i]\n",
    "    return embeddings\n",
    "\n",
    "label_embeddings = build_label_bm25_embeddings(label_keys, label_bm25, dense=True)\n",
    "print(label_embeddings[\"grocery_gourmet_food\"].shape)  # (vocab_size,)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e1bcacb-8765-49a0-b6d6-d18d6f5e5531",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T01:28:59.947460Z",
     "iopub.status.busy": "2025-11-11T01:28:59.947322Z",
     "iopub.status.idle": "2025-11-11T01:28:59.988985Z",
     "shell.execute_reply": "2025-11-11T01:28:59.988493Z",
     "shell.execute_reply.started": "2025-11-11T01:28:59.947446Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5efad2e-4006-4b90-920b-1e70cc6b7ca7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T04:27:06.036773Z",
     "iopub.status.busy": "2025-11-12T04:27:06.036531Z",
     "iopub.status.idle": "2025-11-12T04:27:06.042355Z",
     "shell.execute_reply": "2025-11-12T04:27:06.041870Z",
     "shell.execute_reply.started": "2025-11-12T04:27:06.036755Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roots: [0, 3, 10, 23, 40, 169]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_edges(path):\n",
    "    edges = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            raw = line.strip()\n",
    "            if not raw or raw.startswith(\"#\"):\n",
    "                continue\n",
    "            parts = raw.split()\n",
    "            if len(parts) < 2:\n",
    "                continue\n",
    "            try:\n",
    "                u, v = int(parts[0]), int(parts[1])\n",
    "            except ValueError:\n",
    "                continue\n",
    "            edges.append((u, v))\n",
    "    return edges\n",
    "\n",
    "def find_roots(edges):\n",
    "    parents = set()\n",
    "    children = set()\n",
    "    for u, v in edges:\n",
    "        parents.add(u)\n",
    "        children.add(v)\n",
    "    # 부모로만 나온 애들 = 루트들\n",
    "    roots = parents - children\n",
    "    return sorted(roots)\n",
    "\n",
    "# --- 사용 ---\n",
    "E = load_edges(\"Amazon_products/class_hierarchy.txt\")\n",
    "\n",
    "N = 531\n",
    "A = np.zeros((N, N), dtype=np.uint8)\n",
    "for u, v in E:\n",
    "    A[u, v] = 1\n",
    "    A[v, u] = 1   # 탐색용으로는 무방향 인접행렬 써도 됨\n",
    "\n",
    "B = np.zeros((N, N), dtype=np.uint8)\n",
    "for u, v in E:\n",
    "    B[u, v] = 1\n",
    "\n",
    "roots = find_roots(E)\n",
    "print(\"roots:\", roots)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cde32d7f-3c5e-4810-ab28-07a7abd05c59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T04:27:16.697119Z",
     "iopub.status.busy": "2025-11-12T04:27:16.696695Z",
     "iopub.status.idle": "2025-11-12T04:27:16.705232Z",
     "shell.execute_reply": "2025-11-12T04:27:16.704729Z",
     "shell.execute_reply.started": "2025-11-12T04:27:16.697103Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------\n",
    "# GAT \n",
    "# ---------------------------\n",
    "\n",
    "class SimpleGATLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, heads=4, concat=True, dropout=0.2, negative_slope=0.2, residual=True):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.out_dim = out_dim\n",
    "        self.concat = concat\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.leaky_relu = nn.LeakyReLU(negative_slope)\n",
    "        self.lin = nn.Linear(in_dim, heads * out_dim, bias=False)\n",
    "        self.a_src = nn.Parameter(torch.Tensor(heads, out_dim))\n",
    "        self.a_dst = nn.Parameter(torch.Tensor(heads, out_dim))\n",
    "        self.residual = residual\n",
    "        if residual and (in_dim == (heads * out_dim if concat else out_dim)):\n",
    "            self.res_proj = nn.Identity()\n",
    "        elif residual:\n",
    "            self.res_proj = nn.Linear(in_dim, heads * out_dim if concat else out_dim, bias=False)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.lin.weight)\n",
    "        nn.init.xavier_uniform_(self.a_src)\n",
    "        nn.init.xavier_uniform_(self.a_dst)\n",
    "        if self.residual and not isinstance(getattr(self, \"res_proj\", None), nn.Identity):\n",
    "            nn.init.xavier_uniform_(self.res_proj.weight)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        \"\"\"\n",
    "        x: [N, Fin]\n",
    "        adj: [N, N] (0/1; self-loop 없음)\n",
    "        \"\"\"\n",
    "        N = x.size(0)\n",
    "        Wh = self.lin(x).view(N, self.heads, self.out_dim)  # [N, H, F]\n",
    "\n",
    "        e_src = (Wh * self.a_src).sum(dim=-1)  # [N, H]\n",
    "        e_dst = (Wh * self.a_dst).sum(dim=-1)  # [N, H]\n",
    "        e = e_src.unsqueeze(1) + e_dst.unsqueeze(0)  # [N, N, H]\n",
    "        e = self.leaky_relu(e)\n",
    "        # --- 안전한 masked softmax ---\n",
    "        mask = (adj > 0).unsqueeze(-1)                    # [N, N, 1]\n",
    "        e = e.masked_fill(~mask, -1e9)                    # -inf 대신 -1e9로 NaN 방지\n",
    "        alpha = torch.softmax(e, dim=1)                   # 소프트맥스\n",
    "        alpha = alpha * mask.float()                      # 마스크로 0 처리\n",
    "        denom = alpha.sum(dim=1, keepdim=True).clamp(min=1e-12)  # 이웃 없을 때 0 분모 방지\n",
    "        alpha = alpha / denom                             # 이웃들로 정규화\n",
    "\n",
    "        out = torch.einsum(\"ijh,jhf->ihf\", alpha, Wh)     # [N, H, F]\n",
    "        out = out.reshape(N, self.heads * self.out_dim) if self.concat else out.mean(dim=1)\n",
    "        out = self.dropout(out)\n",
    "        if self.residual:\n",
    "            out = out + self.res_proj(x)                  # self-loop 없는 대신 residual로 자기정보 유지\n",
    "        return out\n",
    "\n",
    "class GATEncoder(nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim=64, out_dim=768, heads1=4, heads2=4, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.gat1 = SimpleGATLayer(in_dim, hid_dim, heads=heads1, concat=True,  dropout=dropout, residual=True)\n",
    "        self.gat2 = SimpleGATLayer(hid_dim*heads1, out_dim, heads=heads2, concat=False, dropout=dropout, residual=True)\n",
    "        self.act = nn.ELU(); self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x, adj):\n",
    "        h = self.gat1(x, adj); h = self.act(h); h = self.dropout(h)\n",
    "        z = self.gat2(h, adj)\n",
    "        return z  # [N, out_dim]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b080c703-91ce-4a83-b745-e87986fcddc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T04:27:18.369875Z",
     "iopub.status.busy": "2025-11-12T04:27:18.369627Z",
     "iopub.status.idle": "2025-11-12T04:27:18.372393Z",
     "shell.execute_reply": "2025-11-12T04:27:18.371884Z",
     "shell.execute_reply.started": "2025-11-12T04:27:18.369857Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da4d3075-be19-4053-9b2f-827bcd31d952",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T04:27:34.344211Z",
     "iopub.status.busy": "2025-11-12T04:27:34.343937Z",
     "iopub.status.idle": "2025-11-12T04:28:00.754190Z",
     "shell.execute_reply": "2025-11-12T04:28:00.753694Z",
     "shell.execute_reply.started": "2025-11-12T04:27:34.344195Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001/700] loss=0.6724 | pos=0.112 neg=0.024 | val AUC=0.8141\n",
      "[002/700] loss=0.6289 | pos=0.302 neg=0.020 | val AUC=0.8769\n",
      "[003/700] loss=0.5972 | pos=0.453 neg=0.014 | val AUC=0.8380\n",
      "[004/700] loss=0.5763 | pos=0.569 neg=0.015 | val AUC=0.8540\n",
      "[005/700] loss=0.5622 | pos=0.658 neg=0.019 | val AUC=0.8702\n",
      "[006/700] loss=0.5464 | pos=0.718 neg=-0.004 | val AUC=0.7879\n",
      "[007/700] loss=0.5381 | pos=0.767 neg=-0.011 | val AUC=0.8227\n",
      "[008/700] loss=0.5338 | pos=0.802 neg=-0.010 | val AUC=0.8721\n",
      "[009/700] loss=0.5249 | pos=0.827 neg=-0.034 | val AUC=0.8036\n",
      "[010/700] loss=0.5327 | pos=0.847 neg=0.005 | val AUC=0.8310\n",
      "[011/700] loss=0.5272 | pos=0.864 neg=-0.008 | val AUC=0.7886\n",
      "[012/700] loss=0.5267 | pos=0.873 neg=-0.007 | val AUC=0.8393\n",
      "[013/700] loss=0.5271 | pos=0.881 neg=-0.001 | val AUC=0.8125\n",
      "[014/700] loss=0.5205 | pos=0.891 neg=-0.024 | val AUC=0.8664\n",
      "[015/700] loss=0.5206 | pos=0.897 neg=-0.024 | val AUC=0.8345\n",
      "[016/700] loss=0.5236 | pos=0.901 neg=-0.013 | val AUC=0.7819\n",
      "[017/700] loss=0.5318 | pos=0.907 neg=0.025 | val AUC=0.7985\n",
      "[018/700] loss=0.5141 | pos=0.911 neg=-0.041 | val AUC=0.8055\n",
      "[019/700] loss=0.5192 | pos=0.914 neg=-0.020 | val AUC=0.8010\n",
      "[020/700] loss=0.5180 | pos=0.919 neg=-0.026 | val AUC=0.8189\n",
      "[021/700] loss=0.5230 | pos=0.922 neg=-0.006 | val AUC=0.8182\n",
      "[022/700] loss=0.5256 | pos=0.924 neg=0.005 | val AUC=0.7752\n",
      "[023/700] loss=0.5254 | pos=0.923 neg=0.005 | val AUC=0.7471\n",
      "[024/700] loss=0.5251 | pos=0.926 neg=0.003 | val AUC=0.8071\n",
      "[025/700] loss=0.5221 | pos=0.927 neg=-0.003 | val AUC=0.7599\n",
      "[026/700] loss=0.5307 | pos=0.926 neg=0.024 | val AUC=0.7184\n",
      "[027/700] loss=0.5184 | pos=0.928 neg=-0.019 | val AUC=0.7714\n",
      "[028/700] loss=0.5142 | pos=0.930 neg=-0.034 | val AUC=0.8077\n",
      "[029/700] loss=0.5209 | pos=0.930 neg=-0.006 | val AUC=0.7650\n",
      "[030/700] loss=0.5205 | pos=0.931 neg=-0.006 | val AUC=0.7959\n",
      "[031/700] loss=0.5235 | pos=0.932 neg=0.004 | val AUC=0.7761\n",
      "[032/700] loss=0.5301 | pos=0.933 neg=0.029 | val AUC=0.7774\n",
      "[033/700] loss=0.5196 | pos=0.934 neg=-0.013 | val AUC=0.7535\n",
      "[034/700] loss=0.5227 | pos=0.936 neg=0.003 | val AUC=0.7439\n",
      "[035/700] loss=0.5172 | pos=0.937 neg=-0.022 | val AUC=0.7392\n",
      "[036/700] loss=0.5285 | pos=0.939 neg=0.025 | val AUC=0.7312\n",
      "[037/700] loss=0.5213 | pos=0.939 neg=-0.003 | val AUC=0.7341\n",
      "[038/700] loss=0.5247 | pos=0.940 neg=0.011 | val AUC=0.7242\n",
      "[039/700] loss=0.5188 | pos=0.937 neg=-0.013 | val AUC=0.6926\n",
      "[040/700] loss=0.5295 | pos=0.935 neg=0.029 | val AUC=0.7414\n",
      "[041/700] loss=0.5191 | pos=0.933 neg=-0.011 | val AUC=0.7494\n",
      "[042/700] loss=0.5283 | pos=0.931 neg=0.024 | val AUC=0.6897\n",
      "[043/700] loss=0.5139 | pos=0.929 neg=-0.033 | val AUC=0.6837\n",
      "[044/700] loss=0.5200 | pos=0.929 neg=-0.007 | val AUC=0.6932\n",
      "[045/700] loss=0.5240 | pos=0.929 neg=0.006 | val AUC=0.6834\n",
      "[046/700] loss=0.5304 | pos=0.930 neg=0.029 | val AUC=0.7659\n",
      "[047/700] loss=0.5251 | pos=0.930 neg=0.010 | val AUC=0.6967\n",
      "[048/700] loss=0.5184 | pos=0.932 neg=-0.012 | val AUC=0.6891\n",
      "[049/700] loss=0.5233 | pos=0.931 neg=0.006 | val AUC=0.7583\n",
      "[050/700] loss=0.5155 | pos=0.932 neg=-0.023 | val AUC=0.7439\n",
      "[051/700] loss=0.5272 | pos=0.932 neg=0.016 | val AUC=0.7305\n",
      "[052/700] loss=0.5190 | pos=0.932 neg=-0.012 | val AUC=0.7280\n",
      "[053/700] loss=0.5206 | pos=0.934 neg=-0.005 | val AUC=0.7012\n",
      "[054/700] loss=0.5139 | pos=0.934 neg=-0.029 | val AUC=0.6719\n",
      "[055/700] loss=0.5248 | pos=0.937 neg=0.009 | val AUC=0.7449\n",
      "[056/700] loss=0.5233 | pos=0.938 neg=0.006 | val AUC=0.6939\n",
      "[057/700] loss=0.5151 | pos=0.939 neg=-0.028 | val AUC=0.6907\n",
      "[058/700] loss=0.5274 | pos=0.940 neg=0.022 | val AUC=0.7806\n",
      "[059/700] loss=0.5222 | pos=0.940 neg=0.002 | val AUC=0.7685\n",
      "[060/700] loss=0.5172 | pos=0.940 neg=-0.018 | val AUC=0.7474\n",
      "[061/700] loss=0.5273 | pos=0.939 neg=0.025 | val AUC=0.7599\n",
      "[062/700] loss=0.5269 | pos=0.937 neg=0.023 | val AUC=0.7621\n",
      "[063/700] loss=0.5197 | pos=0.936 neg=-0.012 | val AUC=0.7535\n",
      "[064/700] loss=0.5278 | pos=0.937 neg=0.024 | val AUC=0.7765\n",
      "[065/700] loss=0.5247 | pos=0.937 neg=0.011 | val AUC=0.7873\n",
      "[066/700] loss=0.5210 | pos=0.936 neg=-0.003 | val AUC=0.8004\n",
      "[067/700] loss=0.5165 | pos=0.935 neg=-0.017 | val AUC=0.7848\n",
      "[068/700] loss=0.5203 | pos=0.936 neg=-0.004 | val AUC=0.7500\n",
      "[069/700] loss=0.5161 | pos=0.936 neg=-0.021 | val AUC=0.7838\n",
      "[070/700] loss=0.5160 | pos=0.934 neg=-0.023 | val AUC=0.8045\n",
      "[071/700] loss=0.5253 | pos=0.936 neg=0.007 | val AUC=0.7761\n",
      "[072/700] loss=0.5127 | pos=0.937 neg=-0.035 | val AUC=0.7124\n",
      "[073/700] loss=0.5282 | pos=0.937 neg=0.024 | val AUC=0.7710\n",
      "[074/700] loss=0.5094 | pos=0.939 neg=-0.046 | val AUC=0.7513\n",
      "[075/700] loss=0.5227 | pos=0.940 neg=0.004 | val AUC=0.7666\n",
      "[076/700] loss=0.5268 | pos=0.941 neg=0.022 | val AUC=0.7427\n",
      "[077/700] loss=0.5308 | pos=0.940 neg=0.037 | val AUC=0.7513\n",
      "[078/700] loss=0.5157 | pos=0.941 neg=-0.015 | val AUC=0.7121\n",
      "[079/700] loss=0.5148 | pos=0.940 neg=-0.023 | val AUC=0.7216\n",
      "[080/700] loss=0.5223 | pos=0.940 neg=0.005 | val AUC=0.8023\n",
      "[081/700] loss=0.5201 | pos=0.938 neg=-0.006 | val AUC=0.7532\n",
      "[082/700] loss=0.5190 | pos=0.937 neg=-0.010 | val AUC=0.7003\n",
      "[083/700] loss=0.5224 | pos=0.937 neg=0.004 | val AUC=0.7446\n",
      "[084/700] loss=0.5241 | pos=0.937 neg=0.013 | val AUC=0.7248\n",
      "[085/700] loss=0.5279 | pos=0.936 neg=0.028 | val AUC=0.7462\n",
      "[086/700] loss=0.5206 | pos=0.938 neg=-0.001 | val AUC=0.7382\n",
      "[087/700] loss=0.5182 | pos=0.939 neg=-0.007 | val AUC=0.7152\n",
      "[088/700] loss=0.5156 | pos=0.940 neg=-0.017 | val AUC=0.7152\n",
      "[089/700] loss=0.5152 | pos=0.943 neg=-0.019 | val AUC=0.6913\n",
      "[090/700] loss=0.5180 | pos=0.945 neg=-0.008 | val AUC=0.7194\n",
      "[091/700] loss=0.5174 | pos=0.947 neg=-0.013 | val AUC=0.6987\n",
      "[092/700] loss=0.5217 | pos=0.948 neg=0.004 | val AUC=0.6987\n",
      "[093/700] loss=0.5184 | pos=0.950 neg=-0.012 | val AUC=0.7465\n",
      "[094/700] loss=0.5195 | pos=0.950 neg=-0.007 | val AUC=0.7003\n",
      "[095/700] loss=0.5283 | pos=0.950 neg=0.024 | val AUC=0.7203\n",
      "[096/700] loss=0.5190 | pos=0.950 neg=-0.008 | val AUC=0.6151\n",
      "[097/700] loss=0.5206 | pos=0.949 neg=-0.004 | val AUC=0.6910\n",
      "[098/700] loss=0.5102 | pos=0.949 neg=-0.048 | val AUC=0.7003\n",
      "[099/700] loss=0.5247 | pos=0.948 neg=0.008 | val AUC=0.6779\n",
      "[100/700] loss=0.5286 | pos=0.945 neg=0.025 | val AUC=0.6767\n",
      "[101/700] loss=0.5302 | pos=0.944 neg=0.030 | val AUC=0.7344\n",
      "[102/700] loss=0.5171 | pos=0.942 neg=-0.017 | val AUC=0.7369\n",
      "[103/700] loss=0.5213 | pos=0.941 neg=-0.004 | val AUC=0.7532\n",
      "[104/700] loss=0.5231 | pos=0.940 neg=0.004 | val AUC=0.7503\n",
      "[105/700] loss=0.5238 | pos=0.940 neg=0.003 | val AUC=0.7018\n",
      "[106/700] loss=0.5099 | pos=0.941 neg=-0.051 | val AUC=0.7003\n",
      "[107/700] loss=0.5158 | pos=0.941 neg=-0.023 | val AUC=0.6177\n",
      "[108/700] loss=0.5203 | pos=0.943 neg=-0.006 | val AUC=0.6728\n",
      "[109/700] loss=0.5276 | pos=0.943 neg=0.019 | val AUC=0.7085\n",
      "[110/700] loss=0.5158 | pos=0.945 neg=-0.023 | val AUC=0.6164\n",
      "[111/700] loss=0.5184 | pos=0.946 neg=-0.012 | val AUC=0.6652\n",
      "[112/700] loss=0.5260 | pos=0.947 neg=0.019 | val AUC=0.6649\n",
      "[113/700] loss=0.5175 | pos=0.947 neg=-0.017 | val AUC=0.6936\n",
      "[114/700] loss=0.5133 | pos=0.947 neg=-0.036 | val AUC=0.7076\n",
      "[115/700] loss=0.5236 | pos=0.948 neg=0.004 | val AUC=0.7302\n",
      "[116/700] loss=0.5250 | pos=0.948 neg=0.011 | val AUC=0.6885\n",
      "[117/700] loss=0.5188 | pos=0.949 neg=-0.014 | val AUC=0.6792\n",
      "[118/700] loss=0.5178 | pos=0.947 neg=-0.015 | val AUC=0.6432\n",
      "[119/700] loss=0.5102 | pos=0.945 neg=-0.046 | val AUC=0.6999\n",
      "[120/700] loss=0.5152 | pos=0.943 neg=-0.030 | val AUC=0.7774\n",
      "[121/700] loss=0.5170 | pos=0.944 neg=-0.016 | val AUC=0.7213\n",
      "[122/700] loss=0.5221 | pos=0.943 neg=0.001 | val AUC=0.7232\n",
      "[123/700] loss=0.5192 | pos=0.940 neg=-0.012 | val AUC=0.7302\n",
      "[124/700] loss=0.5147 | pos=0.940 neg=-0.031 | val AUC=0.7379\n",
      "[125/700] loss=0.5180 | pos=0.941 neg=-0.014 | val AUC=0.6706\n",
      "[126/700] loss=0.5233 | pos=0.938 neg=0.005 | val AUC=0.6684\n",
      "[127/700] loss=0.5188 | pos=0.939 neg=-0.009 | val AUC=0.7490\n",
      "[128/700] loss=0.5138 | pos=0.940 neg=-0.034 | val AUC=0.7140\n",
      "[129/700] loss=0.5233 | pos=0.940 neg=0.003 | val AUC=0.7076\n",
      "[130/700] loss=0.5190 | pos=0.940 neg=-0.016 | val AUC=0.7127\n",
      "[131/700] loss=0.5199 | pos=0.942 neg=-0.011 | val AUC=0.7098\n",
      "[132/700] loss=0.5183 | pos=0.941 neg=-0.019 | val AUC=0.7203\n",
      "[133/700] loss=0.5218 | pos=0.944 neg=-0.003 | val AUC=0.7443\n",
      "[134/700] loss=0.5215 | pos=0.944 neg=-0.009 | val AUC=0.7612\n",
      "[135/700] loss=0.5191 | pos=0.945 neg=-0.014 | val AUC=0.7254\n",
      "[136/700] loss=0.5239 | pos=0.943 neg=0.007 | val AUC=0.6696\n",
      "[137/700] loss=0.5256 | pos=0.942 neg=0.012 | val AUC=0.7197\n",
      "[138/700] loss=0.5216 | pos=0.943 neg=-0.000 | val AUC=0.7698\n",
      "[139/700] loss=0.5238 | pos=0.941 neg=0.007 | val AUC=0.7698\n",
      "[140/700] loss=0.5186 | pos=0.943 neg=-0.013 | val AUC=0.7172\n",
      "[141/700] loss=0.5128 | pos=0.943 neg=-0.033 | val AUC=0.7302\n",
      "[142/700] loss=0.5219 | pos=0.942 neg=-0.000 | val AUC=0.7015\n",
      "[143/700] loss=0.5221 | pos=0.944 neg=0.002 | val AUC=0.7092\n",
      "[144/700] loss=0.5183 | pos=0.945 neg=-0.011 | val AUC=0.7682\n",
      "[145/700] loss=0.5145 | pos=0.944 neg=-0.028 | val AUC=0.7239\n",
      "[146/700] loss=0.5169 | pos=0.944 neg=-0.020 | val AUC=0.7025\n",
      "[147/700] loss=0.5218 | pos=0.944 neg=-0.003 | val AUC=0.7229\n",
      "[148/700] loss=0.5167 | pos=0.944 neg=-0.021 | val AUC=0.7251\n",
      "[149/700] loss=0.5285 | pos=0.943 neg=0.025 | val AUC=0.6636\n",
      "[150/700] loss=0.5151 | pos=0.942 neg=-0.025 | val AUC=0.7583\n",
      "[151/700] loss=0.5177 | pos=0.943 neg=-0.010 | val AUC=0.7296\n",
      "[152/700] loss=0.5305 | pos=0.942 neg=0.035 | val AUC=0.6865\n",
      "[153/700] loss=0.5221 | pos=0.944 neg=0.004 | val AUC=0.7892\n",
      "[154/700] loss=0.5228 | pos=0.943 neg=0.006 | val AUC=0.7165\n",
      "[155/700] loss=0.5150 | pos=0.944 neg=-0.025 | val AUC=0.7159\n",
      "[156/700] loss=0.5215 | pos=0.945 neg=0.003 | val AUC=0.7089\n",
      "[157/700] loss=0.5266 | pos=0.945 neg=0.021 | val AUC=0.6623\n",
      "[158/700] loss=0.5167 | pos=0.947 neg=-0.020 | val AUC=0.7828\n",
      "[159/700] loss=0.5190 | pos=0.948 neg=-0.010 | val AUC=0.7232\n",
      "[160/700] loss=0.5247 | pos=0.950 neg=0.017 | val AUC=0.7101\n",
      "[161/700] loss=0.5158 | pos=0.949 neg=-0.025 | val AUC=0.6830\n",
      "[162/700] loss=0.5259 | pos=0.949 neg=0.015 | val AUC=0.6843\n",
      "[163/700] loss=0.5164 | pos=0.948 neg=-0.021 | val AUC=0.6795\n",
      "[164/700] loss=0.5116 | pos=0.948 neg=-0.039 | val AUC=0.6435\n",
      "[165/700] loss=0.5237 | pos=0.949 neg=0.008 | val AUC=0.7229\n",
      "[166/700] loss=0.5256 | pos=0.949 neg=0.015 | val AUC=0.6403\n",
      "[167/700] loss=0.5159 | pos=0.950 neg=-0.021 | val AUC=0.6738\n",
      "[168/700] loss=0.5213 | pos=0.950 neg=-0.003 | val AUC=0.7602\n",
      "[169/700] loss=0.5196 | pos=0.949 neg=-0.008 | val AUC=0.6629\n",
      "[170/700] loss=0.5210 | pos=0.949 neg=0.001 | val AUC=0.7510\n",
      "[171/700] loss=0.5248 | pos=0.950 neg=0.010 | val AUC=0.7589\n",
      "[172/700] loss=0.5320 | pos=0.949 neg=0.037 | val AUC=0.7462\n",
      "[173/700] loss=0.5138 | pos=0.948 neg=-0.031 | val AUC=0.7149\n",
      "[174/700] loss=0.5151 | pos=0.947 neg=-0.030 | val AUC=0.7532\n",
      "[175/700] loss=0.5191 | pos=0.947 neg=-0.013 | val AUC=0.7452\n",
      "[176/700] loss=0.5221 | pos=0.946 neg=0.004 | val AUC=0.7704\n",
      "[177/700] loss=0.5178 | pos=0.947 neg=-0.015 | val AUC=0.7254\n",
      "[178/700] loss=0.5133 | pos=0.947 neg=-0.034 | val AUC=0.6805\n",
      "[179/700] loss=0.5202 | pos=0.947 neg=-0.008 | val AUC=0.7765\n",
      "[180/700] loss=0.5240 | pos=0.945 neg=0.003 | val AUC=0.7411\n",
      "[181/700] loss=0.5253 | pos=0.943 neg=0.011 | val AUC=0.7254\n",
      "[182/700] loss=0.5274 | pos=0.943 neg=0.018 | val AUC=0.7392\n",
      "[183/700] loss=0.5280 | pos=0.942 neg=0.021 | val AUC=0.7554\n",
      "[184/700] loss=0.5148 | pos=0.940 neg=-0.032 | val AUC=0.6703\n",
      "[185/700] loss=0.5240 | pos=0.940 neg=0.006 | val AUC=0.6904\n",
      "[186/700] loss=0.5263 | pos=0.942 neg=0.015 | val AUC=0.7251\n",
      "[187/700] loss=0.5215 | pos=0.941 neg=-0.005 | val AUC=0.7047\n",
      "[188/700] loss=0.5205 | pos=0.942 neg=-0.008 | val AUC=0.7293\n",
      "[189/700] loss=0.5179 | pos=0.941 neg=-0.022 | val AUC=0.7159\n",
      "[190/700] loss=0.5228 | pos=0.941 neg=-0.002 | val AUC=0.7060\n",
      "[191/700] loss=0.5170 | pos=0.939 neg=-0.019 | val AUC=0.7121\n",
      "[192/700] loss=0.5230 | pos=0.938 neg=-0.003 | val AUC=0.6971\n",
      "[193/700] loss=0.5247 | pos=0.939 neg=0.010 | val AUC=0.6700\n",
      "[194/700] loss=0.5286 | pos=0.938 neg=0.023 | val AUC=0.6224\n",
      "[195/700] loss=0.5301 | pos=0.937 neg=0.030 | val AUC=0.6990\n",
      "[196/700] loss=0.5215 | pos=0.939 neg=-0.002 | val AUC=0.7321\n",
      "[197/700] loss=0.5210 | pos=0.939 neg=-0.001 | val AUC=0.6932\n",
      "[198/700] loss=0.5199 | pos=0.938 neg=-0.006 | val AUC=0.7175\n",
      "[199/700] loss=0.5234 | pos=0.939 neg=0.006 | val AUC=0.6840\n",
      "[200/700] loss=0.5247 | pos=0.938 neg=0.012 | val AUC=0.6502\n",
      "[201/700] loss=0.5219 | pos=0.939 neg=0.002 | val AUC=0.6881\n",
      "[202/700] loss=0.5196 | pos=0.940 neg=-0.007 | val AUC=0.6288\n",
      "[203/700] loss=0.5196 | pos=0.940 neg=-0.007 | val AUC=0.6818\n",
      "[204/700] loss=0.5107 | pos=0.942 neg=-0.046 | val AUC=0.7181\n",
      "[205/700] loss=0.5246 | pos=0.942 neg=0.010 | val AUC=0.6607\n",
      "[206/700] loss=0.5174 | pos=0.945 neg=-0.015 | val AUC=0.7331\n",
      "[207/700] loss=0.5141 | pos=0.945 neg=-0.026 | val AUC=0.7054\n",
      "[208/700] loss=0.5246 | pos=0.946 neg=0.009 | val AUC=0.6754\n",
      "[209/700] loss=0.5154 | pos=0.945 neg=-0.027 | val AUC=0.7519\n",
      "[210/700] loss=0.5344 | pos=0.945 neg=0.048 | val AUC=0.6690\n",
      "[211/700] loss=0.5278 | pos=0.946 neg=0.023 | val AUC=0.7376\n",
      "[212/700] loss=0.5225 | pos=0.946 neg=0.004 | val AUC=0.7085\n",
      "[213/700] loss=0.5133 | pos=0.943 neg=-0.028 | val AUC=0.6658\n",
      "[214/700] loss=0.5225 | pos=0.944 neg=0.002 | val AUC=0.7060\n",
      "[215/700] loss=0.5221 | pos=0.942 neg=-0.001 | val AUC=0.7149\n",
      "[216/700] loss=0.5168 | pos=0.941 neg=-0.020 | val AUC=0.6952\n",
      "[217/700] loss=0.5227 | pos=0.943 neg=0.002 | val AUC=0.6987\n",
      "[218/700] loss=0.5151 | pos=0.945 neg=-0.022 | val AUC=0.7245\n",
      "[219/700] loss=0.5294 | pos=0.947 neg=0.030 | val AUC=0.6476\n",
      "[220/700] loss=0.5267 | pos=0.948 neg=0.017 | val AUC=0.6684\n",
      "[221/700] loss=0.5211 | pos=0.948 neg=0.001 | val AUC=0.6240\n",
      "[222/700] loss=0.5220 | pos=0.947 neg=0.004 | val AUC=0.6763\n",
      "[223/700] loss=0.5124 | pos=0.946 neg=-0.029 | val AUC=0.7251\n",
      "[224/700] loss=0.5160 | pos=0.947 neg=-0.017 | val AUC=0.7038\n",
      "[225/700] loss=0.5196 | pos=0.945 neg=-0.004 | val AUC=0.7427\n",
      "[226/700] loss=0.5195 | pos=0.947 neg=-0.004 | val AUC=0.6744\n",
      "[227/700] loss=0.5145 | pos=0.947 neg=-0.024 | val AUC=0.6483\n",
      "[228/700] loss=0.5172 | pos=0.947 neg=-0.014 | val AUC=0.6349\n",
      "[229/700] loss=0.5240 | pos=0.946 neg=0.011 | val AUC=0.6961\n",
      "[230/700] loss=0.5181 | pos=0.948 neg=-0.010 | val AUC=0.6677\n",
      "[231/700] loss=0.5232 | pos=0.947 neg=0.011 | val AUC=0.6531\n",
      "[232/700] loss=0.5239 | pos=0.945 neg=0.010 | val AUC=0.6473\n",
      "[233/700] loss=0.5194 | pos=0.944 neg=-0.015 | val AUC=0.7239\n",
      "[234/700] loss=0.5228 | pos=0.945 neg=-0.000 | val AUC=0.6872\n",
      "[235/700] loss=0.5232 | pos=0.946 neg=0.003 | val AUC=0.6789\n",
      "[236/700] loss=0.5227 | pos=0.947 neg=0.006 | val AUC=0.7152\n",
      "[237/700] loss=0.5198 | pos=0.947 neg=-0.004 | val AUC=0.7223\n",
      "[238/700] loss=0.5225 | pos=0.946 neg=0.002 | val AUC=0.6834\n",
      "[239/700] loss=0.5158 | pos=0.945 neg=-0.023 | val AUC=0.7140\n",
      "[240/700] loss=0.5222 | pos=0.945 neg=-0.001 | val AUC=0.7478\n",
      "[241/700] loss=0.5294 | pos=0.943 neg=0.028 | val AUC=0.6913\n",
      "[242/700] loss=0.5272 | pos=0.941 neg=0.019 | val AUC=0.6486\n",
      "[243/700] loss=0.5243 | pos=0.941 neg=0.004 | val AUC=0.7564\n",
      "[244/700] loss=0.5246 | pos=0.941 neg=0.009 | val AUC=0.7047\n",
      "[245/700] loss=0.5168 | pos=0.941 neg=-0.017 | val AUC=0.7573\n",
      "[246/700] loss=0.5167 | pos=0.940 neg=-0.023 | val AUC=0.7433\n",
      "[247/700] loss=0.5231 | pos=0.942 neg=0.004 | val AUC=0.7752\n",
      "[248/700] loss=0.5219 | pos=0.943 neg=-0.004 | val AUC=0.6802\n",
      "[249/700] loss=0.5256 | pos=0.946 neg=0.017 | val AUC=0.6786\n",
      "[250/700] loss=0.5289 | pos=0.949 neg=0.023 | val AUC=0.7723\n",
      "[251/700] loss=0.5198 | pos=0.949 neg=-0.006 | val AUC=0.7232\n",
      "[252/700] loss=0.5318 | pos=0.950 neg=0.037 | val AUC=0.7309\n",
      "[253/700] loss=0.5110 | pos=0.948 neg=-0.041 | val AUC=0.6582\n",
      "[254/700] loss=0.5241 | pos=0.949 neg=0.009 | val AUC=0.6977\n",
      "[255/700] loss=0.5245 | pos=0.948 neg=0.013 | val AUC=0.6971\n",
      "[256/700] loss=0.5225 | pos=0.950 neg=0.003 | val AUC=0.7331\n",
      "[257/700] loss=0.5180 | pos=0.951 neg=-0.014 | val AUC=0.7538\n",
      "[258/700] loss=0.5189 | pos=0.950 neg=-0.005 | val AUC=0.7034\n",
      "[259/700] loss=0.5227 | pos=0.952 neg=0.005 | val AUC=0.7095\n",
      "[260/700] loss=0.5283 | pos=0.950 neg=0.031 | val AUC=0.7207\n",
      "[261/700] loss=0.5212 | pos=0.949 neg=0.000 | val AUC=0.6834\n",
      "[262/700] loss=0.5171 | pos=0.948 neg=-0.016 | val AUC=0.7194\n",
      "[263/700] loss=0.5131 | pos=0.947 neg=-0.028 | val AUC=0.6971\n",
      "[264/700] loss=0.5185 | pos=0.948 neg=-0.012 | val AUC=0.6776\n",
      "[265/700] loss=0.5269 | pos=0.948 neg=0.024 | val AUC=0.7420\n",
      "[266/700] loss=0.5227 | pos=0.948 neg=0.008 | val AUC=0.6684\n",
      "[267/700] loss=0.5178 | pos=0.947 neg=-0.013 | val AUC=0.7248\n",
      "[268/700] loss=0.5156 | pos=0.946 neg=-0.022 | val AUC=0.7478\n",
      "[269/700] loss=0.5176 | pos=0.946 neg=-0.013 | val AUC=0.7631\n",
      "[270/700] loss=0.5193 | pos=0.944 neg=-0.005 | val AUC=0.7720\n",
      "[271/700] loss=0.5223 | pos=0.945 neg=0.009 | val AUC=0.7395\n",
      "[272/700] loss=0.5205 | pos=0.944 neg=-0.000 | val AUC=0.7704\n",
      "[273/700] loss=0.5159 | pos=0.944 neg=-0.023 | val AUC=0.7624\n",
      "[274/700] loss=0.5291 | pos=0.944 neg=0.028 | val AUC=0.7232\n",
      "[275/700] loss=0.5190 | pos=0.944 neg=-0.009 | val AUC=0.6977\n",
      "[276/700] loss=0.5281 | pos=0.944 neg=0.021 | val AUC=0.7009\n",
      "[277/700] loss=0.5167 | pos=0.945 neg=-0.018 | val AUC=0.7031\n",
      "[278/700] loss=0.5251 | pos=0.945 neg=0.013 | val AUC=0.6894\n",
      "[279/700] loss=0.5137 | pos=0.947 neg=-0.029 | val AUC=0.7401\n",
      "[280/700] loss=0.5143 | pos=0.946 neg=-0.031 | val AUC=0.6719\n",
      "[281/700] loss=0.5176 | pos=0.946 neg=-0.017 | val AUC=0.6837\n",
      "[282/700] loss=0.5257 | pos=0.946 neg=0.012 | val AUC=0.7095\n",
      "[283/700] loss=0.5202 | pos=0.945 neg=-0.002 | val AUC=0.6604\n",
      "[284/700] loss=0.5248 | pos=0.945 neg=0.013 | val AUC=0.6929\n",
      "[285/700] loss=0.5240 | pos=0.946 neg=0.010 | val AUC=0.6636\n",
      "[286/700] loss=0.5214 | pos=0.946 neg=0.005 | val AUC=0.7401\n",
      "[287/700] loss=0.5210 | pos=0.947 neg=0.000 | val AUC=0.6811\n",
      "[288/700] loss=0.5215 | pos=0.948 neg=-0.001 | val AUC=0.7210\n",
      "[289/700] loss=0.5123 | pos=0.949 neg=-0.035 | val AUC=0.6371\n",
      "[290/700] loss=0.5201 | pos=0.948 neg=-0.001 | val AUC=0.7634\n",
      "[291/700] loss=0.5120 | pos=0.948 neg=-0.034 | val AUC=0.6387\n",
      "[292/700] loss=0.5246 | pos=0.947 neg=0.017 | val AUC=0.6327\n",
      "[293/700] loss=0.5188 | pos=0.947 neg=-0.010 | val AUC=0.6569\n",
      "[294/700] loss=0.5221 | pos=0.947 neg=0.009 | val AUC=0.6945\n",
      "[295/700] loss=0.5230 | pos=0.945 neg=0.007 | val AUC=0.6492\n",
      "[296/700] loss=0.5126 | pos=0.944 neg=-0.032 | val AUC=0.7615\n",
      "[297/700] loss=0.5195 | pos=0.944 neg=-0.003 | val AUC=0.7309\n",
      "[298/700] loss=0.5147 | pos=0.944 neg=-0.026 | val AUC=0.6821\n",
      "[299/700] loss=0.5276 | pos=0.945 neg=0.024 | val AUC=0.7286\n",
      "[300/700] loss=0.5143 | pos=0.944 neg=-0.026 | val AUC=0.7321\n",
      "[301/700] loss=0.5205 | pos=0.944 neg=-0.001 | val AUC=0.7251\n",
      "[302/700] loss=0.5246 | pos=0.943 neg=0.014 | val AUC=0.7070\n",
      "[303/700] loss=0.5272 | pos=0.943 neg=0.025 | val AUC=0.7197\n",
      "[304/700] loss=0.5188 | pos=0.943 neg=-0.005 | val AUC=0.6677\n",
      "[305/700] loss=0.5168 | pos=0.945 neg=-0.016 | val AUC=0.7028\n",
      "[306/700] loss=0.5204 | pos=0.944 neg=0.002 | val AUC=0.7669\n",
      "[307/700] loss=0.5113 | pos=0.944 neg=-0.033 | val AUC=0.6798\n",
      "[308/700] loss=0.5171 | pos=0.945 neg=-0.014 | val AUC=0.6865\n",
      "[309/700] loss=0.5240 | pos=0.946 neg=0.013 | val AUC=0.7280\n",
      "[310/700] loss=0.5187 | pos=0.947 neg=-0.009 | val AUC=0.7363\n",
      "[311/700] loss=0.5182 | pos=0.949 neg=-0.011 | val AUC=0.6916\n",
      "[312/700] loss=0.5097 | pos=0.951 neg=-0.044 | val AUC=0.7009\n",
      "[313/700] loss=0.5156 | pos=0.951 neg=-0.020 | val AUC=0.7341\n",
      "[314/700] loss=0.5238 | pos=0.952 neg=0.010 | val AUC=0.6588\n",
      "[315/700] loss=0.5230 | pos=0.951 neg=0.007 | val AUC=0.6403\n",
      "[316/700] loss=0.5179 | pos=0.953 neg=-0.012 | val AUC=0.7172\n",
      "[317/700] loss=0.5215 | pos=0.951 neg=-0.000 | val AUC=0.5947\n",
      "[318/700] loss=0.5211 | pos=0.952 neg=0.003 | val AUC=0.6932\n",
      "[319/700] loss=0.5206 | pos=0.949 neg=-0.003 | val AUC=0.7114\n",
      "[320/700] loss=0.5171 | pos=0.948 neg=-0.016 | val AUC=0.7076\n",
      "[321/700] loss=0.5137 | pos=0.948 neg=-0.028 | val AUC=0.5781\n",
      "[322/700] loss=0.5244 | pos=0.947 neg=0.011 | val AUC=0.7012\n",
      "[323/700] loss=0.5164 | pos=0.948 neg=-0.018 | val AUC=0.6480\n",
      "[324/700] loss=0.5234 | pos=0.947 neg=0.006 | val AUC=0.6489\n",
      "[325/700] loss=0.5112 | pos=0.947 neg=-0.043 | val AUC=0.6543\n",
      "[326/700] loss=0.5224 | pos=0.947 neg=0.003 | val AUC=0.6712\n",
      "[327/700] loss=0.5183 | pos=0.947 neg=-0.012 | val AUC=0.7175\n",
      "[328/700] loss=0.5181 | pos=0.948 neg=-0.013 | val AUC=0.7487\n",
      "[329/700] loss=0.5265 | pos=0.948 neg=0.021 | val AUC=0.7143\n",
      "[330/700] loss=0.5164 | pos=0.948 neg=-0.019 | val AUC=0.7357\n",
      "[331/700] loss=0.5160 | pos=0.949 neg=-0.023 | val AUC=0.7290\n",
      "[332/700] loss=0.5220 | pos=0.949 neg=0.006 | val AUC=0.7545\n",
      "[333/700] loss=0.5147 | pos=0.949 neg=-0.024 | val AUC=0.7003\n",
      "[334/700] loss=0.5159 | pos=0.949 neg=-0.019 | val AUC=0.7079\n",
      "[335/700] loss=0.5268 | pos=0.949 neg=0.024 | val AUC=0.7471\n",
      "[336/700] loss=0.5190 | pos=0.950 neg=-0.007 | val AUC=0.7095\n",
      "[337/700] loss=0.5199 | pos=0.952 neg=-0.002 | val AUC=0.6958\n",
      "[338/700] loss=0.5230 | pos=0.952 neg=0.012 | val AUC=0.7404\n",
      "[339/700] loss=0.5174 | pos=0.954 neg=-0.010 | val AUC=0.7385\n",
      "[340/700] loss=0.5161 | pos=0.952 neg=-0.013 | val AUC=0.6990\n",
      "[341/700] loss=0.5190 | pos=0.952 neg=-0.006 | val AUC=0.7726\n",
      "[342/700] loss=0.5225 | pos=0.952 neg=0.008 | val AUC=0.7707\n",
      "[343/700] loss=0.5194 | pos=0.953 neg=-0.001 | val AUC=0.7235\n",
      "[344/700] loss=0.5173 | pos=0.953 neg=-0.009 | val AUC=0.7535\n",
      "[345/700] loss=0.5197 | pos=0.951 neg=-0.003 | val AUC=0.7006\n",
      "[346/700] loss=0.5226 | pos=0.950 neg=0.008 | val AUC=0.7293\n",
      "[347/700] loss=0.5186 | pos=0.948 neg=-0.007 | val AUC=0.7315\n",
      "[348/700] loss=0.5167 | pos=0.944 neg=-0.019 | val AUC=0.7344\n",
      "[349/700] loss=0.5117 | pos=0.946 neg=-0.036 | val AUC=0.7360\n",
      "[350/700] loss=0.5189 | pos=0.947 neg=-0.007 | val AUC=0.7254\n",
      "[351/700] loss=0.5153 | pos=0.947 neg=-0.016 | val AUC=0.7481\n",
      "[352/700] loss=0.5166 | pos=0.948 neg=-0.013 | val AUC=0.6849\n",
      "[353/700] loss=0.5193 | pos=0.947 neg=-0.006 | val AUC=0.6948\n",
      "[354/700] loss=0.5121 | pos=0.948 neg=-0.028 | val AUC=0.7038\n",
      "[355/700] loss=0.5128 | pos=0.947 neg=-0.028 | val AUC=0.7985\n",
      "[356/700] loss=0.5210 | pos=0.948 neg=0.004 | val AUC=0.8077\n",
      "[357/700] loss=0.5128 | pos=0.947 neg=-0.027 | val AUC=0.7452\n",
      "[358/700] loss=0.5199 | pos=0.949 neg=0.000 | val AUC=0.7044\n",
      "[359/700] loss=0.5183 | pos=0.950 neg=-0.007 | val AUC=0.7500\n",
      "[360/700] loss=0.5267 | pos=0.950 neg=0.023 | val AUC=0.7242\n",
      "[361/700] loss=0.5197 | pos=0.949 neg=-0.004 | val AUC=0.7522\n",
      "[362/700] loss=0.5202 | pos=0.949 neg=-0.001 | val AUC=0.6665\n",
      "[363/700] loss=0.5196 | pos=0.948 neg=-0.004 | val AUC=0.7484\n",
      "[364/700] loss=0.5161 | pos=0.948 neg=-0.016 | val AUC=0.7468\n",
      "[365/700] loss=0.5157 | pos=0.950 neg=-0.017 | val AUC=0.7242\n",
      "[366/700] loss=0.5148 | pos=0.949 neg=-0.024 | val AUC=0.7130\n",
      "[367/700] loss=0.5261 | pos=0.951 neg=0.020 | val AUC=0.7070\n",
      "[368/700] loss=0.5214 | pos=0.951 neg=0.002 | val AUC=0.7589\n",
      "[369/700] loss=0.5206 | pos=0.950 neg=-0.003 | val AUC=0.6569\n",
      "[370/700] loss=0.5258 | pos=0.949 neg=0.019 | val AUC=0.6792\n",
      "[371/700] loss=0.5172 | pos=0.949 neg=-0.013 | val AUC=0.6540\n",
      "[372/700] loss=0.5225 | pos=0.949 neg=0.006 | val AUC=0.6572\n",
      "[373/700] loss=0.5189 | pos=0.949 neg=-0.006 | val AUC=0.6690\n",
      "[374/700] loss=0.5180 | pos=0.948 neg=-0.012 | val AUC=0.6894\n",
      "[375/700] loss=0.5150 | pos=0.950 neg=-0.022 | val AUC=0.7149\n",
      "[376/700] loss=0.5222 | pos=0.949 neg=0.006 | val AUC=0.7513\n",
      "[377/700] loss=0.5197 | pos=0.948 neg=-0.007 | val AUC=0.7299\n",
      "[378/700] loss=0.5166 | pos=0.949 neg=-0.017 | val AUC=0.6719\n",
      "[379/700] loss=0.5245 | pos=0.948 neg=0.010 | val AUC=0.7095\n",
      "[380/700] loss=0.5239 | pos=0.948 neg=0.005 | val AUC=0.6920\n",
      "[381/700] loss=0.5216 | pos=0.947 neg=-0.001 | val AUC=0.7047\n",
      "[382/700] loss=0.5128 | pos=0.949 neg=-0.034 | val AUC=0.7079\n",
      "[383/700] loss=0.5183 | pos=0.950 neg=-0.013 | val AUC=0.6547\n",
      "[384/700] loss=0.5210 | pos=0.951 neg=-0.005 | val AUC=0.7261\n",
      "[385/700] loss=0.5218 | pos=0.950 neg=-0.003 | val AUC=0.6875\n",
      "[386/700] loss=0.5224 | pos=0.950 neg=0.002 | val AUC=0.7264\n",
      "[387/700] loss=0.5223 | pos=0.950 neg=0.000 | val AUC=0.7471\n",
      "[388/700] loss=0.5210 | pos=0.949 neg=-0.008 | val AUC=0.6575\n",
      "[389/700] loss=0.5248 | pos=0.949 neg=0.013 | val AUC=0.7704\n",
      "[390/700] loss=0.5250 | pos=0.948 neg=0.012 | val AUC=0.7194\n",
      "[391/700] loss=0.5177 | pos=0.948 neg=-0.012 | val AUC=0.7044\n",
      "[392/700] loss=0.5137 | pos=0.948 neg=-0.025 | val AUC=0.7213\n",
      "[393/700] loss=0.5280 | pos=0.948 neg=0.028 | val AUC=0.7038\n",
      "[394/700] loss=0.5203 | pos=0.948 neg=-0.004 | val AUC=0.7580\n",
      "[395/700] loss=0.5262 | pos=0.947 neg=0.021 | val AUC=0.6620\n",
      "[396/700] loss=0.5179 | pos=0.946 neg=-0.009 | val AUC=0.7226\n",
      "[397/700] loss=0.5202 | pos=0.944 neg=-0.005 | val AUC=0.6904\n",
      "[398/700] loss=0.5134 | pos=0.942 neg=-0.033 | val AUC=0.7085\n",
      "[399/700] loss=0.5219 | pos=0.942 neg=0.001 | val AUC=0.6853\n",
      "[400/700] loss=0.5184 | pos=0.941 neg=-0.013 | val AUC=0.7781\n",
      "[401/700] loss=0.5231 | pos=0.943 neg=0.007 | val AUC=0.7309\n",
      "[402/700] loss=0.5315 | pos=0.942 neg=0.041 | val AUC=0.6929\n",
      "[403/700] loss=0.5105 | pos=0.941 neg=-0.040 | val AUC=0.7057\n",
      "[404/700] loss=0.5200 | pos=0.942 neg=-0.001 | val AUC=0.6556\n",
      "[405/700] loss=0.5235 | pos=0.943 neg=0.010 | val AUC=0.7353\n",
      "[406/700] loss=0.5248 | pos=0.944 neg=0.017 | val AUC=0.7264\n",
      "[407/700] loss=0.5152 | pos=0.946 neg=-0.021 | val AUC=0.7615\n",
      "[408/700] loss=0.5195 | pos=0.947 neg=-0.003 | val AUC=0.7570\n",
      "[409/700] loss=0.5134 | pos=0.948 neg=-0.025 | val AUC=0.6945\n",
      "[410/700] loss=0.5183 | pos=0.947 neg=-0.009 | val AUC=0.6910\n",
      "[411/700] loss=0.5166 | pos=0.950 neg=-0.016 | val AUC=0.6649\n",
      "[412/700] loss=0.5145 | pos=0.950 neg=-0.027 | val AUC=0.6923\n",
      "[413/700] loss=0.5233 | pos=0.949 neg=0.009 | val AUC=0.6349\n",
      "[414/700] loss=0.5206 | pos=0.951 neg=-0.002 | val AUC=0.6948\n",
      "[415/700] loss=0.5170 | pos=0.950 neg=-0.019 | val AUC=0.7659\n",
      "[416/700] loss=0.5161 | pos=0.951 neg=-0.022 | val AUC=0.7596\n",
      "[417/700] loss=0.5161 | pos=0.950 neg=-0.020 | val AUC=0.6578\n",
      "[418/700] loss=0.5151 | pos=0.951 neg=-0.024 | val AUC=0.6936\n",
      "[419/700] loss=0.5270 | pos=0.948 neg=0.017 | val AUC=0.6993\n",
      "[420/700] loss=0.5245 | pos=0.947 neg=0.011 | val AUC=0.6964\n",
      "[421/700] loss=0.5284 | pos=0.947 neg=0.024 | val AUC=0.6897\n",
      "[422/700] loss=0.5180 | pos=0.945 neg=-0.016 | val AUC=0.6649\n",
      "[423/700] loss=0.5173 | pos=0.943 neg=-0.019 | val AUC=0.6773\n",
      "[424/700] loss=0.5109 | pos=0.944 neg=-0.044 | val AUC=0.6464\n",
      "[425/700] loss=0.5151 | pos=0.944 neg=-0.028 | val AUC=0.6849\n",
      "[426/700] loss=0.5197 | pos=0.946 neg=-0.010 | val AUC=0.6955\n",
      "[427/700] loss=0.5231 | pos=0.945 neg=0.008 | val AUC=0.7223\n",
      "[428/700] loss=0.5155 | pos=0.946 neg=-0.023 | val AUC=0.6355\n",
      "[429/700] loss=0.5243 | pos=0.945 neg=0.010 | val AUC=0.6617\n",
      "[430/700] loss=0.5197 | pos=0.947 neg=-0.007 | val AUC=0.7034\n",
      "[431/700] loss=0.5171 | pos=0.946 neg=-0.018 | val AUC=0.6907\n",
      "[432/700] loss=0.5145 | pos=0.946 neg=-0.027 | val AUC=0.6783\n",
      "[433/700] loss=0.5153 | pos=0.948 neg=-0.024 | val AUC=0.6885\n",
      "[434/700] loss=0.5182 | pos=0.947 neg=-0.009 | val AUC=0.6467\n",
      "[435/700] loss=0.5260 | pos=0.947 neg=0.017 | val AUC=0.7392\n",
      "[436/700] loss=0.5283 | pos=0.947 neg=0.027 | val AUC=0.6454\n",
      "[437/700] loss=0.5312 | pos=0.946 neg=0.033 | val AUC=0.7124\n",
      "[438/700] loss=0.5260 | pos=0.944 neg=0.015 | val AUC=0.6677\n",
      "[439/700] loss=0.5255 | pos=0.942 neg=0.012 | val AUC=0.7714\n",
      "[440/700] loss=0.5117 | pos=0.942 neg=-0.037 | val AUC=0.7009\n",
      "[441/700] loss=0.5210 | pos=0.945 neg=-0.001 | val AUC=0.6626\n",
      "[442/700] loss=0.5209 | pos=0.945 neg=-0.001 | val AUC=0.7239\n",
      "[443/700] loss=0.5174 | pos=0.946 neg=-0.013 | val AUC=0.7286\n",
      "[444/700] loss=0.5177 | pos=0.946 neg=-0.009 | val AUC=0.6330\n",
      "[445/700] loss=0.5187 | pos=0.947 neg=-0.006 | val AUC=0.7047\n",
      "[446/700] loss=0.5245 | pos=0.944 neg=0.012 | val AUC=0.7286\n",
      "[447/700] loss=0.5160 | pos=0.946 neg=-0.022 | val AUC=0.6932\n",
      "[448/700] loss=0.5181 | pos=0.948 neg=-0.012 | val AUC=0.7312\n",
      "[449/700] loss=0.5218 | pos=0.949 neg=0.002 | val AUC=0.7235\n",
      "[450/700] loss=0.5253 | pos=0.950 neg=0.019 | val AUC=0.6901\n",
      "[451/700] loss=0.5242 | pos=0.951 neg=0.009 | val AUC=0.7663\n",
      "[452/700] loss=0.5199 | pos=0.951 neg=-0.009 | val AUC=0.6358\n",
      "[453/700] loss=0.5298 | pos=0.952 neg=0.031 | val AUC=0.7943\n",
      "[454/700] loss=0.5176 | pos=0.951 neg=-0.014 | val AUC=0.7417\n",
      "[455/700] loss=0.5183 | pos=0.951 neg=-0.010 | val AUC=0.7258\n",
      "[456/700] loss=0.5229 | pos=0.951 neg=0.009 | val AUC=0.8001\n",
      "[457/700] loss=0.5217 | pos=0.950 neg=0.003 | val AUC=0.7510\n",
      "[458/700] loss=0.5163 | pos=0.949 neg=-0.019 | val AUC=0.7653\n",
      "[459/700] loss=0.5199 | pos=0.950 neg=0.000 | val AUC=0.6445\n",
      "[460/700] loss=0.5230 | pos=0.949 neg=0.008 | val AUC=0.6942\n",
      "[461/700] loss=0.5229 | pos=0.950 neg=0.011 | val AUC=0.6942\n",
      "[462/700] loss=0.5201 | pos=0.950 neg=-0.003 | val AUC=0.6990\n",
      "[463/700] loss=0.5135 | pos=0.949 neg=-0.024 | val AUC=0.7395\n",
      "[464/700] loss=0.5174 | pos=0.949 neg=-0.011 | val AUC=0.6952\n",
      "[465/700] loss=0.5151 | pos=0.948 neg=-0.019 | val AUC=0.6913\n",
      "[466/700] loss=0.5216 | pos=0.948 neg=0.002 | val AUC=0.6901\n",
      "[467/700] loss=0.5164 | pos=0.948 neg=-0.015 | val AUC=0.6942\n",
      "[468/700] loss=0.5214 | pos=0.948 neg=0.001 | val AUC=0.7481\n",
      "[469/700] loss=0.5260 | pos=0.948 neg=0.019 | val AUC=0.7251\n",
      "[470/700] loss=0.5189 | pos=0.947 neg=-0.007 | val AUC=0.7414\n",
      "[471/700] loss=0.5190 | pos=0.948 neg=-0.006 | val AUC=0.7277\n",
      "[472/700] loss=0.5214 | pos=0.948 neg=0.005 | val AUC=0.7602\n",
      "[473/700] loss=0.5143 | pos=0.949 neg=-0.025 | val AUC=0.7321\n",
      "[474/700] loss=0.5231 | pos=0.949 neg=0.011 | val AUC=0.7028\n",
      "[475/700] loss=0.5209 | pos=0.947 neg=0.000 | val AUC=0.7519\n",
      "[476/700] loss=0.5275 | pos=0.948 neg=0.026 | val AUC=0.7446\n",
      "[477/700] loss=0.5309 | pos=0.948 neg=0.036 | val AUC=0.6476\n",
      "[478/700] loss=0.5212 | pos=0.947 neg=0.001 | val AUC=0.6601\n",
      "[479/700] loss=0.5211 | pos=0.946 neg=0.002 | val AUC=0.7152\n",
      "[480/700] loss=0.5197 | pos=0.948 neg=0.000 | val AUC=0.7357\n",
      "[481/700] loss=0.5189 | pos=0.948 neg=-0.007 | val AUC=0.7325\n",
      "[482/700] loss=0.5194 | pos=0.949 neg=-0.003 | val AUC=0.7433\n",
      "[483/700] loss=0.5238 | pos=0.951 neg=0.011 | val AUC=0.7219\n",
      "[484/700] loss=0.5178 | pos=0.949 neg=-0.009 | val AUC=0.7283\n",
      "[485/700] loss=0.5305 | pos=0.949 neg=0.031 | val AUC=0.7117\n",
      "[486/700] loss=0.5165 | pos=0.949 neg=-0.014 | val AUC=0.6534\n",
      "[487/700] loss=0.5235 | pos=0.946 neg=0.008 | val AUC=0.7057\n",
      "[488/700] loss=0.5189 | pos=0.945 neg=-0.006 | val AUC=0.6783\n",
      "[489/700] loss=0.5122 | pos=0.945 neg=-0.030 | val AUC=0.6862\n",
      "[490/700] loss=0.5192 | pos=0.946 neg=-0.007 | val AUC=0.6566\n",
      "[491/700] loss=0.5151 | pos=0.945 neg=-0.019 | val AUC=0.7168\n",
      "[492/700] loss=0.5225 | pos=0.945 neg=0.004 | val AUC=0.6888\n",
      "[493/700] loss=0.5184 | pos=0.947 neg=-0.012 | val AUC=0.6623\n",
      "[494/700] loss=0.5171 | pos=0.948 neg=-0.011 | val AUC=0.6856\n",
      "[495/700] loss=0.5262 | pos=0.948 neg=0.020 | val AUC=0.6559\n",
      "[496/700] loss=0.5151 | pos=0.948 neg=-0.019 | val AUC=0.6741\n",
      "[497/700] loss=0.5275 | pos=0.948 neg=0.025 | val AUC=0.7143\n",
      "[498/700] loss=0.5131 | pos=0.947 neg=-0.029 | val AUC=0.7411\n",
      "[499/700] loss=0.5164 | pos=0.947 neg=-0.018 | val AUC=0.6987\n",
      "[500/700] loss=0.5198 | pos=0.947 neg=-0.002 | val AUC=0.7628\n",
      "[501/700] loss=0.5254 | pos=0.947 neg=0.017 | val AUC=0.7299\n",
      "[502/700] loss=0.5227 | pos=0.948 neg=0.005 | val AUC=0.7395\n",
      "[503/700] loss=0.5177 | pos=0.949 neg=-0.011 | val AUC=0.7318\n",
      "[504/700] loss=0.5178 | pos=0.948 neg=-0.013 | val AUC=0.7746\n",
      "[505/700] loss=0.5189 | pos=0.950 neg=-0.007 | val AUC=0.7372\n",
      "[506/700] loss=0.5161 | pos=0.949 neg=-0.015 | val AUC=0.7130\n",
      "[507/700] loss=0.5180 | pos=0.951 neg=-0.007 | val AUC=0.6859\n",
      "[508/700] loss=0.5147 | pos=0.952 neg=-0.021 | val AUC=0.6894\n",
      "[509/700] loss=0.5220 | pos=0.951 neg=0.005 | val AUC=0.7357\n",
      "[510/700] loss=0.5266 | pos=0.951 neg=0.025 | val AUC=0.6614\n",
      "[511/700] loss=0.5225 | pos=0.952 neg=0.008 | val AUC=0.7060\n",
      "[512/700] loss=0.5139 | pos=0.950 neg=-0.025 | val AUC=0.6932\n",
      "[513/700] loss=0.5203 | pos=0.950 neg=-0.001 | val AUC=0.7114\n",
      "[514/700] loss=0.5192 | pos=0.951 neg=-0.004 | val AUC=0.6760\n",
      "[515/700] loss=0.5169 | pos=0.949 neg=-0.015 | val AUC=0.6805\n",
      "[516/700] loss=0.5207 | pos=0.948 neg=0.003 | val AUC=0.7309\n",
      "[517/700] loss=0.5196 | pos=0.950 neg=-0.008 | val AUC=0.6690\n",
      "[518/700] loss=0.5253 | pos=0.949 neg=0.013 | val AUC=0.6885\n",
      "[519/700] loss=0.5156 | pos=0.952 neg=-0.019 | val AUC=0.6814\n",
      "[520/700] loss=0.5118 | pos=0.950 neg=-0.035 | val AUC=0.7047\n",
      "[521/700] loss=0.5280 | pos=0.948 neg=0.023 | val AUC=0.6703\n",
      "[522/700] loss=0.5160 | pos=0.948 neg=-0.020 | val AUC=0.6843\n",
      "[523/700] loss=0.5186 | pos=0.948 neg=-0.010 | val AUC=0.6932\n",
      "[524/700] loss=0.5154 | pos=0.947 neg=-0.020 | val AUC=0.6897\n",
      "[525/700] loss=0.5176 | pos=0.945 neg=-0.020 | val AUC=0.7140\n",
      "[526/700] loss=0.5169 | pos=0.945 neg=-0.016 | val AUC=0.6837\n",
      "[527/700] loss=0.5176 | pos=0.944 neg=-0.010 | val AUC=0.6808\n",
      "[528/700] loss=0.5219 | pos=0.943 neg=0.002 | val AUC=0.6967\n",
      "[529/700] loss=0.5179 | pos=0.944 neg=-0.013 | val AUC=0.7363\n",
      "[530/700] loss=0.5156 | pos=0.945 neg=-0.020 | val AUC=0.7076\n",
      "[531/700] loss=0.5210 | pos=0.946 neg=-0.001 | val AUC=0.7341\n",
      "[532/700] loss=0.5230 | pos=0.948 neg=0.007 | val AUC=0.7070\n",
      "[533/700] loss=0.5182 | pos=0.947 neg=-0.013 | val AUC=0.6859\n",
      "[534/700] loss=0.5160 | pos=0.947 neg=-0.020 | val AUC=0.6741\n",
      "[535/700] loss=0.5133 | pos=0.949 neg=-0.030 | val AUC=0.7098\n",
      "[536/700] loss=0.5229 | pos=0.948 neg=0.006 | val AUC=0.6932\n",
      "[537/700] loss=0.5159 | pos=0.948 neg=-0.019 | val AUC=0.7226\n",
      "[538/700] loss=0.5187 | pos=0.945 neg=-0.010 | val AUC=0.7258\n",
      "[539/700] loss=0.5201 | pos=0.943 neg=-0.004 | val AUC=0.6687\n",
      "[540/700] loss=0.5191 | pos=0.945 neg=-0.011 | val AUC=0.7628\n",
      "[541/700] loss=0.5179 | pos=0.942 neg=-0.013 | val AUC=0.6961\n",
      "[542/700] loss=0.5244 | pos=0.944 neg=0.009 | val AUC=0.6783\n",
      "[543/700] loss=0.5188 | pos=0.943 neg=-0.009 | val AUC=0.7643\n",
      "[544/700] loss=0.5102 | pos=0.944 neg=-0.040 | val AUC=0.7487\n",
      "[545/700] loss=0.5158 | pos=0.947 neg=-0.021 | val AUC=0.6578\n",
      "[546/700] loss=0.5201 | pos=0.945 neg=-0.004 | val AUC=0.7258\n",
      "[547/700] loss=0.5295 | pos=0.944 neg=0.030 | val AUC=0.6993\n",
      "[548/700] loss=0.5205 | pos=0.946 neg=-0.004 | val AUC=0.6983\n",
      "[549/700] loss=0.5232 | pos=0.945 neg=0.006 | val AUC=0.7038\n",
      "[550/700] loss=0.5156 | pos=0.946 neg=-0.016 | val AUC=0.7363\n",
      "[551/700] loss=0.5155 | pos=0.945 neg=-0.021 | val AUC=0.6977\n",
      "[552/700] loss=0.5194 | pos=0.947 neg=-0.005 | val AUC=0.6725\n",
      "[553/700] loss=0.5202 | pos=0.947 neg=-0.002 | val AUC=0.7009\n",
      "[554/700] loss=0.5158 | pos=0.944 neg=-0.018 | val AUC=0.7248\n",
      "[555/700] loss=0.5215 | pos=0.945 neg=0.003 | val AUC=0.7264\n",
      "[556/700] loss=0.5243 | pos=0.946 neg=0.014 | val AUC=0.7309\n",
      "[557/700] loss=0.5189 | pos=0.946 neg=-0.008 | val AUC=0.7624\n",
      "[558/700] loss=0.5201 | pos=0.946 neg=-0.000 | val AUC=0.7532\n",
      "[559/700] loss=0.5222 | pos=0.945 neg=0.006 | val AUC=0.7066\n",
      "[560/700] loss=0.5236 | pos=0.947 neg=0.011 | val AUC=0.6964\n",
      "[561/700] loss=0.5189 | pos=0.946 neg=-0.008 | val AUC=0.7085\n",
      "[562/700] loss=0.5307 | pos=0.946 neg=0.037 | val AUC=0.7404\n",
      "[563/700] loss=0.5163 | pos=0.947 neg=-0.014 | val AUC=0.7554\n",
      "[564/700] loss=0.5185 | pos=0.948 neg=-0.009 | val AUC=0.6837\n",
      "[565/700] loss=0.5212 | pos=0.949 neg=0.007 | val AUC=0.6588\n",
      "[566/700] loss=0.5155 | pos=0.949 neg=-0.017 | val AUC=0.6961\n",
      "[567/700] loss=0.5212 | pos=0.949 neg=0.001 | val AUC=0.6961\n",
      "[568/700] loss=0.5194 | pos=0.950 neg=-0.004 | val AUC=0.6531\n",
      "[569/700] loss=0.5167 | pos=0.949 neg=-0.013 | val AUC=0.6779\n",
      "[570/700] loss=0.5123 | pos=0.951 neg=-0.031 | val AUC=0.6282\n",
      "[571/700] loss=0.5234 | pos=0.949 neg=0.013 | val AUC=0.7226\n",
      "[572/700] loss=0.5238 | pos=0.951 neg=0.015 | val AUC=0.6221\n",
      "[573/700] loss=0.5129 | pos=0.952 neg=-0.029 | val AUC=0.6543\n",
      "[574/700] loss=0.5194 | pos=0.952 neg=-0.007 | val AUC=0.6837\n",
      "[575/700] loss=0.5159 | pos=0.949 neg=-0.015 | val AUC=0.6875\n",
      "[576/700] loss=0.5158 | pos=0.951 neg=-0.015 | val AUC=0.6732\n",
      "[577/700] loss=0.5153 | pos=0.951 neg=-0.018 | val AUC=0.6384\n",
      "[578/700] loss=0.5249 | pos=0.952 neg=0.018 | val AUC=0.6843\n",
      "[579/700] loss=0.5148 | pos=0.949 neg=-0.019 | val AUC=0.6508\n",
      "[580/700] loss=0.5199 | pos=0.950 neg=-0.002 | val AUC=0.6480\n",
      "[581/700] loss=0.5085 | pos=0.950 neg=-0.042 | val AUC=0.7121\n",
      "[582/700] loss=0.5165 | pos=0.948 neg=-0.014 | val AUC=0.7596\n",
      "[583/700] loss=0.5192 | pos=0.949 neg=-0.003 | val AUC=0.7188\n",
      "[584/700] loss=0.5168 | pos=0.949 neg=-0.015 | val AUC=0.7191\n",
      "[585/700] loss=0.5176 | pos=0.949 neg=-0.012 | val AUC=0.7028\n",
      "[586/700] loss=0.5125 | pos=0.949 neg=-0.028 | val AUC=0.7057\n",
      "[587/700] loss=0.5135 | pos=0.950 neg=-0.022 | val AUC=0.7076\n",
      "[588/700] loss=0.5230 | pos=0.948 neg=0.006 | val AUC=0.7710\n",
      "[589/700] loss=0.5148 | pos=0.950 neg=-0.020 | val AUC=0.7280\n",
      "[590/700] loss=0.5213 | pos=0.951 neg=0.006 | val AUC=0.7325\n",
      "[591/700] loss=0.5118 | pos=0.951 neg=-0.036 | val AUC=0.7258\n",
      "[592/700] loss=0.5165 | pos=0.951 neg=-0.015 | val AUC=0.6853\n",
      "[593/700] loss=0.5316 | pos=0.950 neg=0.040 | val AUC=0.7586\n",
      "[594/700] loss=0.5163 | pos=0.951 neg=-0.015 | val AUC=0.7197\n",
      "[595/700] loss=0.5203 | pos=0.951 neg=-0.002 | val AUC=0.7325\n",
      "[596/700] loss=0.5166 | pos=0.949 neg=-0.022 | val AUC=0.7784\n",
      "[597/700] loss=0.5112 | pos=0.951 neg=-0.038 | val AUC=0.7513\n",
      "[598/700] loss=0.5210 | pos=0.953 neg=-0.001 | val AUC=0.7245\n",
      "[599/700] loss=0.5260 | pos=0.953 neg=0.020 | val AUC=0.7408\n",
      "[600/700] loss=0.5238 | pos=0.953 neg=0.012 | val AUC=0.7650\n",
      "[601/700] loss=0.5139 | pos=0.954 neg=-0.022 | val AUC=0.7631\n",
      "[602/700] loss=0.5227 | pos=0.953 neg=0.010 | val AUC=0.7720\n",
      "[603/700] loss=0.5131 | pos=0.952 neg=-0.026 | val AUC=0.7510\n",
      "[604/700] loss=0.5267 | pos=0.951 neg=0.026 | val AUC=0.7761\n",
      "[605/700] loss=0.5212 | pos=0.950 neg=0.001 | val AUC=0.7184\n",
      "[606/700] loss=0.5235 | pos=0.948 neg=0.010 | val AUC=0.7235\n",
      "[607/700] loss=0.5222 | pos=0.949 neg=0.004 | val AUC=0.7168\n",
      "[608/700] loss=0.5167 | pos=0.948 neg=-0.013 | val AUC=0.6795\n",
      "[609/700] loss=0.5272 | pos=0.947 neg=0.030 | val AUC=0.7943\n",
      "[610/700] loss=0.5212 | pos=0.946 neg=0.004 | val AUC=0.7312\n",
      "[611/700] loss=0.5198 | pos=0.944 neg=-0.003 | val AUC=0.7663\n",
      "[612/700] loss=0.5207 | pos=0.940 neg=0.001 | val AUC=0.7219\n",
      "[613/700] loss=0.5119 | pos=0.941 neg=-0.035 | val AUC=0.7181\n",
      "[614/700] loss=0.5176 | pos=0.941 neg=-0.010 | val AUC=0.7720\n",
      "[615/700] loss=0.5239 | pos=0.941 neg=0.006 | val AUC=0.7803\n",
      "[616/700] loss=0.5191 | pos=0.942 neg=-0.008 | val AUC=0.7229\n",
      "[617/700] loss=0.5193 | pos=0.943 neg=-0.009 | val AUC=0.7637\n",
      "[618/700] loss=0.5195 | pos=0.945 neg=-0.006 | val AUC=0.7726\n",
      "[619/700] loss=0.5232 | pos=0.945 neg=0.010 | val AUC=0.7790\n",
      "[620/700] loss=0.5167 | pos=0.947 neg=-0.012 | val AUC=0.7761\n",
      "[621/700] loss=0.5216 | pos=0.949 neg=0.001 | val AUC=0.8090\n",
      "[622/700] loss=0.5172 | pos=0.949 neg=-0.011 | val AUC=0.7915\n",
      "[623/700] loss=0.5137 | pos=0.948 neg=-0.025 | val AUC=0.7758\n",
      "[624/700] loss=0.5160 | pos=0.950 neg=-0.018 | val AUC=0.8017\n",
      "[625/700] loss=0.5171 | pos=0.951 neg=-0.013 | val AUC=0.7309\n",
      "[626/700] loss=0.5219 | pos=0.951 neg=0.004 | val AUC=0.7822\n",
      "[627/700] loss=0.5208 | pos=0.951 neg=0.003 | val AUC=0.8074\n",
      "[628/700] loss=0.5217 | pos=0.949 neg=0.002 | val AUC=0.8013\n",
      "[629/700] loss=0.5144 | pos=0.950 neg=-0.026 | val AUC=0.8476\n",
      "[630/700] loss=0.5145 | pos=0.949 neg=-0.023 | val AUC=0.8288\n",
      "[631/700] loss=0.5287 | pos=0.950 neg=0.031 | val AUC=0.7739\n",
      "[632/700] loss=0.5177 | pos=0.949 neg=-0.011 | val AUC=0.7070\n",
      "[633/700] loss=0.5198 | pos=0.948 neg=-0.001 | val AUC=0.7411\n",
      "[634/700] loss=0.5167 | pos=0.949 neg=-0.014 | val AUC=0.7924\n",
      "[635/700] loss=0.5113 | pos=0.949 neg=-0.032 | val AUC=0.7771\n",
      "[636/700] loss=0.5227 | pos=0.951 neg=0.013 | val AUC=0.8074\n",
      "[637/700] loss=0.5246 | pos=0.951 neg=0.018 | val AUC=0.7535\n",
      "[638/700] loss=0.5157 | pos=0.952 neg=-0.017 | val AUC=0.8026\n",
      "[639/700] loss=0.5174 | pos=0.950 neg=-0.010 | val AUC=0.7082\n",
      "[640/700] loss=0.5148 | pos=0.950 neg=-0.022 | val AUC=0.6971\n",
      "[641/700] loss=0.5195 | pos=0.952 neg=-0.003 | val AUC=0.7774\n",
      "[642/700] loss=0.5170 | pos=0.951 neg=-0.011 | val AUC=0.7239\n",
      "[643/700] loss=0.5225 | pos=0.950 neg=0.008 | val AUC=0.7203\n",
      "[644/700] loss=0.5168 | pos=0.949 neg=-0.012 | val AUC=0.8007\n",
      "[645/700] loss=0.5146 | pos=0.949 neg=-0.023 | val AUC=0.7538\n",
      "[646/700] loss=0.5258 | pos=0.951 neg=0.023 | val AUC=0.7025\n",
      "[647/700] loss=0.5091 | pos=0.950 neg=-0.044 | val AUC=0.7121\n",
      "[648/700] loss=0.5220 | pos=0.952 neg=0.006 | val AUC=0.7261\n",
      "[649/700] loss=0.5209 | pos=0.954 neg=0.004 | val AUC=0.7541\n",
      "[650/700] loss=0.5170 | pos=0.952 neg=-0.009 | val AUC=0.8087\n",
      "[651/700] loss=0.5207 | pos=0.952 neg=0.004 | val AUC=0.7573\n",
      "[652/700] loss=0.5235 | pos=0.951 neg=0.014 | val AUC=0.7529\n",
      "[653/700] loss=0.5224 | pos=0.950 neg=0.008 | val AUC=0.7490\n",
      "[654/700] loss=0.5270 | pos=0.951 neg=0.028 | val AUC=0.6400\n",
      "[655/700] loss=0.5161 | pos=0.951 neg=-0.019 | val AUC=0.7127\n",
      "[656/700] loss=0.5178 | pos=0.951 neg=-0.013 | val AUC=0.7758\n",
      "[657/700] loss=0.5202 | pos=0.952 neg=-0.001 | val AUC=0.7159\n",
      "[658/700] loss=0.5172 | pos=0.954 neg=-0.010 | val AUC=0.7959\n",
      "[659/700] loss=0.5111 | pos=0.953 neg=-0.038 | val AUC=0.7500\n",
      "[660/700] loss=0.5223 | pos=0.954 neg=0.007 | val AUC=0.7787\n",
      "[661/700] loss=0.5217 | pos=0.953 neg=0.006 | val AUC=0.8033\n",
      "[662/700] loss=0.5159 | pos=0.952 neg=-0.018 | val AUC=0.7726\n",
      "[663/700] loss=0.5251 | pos=0.952 neg=0.019 | val AUC=0.6990\n",
      "[664/700] loss=0.5138 | pos=0.952 neg=-0.024 | val AUC=0.7797\n",
      "[665/700] loss=0.5162 | pos=0.951 neg=-0.016 | val AUC=0.7592\n",
      "[666/700] loss=0.5155 | pos=0.950 neg=-0.017 | val AUC=0.7379\n",
      "[667/700] loss=0.5169 | pos=0.950 neg=-0.009 | val AUC=0.7988\n",
      "[668/700] loss=0.5130 | pos=0.948 neg=-0.027 | val AUC=0.7382\n",
      "[669/700] loss=0.5142 | pos=0.947 neg=-0.024 | val AUC=0.7449\n",
      "[670/700] loss=0.5210 | pos=0.948 neg=0.003 | val AUC=0.7784\n",
      "[671/700] loss=0.5227 | pos=0.947 neg=0.009 | val AUC=0.7857\n",
      "[672/700] loss=0.5209 | pos=0.948 neg=0.002 | val AUC=0.7344\n",
      "[673/700] loss=0.5261 | pos=0.949 neg=0.023 | val AUC=0.7341\n",
      "[674/700] loss=0.5154 | pos=0.948 neg=-0.015 | val AUC=0.7526\n",
      "[675/700] loss=0.5237 | pos=0.950 neg=0.016 | val AUC=0.7736\n",
      "[676/700] loss=0.5201 | pos=0.949 neg=0.001 | val AUC=0.7950\n",
      "[677/700] loss=0.5243 | pos=0.949 neg=0.015 | val AUC=0.7385\n",
      "[678/700] loss=0.5237 | pos=0.949 neg=0.012 | val AUC=0.7765\n",
      "[679/700] loss=0.5212 | pos=0.950 neg=0.005 | val AUC=0.6489\n",
      "[680/700] loss=0.5240 | pos=0.949 neg=0.012 | val AUC=0.6987\n",
      "[681/700] loss=0.5160 | pos=0.947 neg=-0.015 | val AUC=0.7481\n",
      "[682/700] loss=0.5149 | pos=0.949 neg=-0.020 | val AUC=0.7328\n",
      "[683/700] loss=0.5226 | pos=0.949 neg=0.011 | val AUC=0.7631\n",
      "[684/700] loss=0.5146 | pos=0.949 neg=-0.020 | val AUC=0.7392\n",
      "[685/700] loss=0.5196 | pos=0.949 neg=0.003 | val AUC=0.7191\n",
      "[686/700] loss=0.5095 | pos=0.949 neg=-0.038 | val AUC=0.6955\n",
      "[687/700] loss=0.5228 | pos=0.950 neg=0.011 | val AUC=0.6725\n",
      "[688/700] loss=0.5175 | pos=0.950 neg=-0.010 | val AUC=0.6722\n",
      "[689/700] loss=0.5179 | pos=0.950 neg=-0.011 | val AUC=0.7529\n",
      "[690/700] loss=0.5236 | pos=0.950 neg=0.006 | val AUC=0.7031\n",
      "[691/700] loss=0.5181 | pos=0.949 neg=-0.007 | val AUC=0.7175\n",
      "[692/700] loss=0.5177 | pos=0.951 neg=-0.008 | val AUC=0.7188\n",
      "[693/700] loss=0.5221 | pos=0.952 neg=0.006 | val AUC=0.7041\n",
      "[694/700] loss=0.5152 | pos=0.953 neg=-0.018 | val AUC=0.7819\n",
      "[695/700] loss=0.5216 | pos=0.952 neg=0.006 | val AUC=0.7331\n",
      "[696/700] loss=0.5250 | pos=0.953 neg=0.021 | val AUC=0.7510\n",
      "[697/700] loss=0.5174 | pos=0.954 neg=-0.008 | val AUC=0.7070\n",
      "[698/700] loss=0.5220 | pos=0.955 neg=0.012 | val AUC=0.6818\n",
      "[699/700] loss=0.5187 | pos=0.955 neg=-0.004 | val AUC=0.7325\n",
      "[700/700] loss=0.5178 | pos=0.956 neg=-0.008 | val AUC=0.7146\n",
      "[OK] saved GAT label embeddings → Amazon_products/label_emb_tf  shape=(531, 3473)\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 학습 유틸: 음성 엣지 샘플/로스\n",
    "# ---------------------------\n",
    "def to_upper_pos_edges(A):\n",
    "    pos = []\n",
    "    N = A.shape[0]\n",
    "    for i in range(N):\n",
    "        for j in range(i+1, N):\n",
    "            if A[i, j] == 1:\n",
    "                pos.append((i, j))\n",
    "    return pos\n",
    "\n",
    "def sample_neg(A, k):\n",
    "    N = A.shape[0]\n",
    "    neg = set()\n",
    "    while len(neg) < k:\n",
    "        u = np.random.randint(0, N); v = np.random.randint(0, N)\n",
    "        if u == v: continue\n",
    "        a, b = (u, v) if u < v else (v, u)\n",
    "        if A[a, b] == 0:\n",
    "            neg.add((a, b))\n",
    "    return list(neg)\n",
    "\n",
    "def sample_neg_excluding(A, k, exclude_edges):\n",
    "    \"\"\"\n",
    "    A: np.array [N,N]  (0/1)\n",
    "    k: 뽑을 음성 개수\n",
    "    exclude_edges: {(u,v), ...}  무조건 빼야 하는 양성(또는 금지) 엣지들 (u<v 형태로 넣기)\n",
    "    \"\"\"\n",
    "    N = A.shape[0]\n",
    "    neg = set()\n",
    "    while len(neg) < k:\n",
    "        u = np.random.randint(0, N); v = np.random.randint(0, N)\n",
    "        if u == v:\n",
    "            continue\n",
    "        a, b = (u, v) if u < v else (v, u)\n",
    "        if A[a, b] == 0 and (a, b) not in exclude_edges:\n",
    "            neg.add((a, b))\n",
    "    return list(neg)\n",
    "\n",
    "\n",
    "def edge_score(z, edges):\n",
    "    u = torch.tensor([a for a, _ in edges], device=z.device, dtype=torch.long)\n",
    "    v = torch.tensor([b for _, b in edges], device=z.device, dtype=torch.long)\n",
    "    return (z[u] * z[v]).sum(dim=1)  # 내적 디코더\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "def eval_auc(z, pos_edges, A_full, k_factor=1.0):\n",
    "    z = F.normalize(z, p=2, dim=1)\n",
    "    neg_edges = sample_neg(A_full, int(len(pos_edges) * k_factor))\n",
    "    s = torch.cat([edge_score(z, pos_edges), edge_score(z, neg_edges)]).detach().cpu().numpy()\n",
    "    y = np.concatenate([np.ones(len(pos_edges)), np.zeros(len(neg_edges))])\n",
    "    return roc_auc_score(y, s)\n",
    "\n",
    "hidden_dim=64\n",
    "out_dim=3472\n",
    "heads1=8\n",
    "heads2=8\n",
    "dropout=0.2\n",
    "epochs=700\n",
    "lr=1e-3\n",
    "weight_decay=5e-4\n",
    "neg_ratio=1.0\n",
    "eval_every=20\n",
    "use_full_graph_for_final=True\n",
    "pad_width=2\n",
    "normalize_out = True\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "ids = np.arange(len(label_keys), dtype=np.int64)\n",
    "X = np.vstack([label_embeddings[k] for k in label_keys]).astype(np.float32)\n",
    "X = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "\n",
    "N, d0 = X.shape\n",
    "pos_edges = to_upper_pos_edges(A)\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "idx = rng.permutation(len(pos_edges))\n",
    "n_val = max(1, int(0.1 * len(pos_edges)))          # 10% val\n",
    "pos_val = [pos_edges[i] for i in idx[:n_val]]\n",
    "pos_train = [pos_edges[i] for i in idx[n_val:]]\n",
    "\n",
    "# train 그래프만으로 학습(누출 방지)\n",
    "A_train = np.zeros_like(A)\n",
    "for u, v in pos_train:\n",
    "    A_train[u, v] = 1; A_train[v, u] = 1\n",
    "\n",
    "adj_train = torch.tensor(A_train, dtype=torch.float32, device=device)\n",
    "# 텐서\n",
    "x = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "adj = torch.tensor(A, dtype=torch.float32, device=device)  # softmax 마스크용\n",
    "\n",
    "model = GATEncoder(in_dim=d0, hid_dim=hidden_dim, out_dim=out_dim, heads1=heads1, heads2=heads2, dropout=dropout).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "bce = nn.BCEWithLogitsLoss()\n",
    "\n",
    "maxauc = 0\n",
    "best_ckpt = \"Amazon_products/best_gat.ckpt\"\n",
    "# 금지 엣지 집합 (train+val 모두)\n",
    "forbidden = set()\n",
    "for u, v in pos_edges:        # pos_edges = train+val 전체\n",
    "    a, b = (u, v) if u < v else (v, u)\n",
    "    forbidden.add((a, b))\n",
    "\n",
    "for ep in range(1, epochs+1):\n",
    "    model.train()\n",
    "    # 🔴 여기서 전체 adj 말고 train용 adj만 본다\n",
    "    z = model(x, adj_train)                          # [N, out_dim]\n",
    "    if normalize_out:\n",
    "        z = F.normalize(z, p=2, dim=1)\n",
    "\n",
    "    # 🔴 실제로 학습에 쓰는 양성 수 기준으로 음성 수 결정\n",
    "    num_pos = len(pos_train)\n",
    "    num_neg = int(num_pos * neg_ratio)\n",
    "    # 🔴 train 그래프 기준으로 뽑되, train+val 양성은 무조건 제외\n",
    "    neg_edges = sample_neg_excluding(A_train, num_neg, forbidden)\n",
    "\n",
    "    score_pos = edge_score(z, pos_train)\n",
    "    score_neg = edge_score(z, neg_edges)\n",
    "    scores = torch.cat([score_pos, score_neg], dim=0)\n",
    "    labels = torch.cat([torch.ones_like(score_pos), torch.zeros_like(score_neg)], dim=0)\n",
    "\n",
    "    loss = bce(scores, labels)\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    # 평가 부분은 거의 그대로\n",
    "    if ep % 1 == 0 or ep == 1:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # val은 여전히 train 그래프로 임베딩\n",
    "            z_val = F.normalize(model(x, adj_train), p=2, dim=1)\n",
    "            auc_val = eval_auc(z_val, pos_val, A, k_factor=1.0)\n",
    "        print(f\"[{ep:03d}/{epochs}] loss={loss.item():.4f} | \"\n",
    "              f\"pos={score_pos.mean().item():.3f} neg={score_neg.mean().item():.3f} | \"\n",
    "              f\"val AUC={auc_val:.4f}\")\n",
    "        if maxauc < auc_val:\n",
    "            maxauc = auc_val\n",
    "            torch.save(model.state_dict(), best_ckpt)\n",
    "\n",
    "model.load_state_dict(torch.load(best_ckpt, weights_only=True))\n",
    "\n",
    "# 최종 임베딩 추출\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    z = model(x, adj)\n",
    "    if normalize_out:\n",
    "        z = F.normalize(z, p=2, dim=1)\n",
    "    Z = z.detach().cpu().numpy()  # [N, out_dim]\n",
    "OUT_CSV = \"Amazon_products/label_emb_tf\"\n",
    "# CSV 저장 (id + feat00..)\n",
    "pad = max(2, len(str(out_dim-1)))\n",
    "feat_cols = [f\"feat{str(i).zfill(pad)}\" for i in range(out_dim)]\n",
    "df = pd.DataFrame(Z, columns=feat_cols)\n",
    "df.insert(0, \"id\", ids)\n",
    "df.to_csv(OUT_CSV, index=False)\n",
    "print(f\"[OK] saved GAT label embeddings → {OUT_CSV}  shape={df.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3db08a7-6154-49f7-b54f-07b725579112",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T04:31:37.507284Z",
     "iopub.status.busy": "2025-11-12T04:31:37.507062Z",
     "iopub.status.idle": "2025-11-12T04:31:39.823709Z",
     "shell.execute_reply": "2025-11-12T04:31:39.823256Z",
     "shell.execute_reply.started": "2025-11-12T04:31:37.507268Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29487, 3472)\n"
     ]
    }
   ],
   "source": [
    "def load_docs_txt(path):\n",
    "    \"\"\"\n",
    "    'idx<TAB>text' 형태의 파일을 읽어서\n",
    "    ids: [int, ...]\n",
    "    texts: [str, ...]\n",
    "    을 리턴\n",
    "    \"\"\"\n",
    "    ids = []\n",
    "    texts = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            # 탭 기준\n",
    "            idx_str, txt = line.split(\"\\t\", 1)\n",
    "            ids.append(int(idx_str))\n",
    "            texts.append(txt)\n",
    "    return ids, texts\n",
    "\n",
    "\n",
    "\n",
    "def build_doc_embeddings_from_bm25_vectorizer(doc_texts, bm25):\n",
    "    \"\"\"\n",
    "    doc_texts: 전처리 전의 원문 리스트\n",
    "    bm25: 라벨들에 대해 먼저 fit 되어 있는 BM25Vectorizer\n",
    "          (= 라벨 코퍼스로 vocab, idf, avgdl이 이미 정해져 있어야 함)\n",
    "    return: dense numpy array [N_docs, vocab_size]\n",
    "    \"\"\"\n",
    "    # 라벨 전처리랑 똑같이\n",
    "    cleaned_docs = [preprocess_label_text(t) for t in doc_texts]\n",
    "    tokenized_docs = [t.split() for t in cleaned_docs]\n",
    "\n",
    "    # 라벨과 같은 vocab으로 BM25 임베딩\n",
    "    doc_bm25 = bm25.transform(tokenized_docs)   # shape: (N_docs, vocab)\n",
    "    doc_bm25 = doc_bm25.astype(np.float32)\n",
    "    return doc_bm25\n",
    "\n",
    "# 사용 예시\n",
    "# 1) 문서 읽기\n",
    "doc_ids, doc_texts = load_docs_txt(\"Amazon_products/train/train_corpus.txt\")\n",
    "\n",
    "# 3) 같은 bm25로 문서도 임베딩\n",
    "doc_embeddings = build_doc_embeddings_from_bm25_vectorizer(doc_texts, bm25)\n",
    "\n",
    "print(doc_embeddings.shape)  # (N_docs, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7bd3ba19-4969-452b-9bbe-e2cdbfce97e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T04:04:50.189751Z",
     "iopub.status.busy": "2025-11-11T04:04:50.189466Z",
     "iopub.status.idle": "2025-11-11T04:04:50.192489Z",
     "shell.execute_reply": "2025-11-11T04:04:50.192082Z",
     "shell.execute_reply.started": "2025-11-11T04:04:50.189734Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29487, 3466)\n"
     ]
    }
   ],
   "source": [
    "print(doc_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2df2fb6-6a10-4b74-a909-c9e60bef1f42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T04:33:21.342918Z",
     "iopub.status.busy": "2025-11-12T04:33:21.342663Z",
     "iopub.status.idle": "2025-11-12T04:33:21.346423Z",
     "shell.execute_reply": "2025-11-12T04:33:21.346014Z",
     "shell.execute_reply.started": "2025-11-12T04:33:21.342900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "[0, 3, 10, 23, 40, 169]\n"
     ]
    }
   ],
   "source": [
    "N = 531 \n",
    "B = np.zeros((N, N), dtype=np.uint8)\n",
    "\n",
    "for u, v in E:\n",
    "    B[u, v] = 1\n",
    "print(B)\n",
    "print(roots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cd05771-6906-4acf-8372-ba0bfd7d4a19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T04:33:25.798297Z",
     "iopub.status.busy": "2025-11-12T04:33:25.797953Z",
     "iopub.status.idle": "2025-11-12T04:33:25.805909Z",
     "shell.execute_reply": "2025-11-12T04:33:25.805430Z",
     "shell.execute_reply.started": "2025-11-12T04:33:25.798269Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy as np\n",
    "\n",
    "def hierarchical_beam_similarity_avg(\n",
    "    doc_vec: np.ndarray,\n",
    "    label_emb: np.ndarray,\n",
    "    adj_upper: np.ndarray,\n",
    "    roots: list[int] = [0],       # 여러 루트\n",
    "    beam: int = 5,\n",
    "    per_parent: str | int = \"l+2\",\n",
    "    tau: float = 0.35,\n",
    "    eps: float = 1e-9,\n",
    "    max_depth: int | None = None,\n",
    "    normalize: bool = False,      # 필요하면 True로\n",
    "):\n",
    "    doc = np.asarray(doc_vec, dtype=np.float32)\n",
    "    L = np.asarray(label_emb, dtype=np.float32)\n",
    "    A = np.asarray(adj_upper).astype(bool)\n",
    "    N, d = L.shape\n",
    "\n",
    "    if normalize:\n",
    "        doc = doc / (np.linalg.norm(doc) + eps)\n",
    "        L = L / (np.linalg.norm(L, axis=1, keepdims=True) + eps)\n",
    "\n",
    "    # 로컬 점수\n",
    "    sims = L @ doc\n",
    "    p = 1.0 / (1.0 + np.exp(-sims / max(tau, 1e-6)))\n",
    "\n",
    "    children = [np.flatnonzero(A[i]) for i in range(N)]\n",
    "\n",
    "    S = np.full(N, -np.inf, dtype=np.float32)\n",
    "    K = np.full(N, -np.inf, dtype=np.float32)\n",
    "    Llen = np.zeros(N, dtype=np.int32)\n",
    "\n",
    "    roots = list(roots)\n",
    "    for r in roots:\n",
    "        S[r] = 0.0\n",
    "        Llen[r] = 0\n",
    "        K[r] = -np.inf\n",
    "\n",
    "    levels = [roots[:]]\n",
    "    cur = roots[:]\n",
    "    level_id = 0\n",
    "\n",
    "    while True:\n",
    "        cand_best = {}\n",
    "        k_parent = (level_id + 2) if (per_parent == \"l+2\") else int(per_parent)\n",
    "\n",
    "        for par in cur:\n",
    "            ch = children[par]\n",
    "            if ch.size == 0:\n",
    "                continue\n",
    "            if ch.size > k_parent:\n",
    "                idx = np.argpartition(-sims[ch], k_parent - 1)[:k_parent]\n",
    "                ch = ch[idx]\n",
    "            for c in ch:\n",
    "                S_c = S[par] + float(p[c])\n",
    "                L_c = Llen[par] + 1\n",
    "                K_c = S_c / (L_c + eps)\n",
    "                if (c not in cand_best) or (K_c > cand_best[c][2]):\n",
    "                    cand_best[c] = (S_c, L_c, K_c)\n",
    "\n",
    "        if not cand_best:\n",
    "            break\n",
    "\n",
    "        kept = sorted(cand_best.items(), key=lambda x: x[1][2], reverse=True)[:min(beam, len(cand_best))]\n",
    "        next_level = [i for i, _ in kept]\n",
    "        for i, (Si, Li, Ki) in kept:\n",
    "            S[i], Llen[i], K[i] = Si, Li, Ki\n",
    "\n",
    "        levels.append(next_level)\n",
    "        cur = next_level\n",
    "        level_id += 1\n",
    "        if max_depth is not None and level_id >= max_depth:\n",
    "            break\n",
    "\n",
    "    return K, levels, sims, p\n",
    "\n",
    "\n",
    "\n",
    "def topk_labels_by_avg(\n",
    "    doc_vec, label_emb, adj_upper, rootㄴ=(0,), beam=5, per_parent=\"l+2\", k=5, **kw\n",
    "):\n",
    "    \"\"\"평균 점수 기반 최종 상위 k 라벨(루트 제외).\"\"\"\n",
    "    K, levels, sims, p = hierarchical_beam_similarity_avg(\n",
    "        doc_vec, label_emb, adj_upper, root=list(roots), beam=beam, per_parent=per_parent, **kw\n",
    "    )\n",
    "    root_set = set(roots)\n",
    "    order = np.argsort(-K)\n",
    "    order = [i for i in order if i not in root_set and np.isfinite(K[i])]\n",
    "    top = order[:k]\n",
    "    return top, K[top]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb5f7b9f-1627-4a14-aca0-769891ef07db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T04:33:33.192359Z",
     "iopub.status.busy": "2025-11-12T04:33:33.192134Z",
     "iopub.status.idle": "2025-11-12T04:33:33.199696Z",
     "shell.execute_reply": "2025-11-12T04:33:33.199288Z",
     "shell.execute_reply.started": "2025-11-12T04:33:33.192342Z"
    }
   },
   "outputs": [],
   "source": [
    "# silver label\n",
    "# documnet bert 마지막층 unfrozen하고\n",
    "# 학습 돌리면서 label 추가하기\n",
    "\n",
    "from __future__ import annotations\n",
    "import argparse\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------- IO Utils -----------------------------\n",
    "\n",
    "def l2_normalize(x: np.ndarray, axis: int = -1, eps: float = 1e-12) -> np.ndarray:\n",
    "    n = np.linalg.norm(x, axis=axis, keepdims=True)\n",
    "    return x / (n + eps)\n",
    "\n",
    "\n",
    "def load_embeddings_csv(path: str | Path, id_col: str = \"id\") -> Tuple[List[int], np.ndarray]:\n",
    "    \"\"\"Load embeddings from CSV where the first column is an id and the rest are feature columns.\n",
    "    Returns (ids, float32 matrix).\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    cols = list(df.columns)\n",
    "    if id_col in df.columns:\n",
    "        id_series = df[id_col]\n",
    "        X = df.drop(columns=[id_col])\n",
    "    else:\n",
    "        # Fallback: use the first column as id\n",
    "        id_series = df.iloc[:, 0]\n",
    "        X = df.iloc[:, 1:]\n",
    "    ids = id_series.astype(int).tolist()\n",
    "    X = X.to_numpy(dtype=np.float32)\n",
    "    return ids, X\n",
    "\n",
    "\n",
    "\n",
    "# ---------------- Hierarchical beam search (average score) ----------------\n",
    "\n",
    "\n",
    "def all_label_similarity(\n",
    "    doc_vec: np.ndarray,\n",
    "    label_emb: np.ndarray,\n",
    "    tau: float = 0.35,\n",
    "    normalize: bool = True,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"문서 임베딩 vs 모든 라벨 임베딩 점수 한 번에 계산.\n",
    "\n",
    "    반환:\n",
    "        sims: (N,) 원시 유사도 (cosine 기반)\n",
    "        p:    (N,) sigmoid 점수\n",
    "    \"\"\"\n",
    "    doc = np.asarray(doc_vec, dtype=np.float32)\n",
    "    L = np.asarray(label_emb, dtype=np.float32)\n",
    "\n",
    "    # 모든 라벨에 대해 한 번에\n",
    "    sims = L @ doc  # (N,)\n",
    "    p = 1.0 / (1.0 + np.exp(-sims / max(tau, 1e-6)))\n",
    "    return sims, p\n",
    "\n",
    "\n",
    "def silver_labeling(\n",
    "    doc_ids: List[int],\n",
    "    docs: np.ndarray,\n",
    "    label_ids: List[int],\n",
    "    label_emb: np.ndarray,\n",
    "    threshold: float = 0.8,\n",
    "    top_k: int = 3,\n",
    "    tau: float = 0.35,\n",
    "    root_id: int = 0,\n",
    "    normalize: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"문서마다 '모든' 라벨 임베딩을 비교해서 점수 높은 라벨을 뽑는 버전.\n",
    "\n",
    "    - 트리(adj) 안 탐.\n",
    "    - root_id는 결과에서 제외.\n",
    "    - p >= threshold 인 애들 중에서 top_k만 고름.\n",
    "    \"\"\"\n",
    "    # 기본 정합성 체크\n",
    "    label_ids = list(label_ids)\n",
    "    N = label_emb.shape[0]\n",
    "    if len(label_ids) != N:\n",
    "        raise ValueError(f\"label_ids 길이({len(label_ids)})와 label_emb 행({N})이 다릅니다.\")\n",
    "\n",
    "    rows = []\n",
    "    for idx, (doc_id, d) in enumerate(zip(doc_ids, docs)):\n",
    "        sims, p = all_label_similarity(d, label_emb, tau=tau, normalize=normalize)\n",
    "\n",
    "        # root는 제외하고, threshold 이상만 후보로\n",
    "        candidates = [\n",
    "            (i, float(p[i]))\n",
    "            for i in range(N)\n",
    "            if i != root_id and np.isfinite(p[i]) and p[i] >= threshold\n",
    "        ]\n",
    "\n",
    "        # 점수 내림차순 정렬\n",
    "        candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        selected = candidates[:top_k]\n",
    "\n",
    "        row = {\"doc_id\": int(doc_id)}\n",
    "        for j in range(top_k):\n",
    "            if j < len(selected):\n",
    "                li, sc = selected[j]\n",
    "                row[f\"label_id_{j+1}\"] = int(label_ids[li])\n",
    "                row[f\"score_{j+1}\"] = float(sc)\n",
    "            else:\n",
    "                row[f\"label_id_{j+1}\"] = np.nan\n",
    "                row[f\"score_{j+1}\"] = np.nan\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "        if (idx + 1) % 100 == 0:\n",
    "            print(f\"Processed {idx+1} / {len(doc_ids)} docs...\")\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e4244ac-1ed8-470c-bc63-1470f460ea33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T04:33:38.366672Z",
     "iopub.status.busy": "2025-11-12T04:33:38.366470Z",
     "iopub.status.idle": "2025-11-12T04:33:38.381058Z",
     "shell.execute_reply": "2025-11-12T04:33:38.380596Z",
     "shell.execute_reply.started": "2025-11-12T04:33:38.366657Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Self-training pipeline with hierarchical silver labeling and dynamic dataloaders.\n",
    "\n",
    "- Reads document/label embeddings CSVs (first column \"id\", rest feat000..feat127)\n",
    "- Reads upper-triangular adjacency (A[i,j]=1 means i->j)\n",
    "- Makes initial silver labels via hierarchical beam search (average score)\n",
    "- Splits into train/val on silver set; keeps the rest as unlabeled pool\n",
    "- Trains a multi-label classifier (Linear/MLP) with BCEWithLogitsLoss\n",
    "- Each epoch, pseudo-labels unlabeled docs whose predicted probs exceed a threshold\n",
    "- Adds them to the training set (up to top_k per doc), with patience-based early stopping\n",
    "\n",
    "Run example\n",
    "-----------\n",
    "python self_training_pipeline.py \\\n",
    "  --doc_csv docs.csv \\\n",
    "  --label_csv labels.csv \\\n",
    "  --adj adj.npy \\\n",
    "  --val_ratio 0.2 --epochs 50 --patience 5 \\\n",
    "  --silver_threshold 0.60 --silver_topk 3 --beam 5 --tau 0.35 --root_id 0 \\\n",
    "  --pseudo_threshold 0.70 --pseudo_topk 3 --batch_size 256 --lr 1e-3\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import argparse\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "def load_embeddings_csv(path: str | Path, id_col: str = \"id\") -> Tuple[List[int], np.ndarray]:\n",
    "    \"\"\"Load embeddings from CSV where the first column is an id and the rest are feature columns.\n",
    "    Returns (ids, float32 matrix).\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    cols = list(df.columns)\n",
    "    if id_col in df.columns:\n",
    "        id_series = df[id_col]\n",
    "        X = df.drop(columns=[id_col])\n",
    "    else:\n",
    "        # Fallback: use the first column as id\n",
    "        id_series = df.iloc[:, 0]\n",
    "        X = df.iloc[:, 1:]\n",
    "    ids = id_series.astype(int).tolist()\n",
    "    X = X.to_numpy(dtype=np.float32)\n",
    "    return ids, X\n",
    "\n",
    "\n",
    "# ----------------------------- Datasets -----------------------------\n",
    "\n",
    "class MultiLabelDataset(Dataset):\n",
    "    def __init__(self, X: np.ndarray, Y: np.ndarray, indices: List[int] | None = None):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.indices = np.array(indices if indices is not None else np.arange(X.shape[0]), dtype=np.int64)\n",
    "    def __len__(self):\n",
    "        return self.indices.shape[0]\n",
    "    def __getitem__(self, idx: int):\n",
    "        i = int(self.indices[idx])\n",
    "        x = torch.from_numpy(self.X[i])\n",
    "        y = torch.from_numpy(self.Y[i])\n",
    "        return x, y\n",
    "\n",
    "class UnlabeledDataset(Dataset):\n",
    "    def __init__(self, X: np.ndarray, indices: List[int]):\n",
    "        self.X = X\n",
    "        self.indices = np.array(indices, dtype=np.int64)\n",
    "    def __len__(self):\n",
    "        return self.indices.shape[0]\n",
    "    def __getitem__(self, idx: int):\n",
    "        i = int(self.indices[idx])\n",
    "        x = torch.from_numpy(self.X[i])\n",
    "        return x, i\n",
    "\n",
    "# ----------------------------- Model -----------------------------\n",
    "\n",
    "class MLPHead(nn.Module):\n",
    "    def __init__(self, in_dim: int, out_dim: int, hidden: int | None = 256, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        if hidden is None or hidden <= 0:\n",
    "            self.net = nn.Sequential(\n",
    "                nn.LayerNorm(in_dim),\n",
    "                nn.Linear(in_dim, out_dim),\n",
    "            )\n",
    "        else:\n",
    "            self.net = nn.Sequential(\n",
    "                nn.LayerNorm(in_dim),\n",
    "                nn.Linear(in_dim, hidden),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(hidden, out_dim),\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# ----------------------------- Utils -----------------------------\n",
    "\n",
    "def to_device(batch, device):\n",
    "    if isinstance(batch, (tuple, list)):\n",
    "        return [b.to(device) if torch.is_tensor(b) else b for b in batch]\n",
    "    return batch.to(device)\n",
    "\n",
    "\n",
    "def micro_f1(y_true: np.ndarray, y_prob: np.ndarray, thr: float = 0.5, eps: float = 1e-9) -> float:\n",
    "    y_pred = (y_prob >= thr).astype(np.float32)\n",
    "    tp = (y_true * y_pred).sum()\n",
    "    fp = ((1 - y_true) * y_pred).sum()\n",
    "    fn = (y_true * (1 - y_pred)).sum()\n",
    "    prec = tp / (tp + fp + eps)\n",
    "    rec = tp / (tp + fn + eps)\n",
    "    f1 = 2 * prec * rec / (prec + rec + eps)\n",
    "    return float(f1)\n",
    "\n",
    "# -------- Initial silver labeling (no CSV save; in-memory) --------\n",
    "def make_initial_silver_hier(\n",
    "    docs: np.ndarray,\n",
    "    labels: np.ndarray,\n",
    "    adj: np.ndarray,\n",
    "    roots: list[int] = [0],\n",
    "    silver_threshold: float = 0.6,    # 이건 avg(K) 기준\n",
    "    silver_topk: int = 3,\n",
    "    beam: int = 5,\n",
    "    per_parent: str | int = \"l+2\",\n",
    "    tau: float = 0.35,\n",
    ") -> list[list[int]]:\n",
    "    \"\"\"\n",
    "    계층 빔 서치로 각 문서의 라벨 후보를 뽑는다.\n",
    "    - 계층 밖 라벨은 애초에 안 들어옴\n",
    "    - 루트들은 결과에서 제외\n",
    "    - K(경로 평균) >= silver_threshold 인 애들 중 top-k\n",
    "    \"\"\"\n",
    "    N = labels.shape[0]\n",
    "    silver: list[list[int]] = []\n",
    "    root_set = set(roots)\n",
    "\n",
    "    for d in docs:\n",
    "        K, levels, sims, p = hierarchical_beam_similarity_avg(\n",
    "            d, labels, adj,\n",
    "            roots=roots,\n",
    "            beam=beam,\n",
    "            per_parent=per_parent,\n",
    "            tau=tau,\n",
    "            normalize=False,   # 너 임베딩이 이미 L2라면 False\n",
    "        )\n",
    "        # 평균 점수로 정렬\n",
    "        order = np.argsort(-K)\n",
    "        # 루트는 제외, 유한한 것만\n",
    "        order = [i for i in order if (i not in root_set) and np.isfinite(K[i])]\n",
    "        # threshold 통과한 것만\n",
    "        cand = [i for i in order if K[i] >= silver_threshold]\n",
    "        selected = cand[:silver_topk]\n",
    "        silver.append(selected)\n",
    "\n",
    "    return silver\n",
    "\n",
    "def make_initial_silver(\n",
    "    docs: np.ndarray,\n",
    "    labels: np.ndarray,\n",
    "    adj: np.ndarray,              # 이제 안 씀 (호환용으로만 둠)\n",
    "    silver_threshold: float = 0.9,\n",
    "    silver_topk: int = 3,\n",
    "    beam: int = 5,                # 이제 안 씀\n",
    "    tau: float = 0.35,\n",
    "    root_id: int = 0,\n",
    ") -> List[List[int]]:\n",
    "    \"\"\"\n",
    "    문서마다 전 라벨 임베딩과의 유사도를 보고 초기 silver label을 만든다.\n",
    "    - 트리/경로 탐색 안 함\n",
    "    - root_id는 결과에서 제외\n",
    "    - p >= silver_threshold인 라벨 중에서 상위 silver_topk만 남김\n",
    "    \"\"\"\n",
    "    N = labels.shape[0]\n",
    "    silver: List[List[int]] = []\n",
    "\n",
    "    for d in docs:\n",
    "        # 문서 vs 모든 라벨 점수\n",
    "        sims, p = all_label_similarity(d, labels, tau=tau, normalize=True)\n",
    "\n",
    "        # threshold 통과 + root 제외\n",
    "        cand = [\n",
    "            (i, float(p[i]))\n",
    "            for i in range(N)\n",
    "            if i != root_id and np.isfinite(p[i]) and p[i] >= silver_threshold\n",
    "        ]\n",
    "\n",
    "        # 점수 높은 순\n",
    "        cand.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # label index만 추출\n",
    "        selected = [i for i, _ in cand[:silver_topk]]\n",
    "        silver.append(selected)\n",
    "\n",
    "    return silver\n",
    "# ------------------------ Train / Self-Training ------------------------\n",
    "\n",
    "def train_epoch(model, loader, optim, device, criterion):\n",
    "    model.train()\n",
    "    total = 0.0\n",
    "    for x, y in loader:\n",
    "        x, y = to_device(x, device), to_device(y, device)\n",
    "        optim.zero_grad(set_to_none=True)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        total += float(loss.detach().cpu().item()) * x.size(0)\n",
    "    return total / max(1, len(loader.dataset))\n",
    "\n",
    "\n",
    "def eval_epoch(model, loader, device, criterion, thr=0.5):\n",
    "    model.eval()\n",
    "    total = 0.0\n",
    "    ys = []\n",
    "    ps = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = to_device(x, device), to_device(y, device)\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            total += float(loss.detach().cpu().item()) * x.size(0)\n",
    "            prob = torch.sigmoid(logits).detach().cpu().numpy()\n",
    "            ys.append(y.detach().cpu().numpy())\n",
    "            ps.append(prob)\n",
    "    y_true = np.concatenate(ys, axis=0)\n",
    "    y_prob = np.concatenate(ps, axis=0)\n",
    "    f1 = micro_f1(y_true, y_prob, thr=thr)\n",
    "    return total / max(1, len(loader.dataset)), f1, y_prob\n",
    "\n",
    "\n",
    "def pseudo_label_and_grow(model, unl_ds: UnlabeledDataset,\n",
    "                          num_labels: int,\n",
    "                          pseudo_threshold: float = 0.9, pseudo_topk: int = 3,\n",
    "                          device: str = \"cpu\", batch_size: int = 512):\n",
    "    \"\"\"Infer on unlabeled, select labels with prob>=threshold (top-k), and return new_indices and Y matrix.\"\"\"\n",
    "    if len(unl_ds) == 0:\n",
    "        return [], np.zeros((0, num_labels), dtype=np.float32)\n",
    "    loader = DataLoader(unl_ds, batch_size=batch_size, shuffle=False)\n",
    "    model.eval()\n",
    "    all_idx: List[int] = []\n",
    "    all_y: List[np.ndarray] = []\n",
    "    with torch.no_grad():\n",
    "        for xb, idxs in loader:\n",
    "            xb = xb.to(device)\n",
    "            logits = model(xb)\n",
    "            prob = torch.sigmoid(logits).detach().cpu().numpy()\n",
    "            for p, i in zip(prob, idxs.numpy().tolist()):\n",
    "                sel = np.flatnonzero(p >= pseudo_threshold)\n",
    "                if sel.size > 0:\n",
    "                    # keep at most top-k by prob\n",
    "                    if sel.size > pseudo_topk:\n",
    "                        top = np.argpartition(-p[sel], pseudo_topk - 1)[:pseudo_topk]\n",
    "                        sel = sel[top]\n",
    "                    y = np.zeros(num_labels, dtype=np.float32)\n",
    "                    y[sel] = 1.0\n",
    "                    all_idx.append(int(i))\n",
    "                    all_y.append(y)\n",
    "    if len(all_idx) == 0:\n",
    "        return [], np.zeros((0, num_labels), dtype=np.float32)\n",
    "    Y_new = np.stack(all_y, axis=0)\n",
    "    return all_idx, Y_new\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a6c93d9-087d-4037-a7f2-ed637b3a2344",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T04:33:42.735711Z",
     "iopub.status.busy": "2025-11-12T04:33:42.735435Z",
     "iopub.status.idle": "2025-11-12T04:34:29.853642Z",
     "shell.execute_reply": "2025-11-12T04:34:29.853155Z",
     "shell.execute_reply.started": "2025-11-12T04:33:42.735696Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23529\n",
      "5882\n",
      "76\n"
     ]
    }
   ],
   "source": [
    "\n",
    "doc_ids = np.arange(len(doc_embeddings), dtype=np.int64)   # 0..num_docs-1\n",
    "X = doc_embeddings.astype(np.float32)                      # [num_docs, d_doc]\n",
    "\n",
    "# 라벨 임베딩 세팅\n",
    "label_ids = np.arange(len(label_keys), dtype=np.int64)     # 0..530\n",
    "L = np.vstack([label_embeddings[k] for k in label_keys]).astype(np.float32)   # [531, d_label]\n",
    "\n",
    "B\n",
    "\n",
    "# Ensure label order matches adjacency\n",
    "order = np.argsort(label_ids)\n",
    "label_ids_sorted = [label_ids[i] for i in order]\n",
    "L = L[order]\n",
    "assert B.shape == (L.shape[0], L.shape[0]), \"Adjacency/label size mismatch\"\n",
    "\n",
    "# Initial silver labeling\n",
    "# silver = make_initial_silver(X, L, B, silver_threshold=0.9, silver_topk=3, tau=0.35, root_id=0)\n",
    "silver = make_initial_silver_hier(\n",
    "    X,          # docs\n",
    "    L,          # label_emb\n",
    "    B,          # upper adj\n",
    "    roots=roots,  # 여러 개면 [0, 10, 25] 이런 식\n",
    "    silver_threshold=0.6,\n",
    "    silver_topk=3,\n",
    "    beam=5,\n",
    "    per_parent=\"l+2\",\n",
    "    tau=0.35,\n",
    ")\n",
    "\n",
    "val_ratio  = 0.2\n",
    "epochs = 50\n",
    "patience = 5\n",
    "batch_size = 256\n",
    "lr = 1e-3\n",
    "hidden = 256\n",
    "dropout = 0.1\n",
    "pseudo_threshold = 0.6\n",
    "pseudo_topk = 3\n",
    "seed = 42\n",
    "# Build multi-hot targets for silver docs; split into train/val; keep unlabeled indices\n",
    "N_labels = L.shape[0]\n",
    "Y = np.zeros((X.shape[0], N_labels), dtype=np.float32)\n",
    "silver_mask = np.zeros(X.shape[0], dtype=bool)\n",
    "for i, lab_list in enumerate(silver):\n",
    "    if len(lab_list) > 0:\n",
    "        Y[i, lab_list] = 1.0\n",
    "        silver_mask[i] = True\n",
    "\n",
    "idx_silver = np.flatnonzero(silver_mask)\n",
    "idx_unl = np.flatnonzero(~silver_mask)\n",
    "\n",
    "# Train/val split\n",
    "rng = np.random.default_rng(seed)\n",
    "rng.shuffle(idx_silver)\n",
    "n_val = max(1, int(len(idx_silver) * val_ratio)) if len(idx_silver) > 0 else 0\n",
    "idx_val = idx_silver[:n_val]\n",
    "idx_train = idx_silver[n_val:]\n",
    "\n",
    "train_ds = MultiLabelDataset(X, Y, indices=idx_train)\n",
    "val_ds = MultiLabelDataset(X, Y, indices=idx_val) if n_val > 0 else None\n",
    "unl_ds = UnlabeledDataset(X, idx_unl.tolist())\n",
    "\n",
    "print(len(train_ds))\n",
    "print(len(val_ds))\n",
    "print(len(unl_ds))\n",
    "# Model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = MLPHead(in_dim=X.shape[1], out_dim=N_labels, hidden=hidden, dropout=dropout).to(device)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "crit = nn.BCEWithLogitsLoss()\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False) if val_ds is not None else None\n",
    "\n",
    "best_val = float(\"inf\")\n",
    "best_epoch = -1\n",
    "no_improve = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0795c01b-891d-48d1-a90f-e252f47da5f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T04:34:29.854403Z",
     "iopub.status.busy": "2025-11-12T04:34:29.854259Z",
     "iopub.status.idle": "2025-11-12T04:34:29.856940Z",
     "shell.execute_reply": "2025-11-12T04:34:29.856553Z",
     "shell.execute_reply.started": "2025-11-12T04:34:29.854389Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23529\n",
      "5882\n",
      "76\n"
     ]
    }
   ],
   "source": [
    "print(len(train_ds))\n",
    "print(len(val_ds))\n",
    "\n",
    "print(len(unl_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a44d818e-888e-487c-a9fe-c65d524e0b20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T00:44:04.756635Z",
     "iopub.status.busy": "2025-11-11T00:44:04.756408Z",
     "iopub.status.idle": "2025-11-11T00:44:14.986667Z",
     "shell.execute_reply": "2025-11-11T00:44:14.985429Z",
     "shell.execute_reply.started": "2025-11-11T00:44:04.756619Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train_loss=0.183  val_loss=0.021  val_f1=0.000\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 002 | train_loss=0.020  val_loss=0.018  val_f1=0.001\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 003 | train_loss=0.017  val_loss=0.015  val_f1=0.203\n",
      "  + Added 827 pseudo-labeled docs (unl pool → 21598 left)\n",
      "Epoch 004 | train_loss=0.014  val_loss=0.014  val_f1=0.257\n",
      "  + Added 1096 pseudo-labeled docs (unl pool → 20502 left)\n",
      "Epoch 005 | train_loss=0.012  val_loss=0.013  val_f1=0.277\n",
      "  + Added 2385 pseudo-labeled docs (unl pool → 18117 left)\n",
      "Epoch 006 | train_loss=0.009  val_loss=0.013  val_f1=0.329\n",
      "  + Added 1878 pseudo-labeled docs (unl pool → 16239 left)\n",
      "Epoch 007 | train_loss=0.008  val_loss=0.013  val_f1=0.342\n",
      "  + Added 2058 pseudo-labeled docs (unl pool → 14181 left)\n",
      "Epoch 008 | train_loss=0.006  val_loss=0.013  val_f1=0.345\n",
      "  + Added 2849 pseudo-labeled docs (unl pool → 11332 left)\n",
      "Epoch 009 | train_loss=0.005  val_loss=0.012  val_f1=0.369\n",
      "  + Added 2377 pseudo-labeled docs (unl pool → 8955 left)\n",
      "Epoch 010 | train_loss=0.004  val_loss=0.012  val_f1=0.432\n",
      "  + Added 2174 pseudo-labeled docs (unl pool → 6781 left)\n",
      "Epoch 011 | train_loss=0.004  val_loss=0.012  val_f1=0.438\n",
      "  + Added 1681 pseudo-labeled docs (unl pool → 5100 left)\n",
      "Epoch 012 | train_loss=0.004  val_loss=0.011  val_f1=0.447\n",
      "  + Added 1077 pseudo-labeled docs (unl pool → 4023 left)\n",
      "Epoch 013 | train_loss=0.003  val_loss=0.011  val_f1=0.461\n",
      "  + Added 624 pseudo-labeled docs (unl pool → 3399 left)\n",
      "Epoch 014 | train_loss=0.003  val_loss=0.011  val_f1=0.477\n",
      "  + Added 893 pseudo-labeled docs (unl pool → 2506 left)\n",
      "Epoch 015 | train_loss=0.003  val_loss=0.011  val_f1=0.486\n",
      "  + Added 119 pseudo-labeled docs (unl pool → 2387 left)\n",
      "Epoch 016 | train_loss=0.003  val_loss=0.011  val_f1=0.500\n",
      "  + Added 329 pseudo-labeled docs (unl pool → 2058 left)\n",
      "Epoch 017 | train_loss=0.003  val_loss=0.010  val_f1=0.515\n",
      "  + Added 131 pseudo-labeled docs (unl pool → 1927 left)\n",
      "Epoch 018 | train_loss=0.003  val_loss=0.010  val_f1=0.528\n",
      "  + Added 433 pseudo-labeled docs (unl pool → 1494 left)\n",
      "Epoch 019 | train_loss=0.003  val_loss=0.010  val_f1=0.525\n",
      "  + Added 172 pseudo-labeled docs (unl pool → 1322 left)\n",
      "Epoch 020 | train_loss=0.003  val_loss=0.010  val_f1=0.560\n",
      "  + Added 87 pseudo-labeled docs (unl pool → 1235 left)\n",
      "Epoch 021 | train_loss=0.002  val_loss=0.010  val_f1=0.544\n",
      "  + Added 49 pseudo-labeled docs (unl pool → 1186 left)\n",
      "Epoch 022 | train_loss=0.002  val_loss=0.010  val_f1=0.504\n",
      "  + Added 108 pseudo-labeled docs (unl pool → 1078 left)\n",
      "Epoch 023 | train_loss=0.002  val_loss=0.010  val_f1=0.555\n",
      "  + Added 98 pseudo-labeled docs (unl pool → 980 left)\n",
      "Epoch 024 | train_loss=0.002  val_loss=0.010  val_f1=0.544\n",
      "  + Added 125 pseudo-labeled docs (unl pool → 855 left)\n",
      "Epoch 025 | train_loss=0.002  val_loss=0.010  val_f1=0.559\n",
      "  + Added 75 pseudo-labeled docs (unl pool → 780 left)\n",
      "Epoch 026 | train_loss=0.002  val_loss=0.010  val_f1=0.558\n",
      "  + Added 32 pseudo-labeled docs (unl pool → 748 left)\n",
      "Epoch 027 | train_loss=0.002  val_loss=0.010  val_f1=0.566\n",
      "  + Added 183 pseudo-labeled docs (unl pool → 565 left)\n",
      "Epoch 028 | train_loss=0.002  val_loss=0.010  val_f1=0.573\n",
      "Early stopping at epoch 28 (best@23)\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    tr_loss = train_epoch(model, train_loader, opt, device, crit)\n",
    "    if val_loader is not None and len(val_ds) > 0:\n",
    "        va_loss, va_f1, _ = eval_epoch(model, val_loader, device, crit, thr=0.5)\n",
    "        print(f\"Epoch {epoch:03d} | train_loss={tr_loss:.3f}  val_loss={va_loss:.3f}  val_f1={va_f1:.3f}\")\n",
    "        # Early stopping on val_loss\n",
    "        if va_loss + 1e-6 < best_val:\n",
    "            best_val = va_loss\n",
    "            best_epoch = epoch\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch} (best@{best_epoch})\")\n",
    "                break\n",
    "    else:\n",
    "        print(f\"Epoch {epoch:03d} | train_loss={tr_loss:.3f}\")\n",
    "\n",
    "    # Pseudo-labeling step (after each epoch)\n",
    "    new_idx, Y_new = pseudo_label_and_grow(\n",
    "        model, unl_ds, N_labels,\n",
    "        pseudo_threshold=pseudo_threshold,\n",
    "        pseudo_topk=pseudo_topk,\n",
    "        device=device,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    if len(new_idx) > 0:\n",
    "        # Extend training set\n",
    "        # Update Y with new labels\n",
    "        for i, y in zip(new_idx, Y_new):\n",
    "            Y[i] = np.maximum(Y[i], y)  # merge if any\n",
    "        # Move indices from unlabeled to train\n",
    "        keep_mask = ~np.isin(unl_ds.indices, np.array(new_idx, dtype=np.int64))\n",
    "        unl_ds.indices = unl_ds.indices[keep_mask]\n",
    "        train_ds.indices = np.concatenate([train_ds.indices, np.array(new_idx, dtype=np.int64)])\n",
    "        # Rebuild train loader to reflect length change\n",
    "        train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "        print(f\"  + Added {len(new_idx)} pseudo-labeled docs (unl pool → {len(unl_ds)} left)\")\n",
    "    else:\n",
    "        print(\"  + No pseudo-labeled docs added this epoch\")\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cccc039-f93f-49de-8843-19c7b5e4338d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d15867a-f278-41ab-88a0-e751c62e5873",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0b4cfc-c593-4841-b847-2759ad663984",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4c63ecfd-663d-4ed8-88b2-e8ecbc14de5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T04:49:46.784666Z",
     "iopub.status.busy": "2025-11-12T04:49:46.784407Z",
     "iopub.status.idle": "2025-11-12T04:50:46.903754Z",
     "shell.execute_reply": "2025-11-12T04:50:46.903311Z",
     "shell.execute_reply.started": "2025-11-12T04:49:46.784646Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 29487\n",
      "with silver: 29152\n",
      "unlabeled : 335\n",
      "23322 5830 335\n",
      "Epoch 001 | train_loss=0.060  val_loss=0.029  val_f1=0.415\n",
      "  + (skip pseudo-labeling on warmup epoch)\n",
      "Epoch 002 | train_loss=0.024  val_loss=0.025  val_f1=0.439\n",
      "  + Added 117 pseudo-labeled docs (unl pool → 218 left)\n",
      "Epoch 003 | train_loss=0.020  val_loss=0.024  val_f1=0.445\n",
      "  + Added 32 pseudo-labeled docs (unl pool → 186 left)\n",
      "Epoch 004 | train_loss=0.018  val_loss=0.024  val_f1=0.439\n",
      "  + Added 31 pseudo-labeled docs (unl pool → 155 left)\n",
      "Epoch 005 | train_loss=0.016  val_loss=0.024  val_f1=0.429\n",
      "  + Added 19 pseudo-labeled docs (unl pool → 136 left)\n",
      "Epoch 006 | train_loss=0.015  val_loss=0.025  val_f1=0.430\n",
      "  + Added 12 pseudo-labeled docs (unl pool → 124 left)\n",
      "Epoch 007 | train_loss=0.014  val_loss=0.025  val_f1=0.430\n",
      "  + Added 19 pseudo-labeled docs (unl pool → 105 left)\n",
      "Epoch 008 | train_loss=0.013  val_loss=0.026  val_f1=0.429\n",
      "  + Added 17 pseudo-labeled docs (unl pool → 88 left)\n",
      "Epoch 009 | train_loss=0.012  val_loss=0.027  val_f1=0.419\n",
      "  + Added 11 pseudo-labeled docs (unl pool → 77 left)\n",
      "Epoch 010 | train_loss=0.011  val_loss=0.028  val_f1=0.418\n",
      "  + Added 7 pseudo-labeled docs (unl pool → 70 left)\n",
      "Epoch 011 | train_loss=0.010  val_loss=0.029  val_f1=0.409\n",
      "  + Added 10 pseudo-labeled docs (unl pool → 60 left)\n",
      "Epoch 012 | train_loss=0.010  val_loss=0.030  val_f1=0.413\n",
      "  + Added 5 pseudo-labeled docs (unl pool → 55 left)\n",
      "Epoch 013 | train_loss=0.009  val_loss=0.031  val_f1=0.414\n",
      "  + Added 10 pseudo-labeled docs (unl pool → 45 left)\n",
      "Epoch 014 | train_loss=0.008  val_loss=0.033  val_f1=0.414\n",
      "  + Added 7 pseudo-labeled docs (unl pool → 38 left)\n",
      "Epoch 015 | train_loss=0.007  val_loss=0.035  val_f1=0.412\n",
      "  + Added 5 pseudo-labeled docs (unl pool → 33 left)\n",
      "Epoch 016 | train_loss=0.007  val_loss=0.036  val_f1=0.414\n",
      "  + Added 8 pseudo-labeled docs (unl pool → 25 left)\n",
      "Epoch 017 | train_loss=0.006  val_loss=0.038  val_f1=0.409\n",
      "  + Added 1 pseudo-labeled docs (unl pool → 24 left)\n",
      "Epoch 018 | train_loss=0.005  val_loss=0.040  val_f1=0.411\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 019 | train_loss=0.005  val_loss=0.042  val_f1=0.408\n",
      "  + Added 4 pseudo-labeled docs (unl pool → 20 left)\n",
      "Epoch 020 | train_loss=0.004  val_loss=0.044  val_f1=0.410\n",
      "  + Added 2 pseudo-labeled docs (unl pool → 18 left)\n",
      "Epoch 021 | train_loss=0.004  val_loss=0.047  val_f1=0.405\n",
      "  + Added 1 pseudo-labeled docs (unl pool → 17 left)\n",
      "Epoch 022 | train_loss=0.003  val_loss=0.049  val_f1=0.411\n",
      "  + No pseudo-labeled docs added this epoch\n",
      "Epoch 023 | train_loss=0.003  val_loss=0.051  val_f1=0.406\n",
      "Early stopping at epoch 23 (best f1=0.4446)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "doc_ids = np.arange(len(doc_embeddings), dtype=np.int64)   # 0..num_docs-1\n",
    "X = doc_embeddings.astype(np.float32)                      # [num_docs, d_doc]\n",
    "\n",
    "# 라벨 임베딩 세팅\n",
    "label_ids = np.arange(len(label_keys), dtype=np.int64)     # 0..530\n",
    "L = np.vstack([label_embeddings[k] for k in label_keys]).astype(np.float32)   # [531, d_label]\n",
    "\n",
    "\n",
    "# 1) 라벨 순서와 B(부모->자식) 맞추기\n",
    "order = np.argsort(label_ids)\n",
    "label_ids = [label_ids[i] for i in order]\n",
    "L = L[order]\n",
    "assert B.shape == (L.shape[0], L.shape[0]), \"Adjacency/label size mismatch\"\n",
    "\n",
    "# 2) 계층 silver 만들기\n",
    "silver = make_initial_silver_hier(\n",
    "    X,          # docs (N, d)\n",
    "    L,          # label_emb (C, d)\n",
    "    B,          # upper adj (C, C)\n",
    "    roots=roots,\n",
    "    silver_threshold=0.8,\n",
    "    silver_topk=3,\n",
    "    beam=5,\n",
    "    per_parent=\"l+2\",\n",
    "    tau=0.35,\n",
    ")\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 1) 계층 정보에서 parents / children 뽑기\n",
    "#    B[parent, child] = 1 이라고 했으니까 그대로 씀\n",
    "# -------------------------------------------------\n",
    "# B: [C, C] (parent -> child)\n",
    "def build_parents_children(adj):\n",
    "    C = adj.shape[0]\n",
    "    parents = [np.flatnonzero(adj[:, j]).astype(np.int64) for j in range(C)]\n",
    "    children = [np.flatnonzero(adj[j]).astype(np.int64) for j in range(C)]\n",
    "    return parents, children\n",
    "\n",
    "parents, children = build_parents_children(B)\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2) silver → 계층 pos/neg 마스크로 변환\n",
    "# -------------------------------------------------\n",
    "def build_pos_neg_masks(silver, parents, children, num_labels):\n",
    "    \"\"\"\n",
    "    silver: list[list[int]]  # 문서마다 core label index들\n",
    "    parents / children: list[np.ndarray]\n",
    "    return:\n",
    "      pos_masks: np.array [N_docs, C]\n",
    "      neg_masks: np.array [N_docs, C]\n",
    "    \"\"\"\n",
    "    N = len(silver)\n",
    "    C = num_labels\n",
    "    pos_masks = np.zeros((N, C), dtype=np.float32)\n",
    "    neg_masks = np.zeros((N, C), dtype=np.float32)\n",
    "\n",
    "    all_idx = np.arange(C)\n",
    "\n",
    "    for i, core in enumerate(silver):\n",
    "        core = list(core)\n",
    "        # 1) core의 부모까지 positive\n",
    "        pos_set = set(core)\n",
    "        for c in core:\n",
    "            for p in parents[c]:\n",
    "                pos_set.add(int(p))\n",
    "\n",
    "        # 2) children은 나중에 negative에서 제외\n",
    "        child_set = set()\n",
    "        for c in core:\n",
    "            for ch in children[c]:\n",
    "                child_set.add(int(ch))\n",
    "\n",
    "        # pos 마스크\n",
    "        for p in pos_set:\n",
    "            pos_masks[i, p] = 1.0\n",
    "\n",
    "        # neg = 전체 - pos - children\n",
    "        for j in all_idx:\n",
    "            if j in pos_set:\n",
    "                continue\n",
    "            if j in child_set:\n",
    "                continue\n",
    "            neg_masks[i, j] = 1.0\n",
    "\n",
    "    return pos_masks, neg_masks\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3) Dataset: 문서 임베딩 + pos/neg 마스크\n",
    "# -------------------------------------------------\n",
    "class HierMultiLabelDataset(Dataset):\n",
    "    def __init__(self, X, pos_masks, neg_masks, indices=None):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.pos = pos_masks.astype(np.float32)\n",
    "        self.neg = neg_masks.astype(np.float32)\n",
    "        if indices is None:\n",
    "            self.indices = np.arange(self.X.shape[0], dtype=np.int64)\n",
    "        else:\n",
    "            self.indices = np.array(indices, dtype=np.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.indices.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i = int(self.indices[idx])\n",
    "        x = torch.from_numpy(self.X[i])\n",
    "        pos = torch.from_numpy(self.pos[i])\n",
    "        neg = torch.from_numpy(self.neg[i])\n",
    "        return x, pos, neg\n",
    "\n",
    "class UnlabeledDataset(Dataset):\n",
    "    def __init__(self, X, indices):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.indices = np.array(indices, dtype=np.int64)\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    def __getitem__(self, idx):\n",
    "        i = int(self.indices[idx])\n",
    "        return torch.from_numpy(self.X[i]), i\n",
    "\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 4) Bilinear classifier\n",
    "#    doc_emb: [B, d_doc]\n",
    "#    label_emb: [C, d_lab]  (미리 GAT로 만든 거)\n",
    "#    점수: doc @ W @ label_emb^T\n",
    "# -------------------------------------------------\n",
    "class BilinearHierClassifier(nn.Module):\n",
    "    def __init__(self, doc_dim, label_emb, hidden_dim=None):\n",
    "        super().__init__()\n",
    "        # label_emb는 파라미터로 들고있되, 업데이트 안 한다고 가정(원하면 nn.Parameter로)\n",
    "        self.register_buffer(\"label_emb\", torch.tensor(label_emb, dtype=torch.float32))\n",
    "        C, d_lab = self.label_emb.shape\n",
    "        self.doc_dim = doc_dim\n",
    "        self.label_dim = d_lab\n",
    "\n",
    "        if hidden_dim is None:\n",
    "            # 바로 doc_dim -> label_dim\n",
    "            self.interaction = nn.Linear(doc_dim, d_lab, bias=False)\n",
    "            self.proj = None\n",
    "        else:\n",
    "            # doc_dim -> hidden -> label_dim 같은 것도 가능\n",
    "            self.interaction = nn.Sequential(\n",
    "                nn.Linear(doc_dim, hidden_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(hidden_dim, d_lab, bias=False),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [B, d_doc]\n",
    "        return: logits [B, C]\n",
    "        \"\"\"\n",
    "        # x -> same dim as label\n",
    "        h = self.interaction(x)                             # [B, d_lab]\n",
    "        # [B, d_lab] @ [d_lab, C] -> [B, C]\n",
    "        logits = torch.matmul(h, self.label_emb.t())\n",
    "        return logits\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 5) loss: 계층 마스크를 씌운 BCE\n",
    "# -------------------------------------------------\n",
    "def hierarchical_bce_loss(logits, pos_mask, neg_mask):\n",
    "    # logits: [B, C]\n",
    "    # pos_mask, neg_mask: [B, C]\n",
    "    loss_pos = -(pos_mask * F.logsigmoid(logits)).sum()\n",
    "    loss_neg = -(neg_mask * F.logsigmoid(-logits)).sum()\n",
    "    denom = (pos_mask.sum() + neg_mask.sum()).clamp(min=1.0)\n",
    "    return (loss_pos + loss_neg) / denom\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 6) 학습 루프 예시\n",
    "# -------------------------------------------------\n",
    "# 이미 있는 것들: X (문서 BERT 임베딩) : [N_docs, d_doc]\n",
    "#                  L (라벨 GAT 임베딩)  : [C, d_lab]\n",
    "#                  B_adj (부모->자식)   : [C, C]\n",
    "#                  silver (list[list[int]]) : 문서별 core label index\n",
    "def train_epoch_hier(model, loader, opt, device):\n",
    "    model.train()\n",
    "    total = 0.0\n",
    "    for xb, posb, negb in loader:\n",
    "        xb = xb.to(device)\n",
    "        posb = posb.to(device)\n",
    "        negb = negb.to(device)\n",
    "        logits = model(xb)\n",
    "        loss = hierarchical_bce_loss(logits, posb, negb)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        total += loss.item() * xb.size(0)\n",
    "    return total / len(loader.dataset)\n",
    "\n",
    "# 1) micro F1 계산\n",
    "def micro_f1_from_logits(logits, pos_mask, thr=0.5, eps=1e-9):\n",
    "    \"\"\"\n",
    "    logits: [B, C]\n",
    "    pos_mask: [B, C]  (1: positive, 0: else)\n",
    "    \"\"\"\n",
    "    probs = torch.sigmoid(logits)\n",
    "    preds = (probs >= thr).float()\n",
    "\n",
    "    y_true = pos_mask\n",
    "    y_pred = preds\n",
    "\n",
    "    tp = (y_true * y_pred).sum()\n",
    "    fp = ((1 - y_true) * y_pred).sum()\n",
    "    fn = (y_true * (1 - y_pred)).sum()\n",
    "\n",
    "    precision = tp / (tp + fp + eps)\n",
    "    recall    = tp / (tp + fn + eps)\n",
    "    f1 = 2 * precision * recall / (precision + recall + eps)\n",
    "    return f1.item()\n",
    "\n",
    "# 2) eval 함수 수정: loss + f1 둘 다\n",
    "def eval_epoch_hier(model, loader, device, k=3, thr=None):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    f1_list = []\n",
    "    with torch.no_grad():\n",
    "        for xb, posb, negb in loader:\n",
    "            xb = xb.to(device)\n",
    "            posb = posb.to(device)\n",
    "            negb = negb.to(device)\n",
    "\n",
    "            logits = model(xb)\n",
    "            loss = hierarchical_bce_loss(logits, posb, negb)  # 위에 바꾼 버전\n",
    "            total_loss += loss.item() * xb.size(0)\n",
    "\n",
    "            probs = torch.sigmoid(logits)\n",
    "\n",
    "            if thr is not None:\n",
    "                pred = (probs >= thr).float()\n",
    "            else:\n",
    "                # top-k 방식\n",
    "                B, C = probs.shape\n",
    "                pred = torch.zeros_like(probs)\n",
    "                topk = probs.topk(k, dim=1).indices\n",
    "                pred.scatter_(1, topk, 1.0)\n",
    "\n",
    "            # micro-f1\n",
    "            y_true = posb\n",
    "            y_pred = pred\n",
    "            tp = (y_true * y_pred).sum().item()\n",
    "            fp = ((1 - y_true) * y_pred).sum().item()\n",
    "            fn = (y_true * (1 - y_pred)).sum().item()\n",
    "            prec = tp / (tp + fp + 1e-9)\n",
    "            rec  = tp / (tp + fn + 1e-9)\n",
    "            f1   = 2 * prec * rec / (prec + rec + 1e-9)\n",
    "            f1_list.append(f1)\n",
    "\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    avg_f1 = float(np.mean(f1_list)) if f1_list else 0.0\n",
    "    return avg_loss, avg_f1\n",
    "def pseudo_label_and_grow_hier(\n",
    "    model,\n",
    "    unl_ds,             # UnlabeledDataset\n",
    "    X_all,              # 전체 문서 임베딩 (numpy)\n",
    "    parents, children,\n",
    "    num_labels,\n",
    "    device,\n",
    "    pseudo_threshold=0.45,\n",
    "    pseudo_topk=3,\n",
    "    batch_size=512,\n",
    "):\n",
    "    if len(unl_ds) == 0:\n",
    "        return [], None, None\n",
    "\n",
    "    loader = DataLoader(unl_ds, batch_size=batch_size, shuffle=False)\n",
    "    model.eval()\n",
    "    new_idx = []\n",
    "    new_pos_list = []\n",
    "    new_neg_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, idxs in loader:\n",
    "            xb = xb.to(device)\n",
    "            logits = model(xb)\n",
    "            prob = torch.sigmoid(logits).cpu().numpy()\n",
    "\n",
    "            for p, i_doc in zip(prob, idxs.numpy().tolist()):\n",
    "                order = np.argsort(-p)\n",
    "                top1 = p[order[0]]\n",
    "                # 1) top-1이 threshold를 못 넘으면 그냥 버린다\n",
    "                if top1 < pseudo_threshold:\n",
    "                    continue\n",
    "                core = [j for j in order if p[j] >= pseudo_threshold][:pseudo_topk]\n",
    "                if len(core) == 0:\n",
    "                    # 아예 이 문서는 이번 epoch에 안 넣음\n",
    "                    continue\n",
    "\n",
    "                # 계층 pos/neg 구성\n",
    "                pos = set(core)\n",
    "                for c in core:\n",
    "                    for pa in parents[c]:\n",
    "                        pos.add(int(pa))\n",
    "                child = set()\n",
    "                for c in core:\n",
    "                    for ch in children[c]:\n",
    "                        child.add(int(ch))\n",
    "\n",
    "                pos_mask = np.zeros(num_labels, dtype=np.float32)\n",
    "                neg_mask = np.zeros(num_labels, dtype=np.float32)\n",
    "                for j in pos:\n",
    "                    pos_mask[j] = 1.0\n",
    "                for j in range(num_labels):\n",
    "                    if j in pos:    # 이미 양성\n",
    "                        continue\n",
    "                    if j in child:  # 모르겠음 → negative에서 제외\n",
    "                        continue\n",
    "                    neg_mask[j] = 1.0\n",
    "\n",
    "                new_idx.append(int(i_doc))\n",
    "                new_pos_list.append(pos_mask)\n",
    "                new_neg_list.append(neg_mask)\n",
    "\n",
    "\n",
    "\n",
    "    if len(new_idx) == 0:\n",
    "        return [], None, None\n",
    "\n",
    "    new_pos = np.stack(new_pos_list, axis=0)\n",
    "    new_neg = np.stack(new_neg_list, axis=0)\n",
    "    return new_idx, new_pos, new_neg\n",
    "\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "has_silver = np.array([len(lbls) > 0 for lbls in silver], dtype=bool)\n",
    "N_docs = X.shape[0]\n",
    "C = L.shape[0]\n",
    "\n",
    "# silver 있는 문서 / 없는 문서\n",
    "has_silver = np.array([len(lbls) > 0 for lbls in silver], dtype=bool)\n",
    "idx_silver = np.flatnonzero(has_silver)      # 여기가 train/val 후보\n",
    "idx_unl    = np.flatnonzero(~has_silver)     # 진짜 unl\n",
    "\n",
    "print(\"total:\", N_docs)\n",
    "print(\"with silver:\", len(idx_silver))\n",
    "print(\"unlabeled :\", len(idx_unl))\n",
    "\n",
    "# 이제 train/val은 silver 있는 애들만 섞어서 나눈다\n",
    "rng = np.random.default_rng(42)\n",
    "rng.shuffle(idx_silver)\n",
    "n_val = int(len(idx_silver) * 0.2)\n",
    "idx_val   = idx_silver[:n_val]\n",
    "idx_train = idx_silver[n_val:]\n",
    "\n",
    "# parents, children 만들기\n",
    "def build_parents_children(adj):\n",
    "    C = adj.shape[0]\n",
    "    parents = [np.flatnonzero(adj[:, j]).astype(np.int64) for j in range(C)]\n",
    "    children = [np.flatnonzero(adj[j]).astype(np.int64) for j in range(C)]\n",
    "    return parents, children\n",
    "\n",
    "parents, children = build_parents_children(B)\n",
    "\n",
    "pos_masks = np.zeros((N_docs, C), dtype=np.float32)\n",
    "neg_masks = np.zeros((N_docs, C), dtype=np.float32)\n",
    "\n",
    "for i in idx_silver:  # silver 있는 애만 돈다\n",
    "    core = silver[i]\n",
    "\n",
    "    # 1) core + parents\n",
    "    pos = set(core)\n",
    "    for c in core:\n",
    "        for p in parents[c]:\n",
    "            pos.add(int(p))\n",
    "\n",
    "    # 2) children은 모름\n",
    "    child = set()\n",
    "    for c in core:\n",
    "        for ch in children[c]:\n",
    "            child.add(int(ch))\n",
    "\n",
    "    for p in pos:\n",
    "        pos_masks[i, p] = 1.0\n",
    "\n",
    "    for j in range(C):\n",
    "        if j in pos:      # 이미 양성\n",
    "            continue\n",
    "        if j in child:    # 모름\n",
    "            continue\n",
    "        neg_masks[i, j] = 1.0\n",
    "\n",
    "\n",
    "\n",
    "train_ds = HierMultiLabelDataset(X, pos_masks, neg_masks, indices=idx_train)\n",
    "val_ds   = HierMultiLabelDataset(X, pos_masks, neg_masks, indices=idx_val) if len(idx_val) > 0 else None\n",
    "unl_ds   = UnlabeledDataset(X, idx_unl.tolist())\n",
    "print(len(train_ds),len(val_ds),len(unl_ds))\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=256, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=256, shuffle=False)\n",
    "\n",
    "model = BilinearHierClassifier(doc_dim=X.shape[1], label_emb=L, hidden_dim=256).to(device)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "epochs = 150\n",
    "\n",
    "N_labels = L.shape[0]\n",
    "best_f1 = -1.0\n",
    "patience = 20\n",
    "no_improve = 0\n",
    "warmup_self = 1   # 1 epoch은 self-training 안 하게 해서 한 번 안정화\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    # train\n",
    "    tr_loss = train_epoch_hier(model, train_loader, opt, device)\n",
    "\n",
    "    # val: f1 기준\n",
    "    if val_loader is not None and len(val_ds) > 0:\n",
    "        va_loss, va_f1 = eval_epoch_hier(model, val_loader, device, k=3)\n",
    "        print(f\"Epoch {epoch:03d} | train_loss={tr_loss:.3f}  val_loss={va_loss:.3f}  val_f1={va_f1:.3f}\")\n",
    "\n",
    "        # early stopping을 f1로\n",
    "        if va_f1 > best_f1 + 1e-6:\n",
    "            best_f1 = va_f1\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch} (best f1={best_f1:.4f})\")\n",
    "                break\n",
    "    else:\n",
    "        print(f\"Epoch {epoch:03d} | train_loss={tr_loss:.3f}\")\n",
    "\n",
    "    # self-training: 1에폭에 전부 들어가는 거 방지용으로 warmup 넣음\n",
    "    if epoch <= warmup_self:\n",
    "        print(\"  + (skip pseudo-labeling on warmup epoch)\")\n",
    "        continue\n",
    "\n",
    "    new_idx, new_pos, new_neg = pseudo_label_and_grow_hier(\n",
    "        model,\n",
    "        unl_ds,\n",
    "        X,\n",
    "        parents,\n",
    "        children,\n",
    "        C,                   # num_labels\n",
    "        device=device,\n",
    "        pseudo_threshold=0.6,\n",
    "        pseudo_topk=3,\n",
    "        batch_size=512,\n",
    "    )\n",
    "\n",
    "    if len(new_idx) > 0:\n",
    "        # 전역 마스크 갱신\n",
    "        pos_masks[new_idx] = new_pos\n",
    "        neg_masks[new_idx] = new_neg\n",
    "\n",
    "        # unl에서 제거\n",
    "        keep_mask = ~np.isin(unl_ds.indices, np.array(new_idx, dtype=np.int64))\n",
    "        unl_ds.indices = unl_ds.indices[keep_mask]\n",
    "\n",
    "        # train에 추가\n",
    "        train_ds.indices = np.concatenate([train_ds.indices, np.array(new_idx, dtype=np.int64)])\n",
    "        train_loader = DataLoader(train_ds, batch_size=256, shuffle=True, drop_last=False)\n",
    "\n",
    "        print(f\"  + Added {len(new_idx)} pseudo-labeled docs (unl pool → {len(unl_ds)} left)\")\n",
    "    else:\n",
    "        print(\"  + No pseudo-labeled docs added this epoch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec157776-e172-4c16-93a1-aacdf0a962d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd03ade-045a-4c94-89e4-de3d1e523595",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b88df0-bac0-49b4-9f58-d0c5e77583e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c55049ed-898c-4b80-aba2-d83ca27f82bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T02:05:06.916072Z",
     "iopub.status.busy": "2025-11-11T02:05:06.915804Z",
     "iopub.status.idle": "2025-11-11T02:07:20.506718Z",
     "shell.execute_reply": "2025-11-11T02:07:20.505887Z",
     "shell.execute_reply.started": "2025-11-11T02:05:06.916052Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] saved document embeddings → Amazon_products/test_doc_embeddings.csv  shape=(19658, 769)\n"
     ]
    }
   ],
   "source": [
    "# Minimal submission generator: pick 2–3 labels per sample via hierarchical beam scoring\n",
    "import csv, os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ------------ Paths (edit if needed) ------------\n",
    "TEST_CORPUS = \"Amazon_products/test/test_corpus.txt\"   # lines: pid \\t text\n",
    "DOC_CSV     = \"Amazon_products/test_doc_embeddings.csv\"  # first col: id (pid), rest: feat000..feat127\n",
    "LABEL_CSV   = \"Amazon_products/label_embeddings_gat.csv\" # first col: id (== node index 0..N-1)\n",
    "OUT_PATH    = \"submission.csv\"\n",
    "DOC_CSV = \"Amazon_products/test_doc_embeddings.csv\" # first col: id (pid), rest: feat000..feat127\n",
    "build_and_save_document_embeddings(\n",
    "    corpus_path=TEST_CORPUS,\n",
    "    model_name=\"bert-base-uncased\",\n",
    "    alpha=0.7,\n",
    "    max_length=256,    # 문서 길이에 맞게 조절\n",
    "    batch_size=32,\n",
    "    out_csv=DOC_CSV,\n",
    "    pad_width=2,\n",
    ")\n",
    "# ------------ Hyperparams ------------\n",
    "MIN_LABS  = 2\n",
    "MAX_LABS  = 3\n",
    "BATCH = 1024\n",
    "\n",
    "# load test pids\n",
    "pids = []\n",
    "with open(TEST_CORPUS, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        parts = line.rstrip(\"\\n\").split(\"\\t\", 1)\n",
    "        if len(parts) == 2:\n",
    "            pids.append(parts[0])\n",
    "\n",
    "# load doc embeddings (map pid->vec)\n",
    "df_doc = pd.read_csv(DOC_CSV)\n",
    "doc_ids = df_doc.iloc[:,0].astype(str).tolist()\n",
    "D = df_doc.iloc[:,1:].to_numpy(dtype=np.float32)\n",
    "D = l2_normalize(D)\n",
    "pid2idx = {pid: i for i, pid in enumerate(doc_ids)}\n",
    "\n",
    "# load label embeddings (ensure order aligns with adjacency rows)\n",
    "df_lab = pd.read_csv(LABEL_CSV)\n",
    "lab_ids = df_lab.iloc[:,0].astype(int).to_numpy()\n",
    "L = df_lab.iloc[:,1:].to_numpy(dtype=np.float32)\n",
    "ord = np.argsort(lab_ids)\n",
    "lab_ids = lab_ids[ord]\n",
    "L = L[ord]\n",
    "\n",
    "# precompute children lists\n",
    "children = [np.flatnonzero(A[i]) for i in range(A.shape[0])]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea357b8-0f1b-4069-bbd2-8e56029e65b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3302f3f7-2fb6-4ed9-ba2e-4714cbc76dd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574d819b-4455-4b10-8541-4a9a9f2ddec4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7411b961-6f29-4ac7-ba7a-97098db48b1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T04:37:33.326948Z",
     "iopub.status.busy": "2025-11-12T04:37:33.326524Z",
     "iopub.status.idle": "2025-11-12T04:37:34.932859Z",
     "shell.execute_reply": "2025-11-12T04:37:34.932375Z",
     "shell.execute_reply.started": "2025-11-12T04:37:33.326922Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv, os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ------------ Paths (edit if needed) ------------\n",
    "TEST_CORPUS = \"Amazon_products/test/test_corpus.txt\"   # lines: pid \\t text\n",
    "OUT_PATH    = \"submission_bda.csv\"\n",
    "# ------------ Hyperparams ------------\n",
    "MIN_LABS  = 2\n",
    "MAX_LABS  = 3\n",
    "BATCH = 1024\n",
    "doc_ids, doc_texts = load_docs_txt(TEST_CORPUS)\n",
    "\n",
    "# 2) 라벨 때 만든 vectorizer 재사용해서 임베딩 만들기\n",
    "test_embeddings = build_doc_embeddings_from_bm25_vectorizer(doc_texts, bm25)\n",
    "test_embeddings = test_embeddings.astype(np.float32)   # [num_test, d]\n",
    "# load test pids\n",
    "pids = doc_ids   # 이미 문자열 id\n",
    "if \"L\" in globals():\n",
    "    if not isinstance(L, np.ndarray):\n",
    "        # 예: L이 torch.Tensor인 경우\n",
    "        L = L.detach().cpu().numpy().astype(np.float32)\n",
    "else:\n",
    "    raise ValueError(\"라벨 임베딩 L이 메모리에 없어! GAT 끝난 뒤의 임베딩을 L로 둬야 해.\")\n",
    "\n",
    "# 5) 라벨 id는 0..N-1로 생성 (네가 말한 대로 adjacency랑 순서가 이미 맞다고 했으니까)\n",
    "lab_ids = np.arange(L.shape[0], dtype=np.int64)\n",
    "\n",
    "# 6) adjacency도 메모리에 있는 걸 그대로 쓴다\n",
    "#    여기서 A는 531x531 같은 numpy array라고 가정\n",
    "assert B.shape == (L.shape[0], L.shape[0]), \"Adjacency/label size mismatch\"\n",
    "\n",
    "# 7) children 리스트 미리 만들어두기\n",
    "children = [np.flatnonzero(B[i]) for i in range(B.shape[0])]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ef344a9-c996-4932-b7ab-9f7c5e7578b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T04:37:35.677177Z",
     "iopub.status.busy": "2025-11-12T04:37:35.676933Z",
     "iopub.status.idle": "2025-11-12T04:37:50.316876Z",
     "shell.execute_reply": "2025-11-12T04:37:50.316440Z",
     "shell.execute_reply.started": "2025-11-12T04:37:35.677161Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: submission_bda.csv | samples=19658 | min-max labels per sample=2-3 | missing_pids=0\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "def ancestors_of(node, adj):\n",
    "    # adj[parent, child] = 1 가정\n",
    "    parents = np.flatnonzero(adj[:, node])  # (N,)\n",
    "    return parents.tolist()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "IN_DIM = test_embeddings.shape[1]\n",
    "missing = 0  # 지금은 쓸 일 없지만 원래 코드랑 형태 맞춰둠\n",
    "\n",
    "with open(OUT_PATH, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow([\"id\", \"label\"])\n",
    "\n",
    "    buf_x, buf_pid = [], []\n",
    "\n",
    "    def flush():\n",
    "        if not buf_x:\n",
    "            return\n",
    "        xb = torch.from_numpy(np.stack(buf_x, axis=0).astype(np.float32)).to(device)\n",
    "        with torch.inference_mode():\n",
    "            prob = torch.sigmoid(model(xb)).detach().cpu().numpy()\n",
    "        prob = np.nan_to_num(prob, nan=-1.0, posinf=1.0, neginf=0.0)\n",
    "\n",
    "        for pid, p in zip(buf_pid, prob):\n",
    "            order = np.argsort(-p)\n",
    "\n",
    "            # 1) 기본 후보 뽑기\n",
    "            thr_keep = [i for i in order if p[i] >= 0.5][:MAX_LABS]\n",
    "            if len(thr_keep) >= MIN_LABS:\n",
    "                keep = thr_keep[:MAX_LABS]\n",
    "            else:\n",
    "                keep = order[:max(MIN_LABS, len(thr_keep))]\n",
    "                if len(keep) < MIN_LABS:\n",
    "                    keep = order[:MIN_LABS]\n",
    "\n",
    "            # 2) 부모 후보\n",
    "            parent_cands = []\n",
    "            for c in keep:\n",
    "                pars = ancestors_of(c, B)\n",
    "                for pa in pars:\n",
    "                    if pa not in keep and pa not in parent_cands:\n",
    "                        parent_cands.append(pa)\n",
    "\n",
    "            parent_cands.sort(key=lambda idx: p[idx], reverse=True)\n",
    "\n",
    "            # 3) 남는 슬롯 부모로 채우기\n",
    "            final_idxs = list(keep)\n",
    "            for pa in parent_cands:\n",
    "                if len(final_idxs) >= MAX_LABS:\n",
    "                    break\n",
    "                final_idxs.append(pa)\n",
    "\n",
    "            # 4) 그래도 모자라면 확률순\n",
    "            if len(final_idxs) < MIN_LABS:\n",
    "                for idx in order:\n",
    "                    if idx not in final_idxs:\n",
    "                        final_idxs.append(idx)\n",
    "                    if len(final_idxs) >= MIN_LABS:\n",
    "                        break\n",
    "\n",
    "            labels = sorted(int(lab_ids[i]) for i in final_idxs)\n",
    "            w.writerow([pid, \",\".join(map(str, labels))])\n",
    "\n",
    "        buf_x.clear()\n",
    "        buf_pid.clear()\n",
    "\n",
    "    # 여기서 바로 pids와 test_embeddings를 같이 순회\n",
    "    for pid, emb in zip(pids, test_embeddings):\n",
    "        x = emb\n",
    "        if x.dtype != np.float32:\n",
    "            x = x.astype(np.float32, copy=False)\n",
    "        buf_x.append(x)\n",
    "        buf_pid.append(pid)\n",
    "        if len(buf_x) >= BATCH:\n",
    "            flush()\n",
    "    flush()\n",
    "\n",
    "print(f\"Saved: {OUT_PATH} | samples={len(pids)} | min-max labels per sample={MIN_LABS}-{MAX_LABS} | missing_pids={missing}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e293bbf-8597-4c2c-91e1-b03ce7477217",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b72c5b-1d7a-4d59-a975-e7fa22dfbb32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a677853-591b-488e-835f-cf4b4c164c1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343dfe98-d91d-4c90-9325-ec5a6ff5c560",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49cebb9-e667-46aa-8c96-64a63c801ab3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d535af4-4d65-4127-b625-caac88456b68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b2e45b-844d-437a-9fc5-1d47ad4a9832",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a133dad-b306-4349-b9d3-073022f3eb43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba53595b-f997-468a-a4f9-6deee2dc1a08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb8152f-64ca-44fc-a37b-4ab46e4291d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5d29a7-c5b4-4a26-954b-b65a737f632c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
